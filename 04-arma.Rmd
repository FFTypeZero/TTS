# ARMA

## Definition

## MA / AR Operators

## Redundancy

## Causal + Invertible

## Estimation of Parameters

Consider a time series given by $x_t \sim ARMA(p,q)$. This gives us with a paramter space $\Omega$ that looks like so: 

\[\vec \varphi  = \left[ {\begin{array}{*{20}{c}}
  {{\phi _1}} \\ 
   \vdots  \\ 
  {{\phi _p}} \\ 
  {{\theta _1}} \\ 
   \vdots  \\ 
  {{\theta _q}} \\ 
  {{\sigma ^2}} 
\end{array}} \right]\]

In order to estimate this parameter space, we must assume the following three conditions:

1. The process is casual
1. The process is invertible
1. The process has Gaussian innovations.

**Innovations** are a time series equivalent to residuals. That is, an innovation is given by ${x_t} - \hat x_t^{t - 1}$, where $\hat x_t^{t - 1}$ is the prediction at time $t$ given $t-1$ observations and ${x_t}$ is the true value observed at time $t$.

There are two main ways of performing such an estimation of the parameter space.

1. Maximum Likelihood / Least Squares Estimation [MLE / LSE]
1. Method of Moments (MoM)

To begin, we'll explore using the MLE to perform the estimation.

### Maximum Likelihood Estimation

**Definition** 
Consider $X_n = (X_1, X_2, \ldots, X_n)$ with the joint density $f(X_1, X_2, \ldots, X_n ; \theta)$ where $\theta \in \Theta$. Given $X_1 = x_1, X_2 = x_2, \ldots, X_n = x_n$ is observed, we have the likelihood function of $\theta$ as

$$L(\theta) = L(\theta|x_1,x_2, \ldots, x_n) = f(x_1,x_2, \ldots, x_n | \theta)$$

If the $X_i$ are iid, then the likelihood simplifies to: 

\[L(\theta) = \prod\limits_{i = 1}^n {f\left( {{x_i}|\theta } \right)} \]

However, that's a bit painful to maximize with calculus. So, we opt to use the log of the function since derivatives are easier and the logarithmic function is always increasing. Thus, we traditionally use:

\[l\left( \theta  \right) = \log \left( {L\left( \theta  \right)} \right) = \sum\limits_{i = 1}^n {\log \left( {f\left( {{x_i}|\theta } \right)} \right)} \]

From maximizes the likelihood function $L(\theta)$, we get the **maximum likelihood estimate (MLE)** of $\theta$. So, we end up with a value that makes the observed data the "most probable."

Note: The likelihood function is **not** a probability density function.

#### $AR(1)$ with mean $\mu$

Consider an $AR(1)$ process given as $y_t = \phi y_{t-1} + w_t$, ${w_t}\mathop  \sim \limits^{iid} N\left( {0,{\sigma ^2}} \right)$, with $E\left[ {{y_t}} \right] = 0$, $\left| \phi  \right| < 1$.

Let $x_t = y_t + \mu$, so that $E\left[ {{x_t}} \right] = \mu$.

Then, ${x_t} - \mu  = {y_t}$. Substituting in for $y_t$, we get:

\[\begin{aligned}
y_t &= \phi y_{t-1} + w_t \\
\underbrace {\left( {{x_t} - \mu } \right)}_{ = {y_t}} &= \phi \underbrace {\left( {{x_{t - 1}} - \mu } \right)}_{ = {y_t}} + {w_t} \\
{x_t} &= \mu  + \phi \left( {{x_{t - 1}} - \mu } \right) + {w_t}
\end{aligned}\]

In this case, $x_t$ is an $AR(1)$ process with mean $\mu$. 

This means that we have:

1. $E\left[ {{x_t}} \right] = \mu$
1. \[\begin{aligned}
  Var\left( {{x_t}} \right) &= Var\left( {{x_t} - \mu } \right) \hfill \\
   &= Var\left( {{y_t}} \right) \hfill \\
   &= Var\left( {\sum\limits_{j = 0}^\infty  {{\phi ^j}{w_{t - j}}} } \right) \hfill \\
   &= \sum\limits_{j = 0}^\infty  {{\phi ^{2j}}Var\left( {{w_{t - j}}} \right)}  \hfill \\
   &= {\sigma ^2}\sum\limits_{j = 0}^\infty  {{\phi ^{2j}}}  \hfill \\
   &= \frac{{{\sigma ^2}}}{{1 - {\phi ^2}}},{\text{  since }}\left| \phi  \right| < 1{\text{ and }}\sum\limits_{k = 0}^n {a{r^k}}  = \frac{a}{{1 - r}} \hfill \\ 
\end{aligned} \]

So, $x_t \sim N\left({ \mu, \frac{{{\sigma ^2}}}{{1 - {\phi ^2}}} }\right)$.

Note that the distribution of $x_t$ is normal and, thus, the density function of $x_t$ is given by:
\[\begin{aligned}
  f\left( {{x_t}} \right) &= \sqrt {\frac{{1 - {\phi ^2}}}{{2\pi {\sigma ^2}}}} \exp \left( { - \frac{1}{2} \cdot \frac{{1 - {\phi ^2}}}{{{\sigma ^2}}} \cdot {{\left( {{x_t} - \mu } \right)}^2}} \right) \hfill \\
   &= {\left( {2\pi } \right)^{ - \frac{1}{2}}}{\left( {{\sigma ^2}} \right)^{ - \frac{1}{2}}}{\left( {1 - {\phi ^2}} \right)^{\frac{1}{2}}}\exp \left( { - \frac{1}{2} \cdot \frac{{1 - {\phi ^2}}}{{{\sigma ^2}}} \cdot {{\left( {{x_t} - \mu } \right)}^2}} \right) \textrm{           [1]}  \\ 
\end{aligned} \]

We'll call the last equation [1].

#### Conditioning time $x_t | x_{t-1}$

Now, consider $x_t | x_{t-1}$ for $t > 1$.

The mean is given by:

\[\begin{aligned}
  E\left[ {{x_t}|{x_{t - 1}}} \right] &= E\left[ {\mu  + \phi \left( {{x_{t - 1}} - \mu } \right) + {w_t}|{x_{t - 1}}} \right] \nonumber \\
   &= \mu  + \phi \left( {{x_{t - 1}} - \mu } \right)
\end{aligned} \]

This is the case since $E\left[ {{x_{t - 1}}|{x_{t - 1}}} \right] = {x_{t - 1}}$ and $E\left[ {{w_t}|{x_{t - 1}}} \right] = 0$

Now, the variance is:

\[\begin{aligned}
  Var\left( {{x_t}|{x_{t - 1}}} \right) &= Var\left( {\mu  + \phi \left( {{x_{t - 1}} - \mu } \right) + {w_t}|{x_{t - 1}}} \right) \hfill \\
   &= \underbrace {Var\left( {\mu  + \phi \left( {{x_{t - 1}} - \mu } \right)|{x_{t - 1}}} \right)}_{ = 0} + Var\left( {{w_t}|{x_{t - 1}}} \right) \hfill \\
   &= Var\left( {{w_t}} \right) \hfill \\
   &= {\sigma ^2} \hfill \\ 
\end{aligned} \]

Thus, we have: ${x_t}\sim N\left( {\mu  + \phi \left( {{x_{t - 1}} - \mu } \right),{\sigma ^2}} \right)$.

Again, note that the distribution of $x_t$ is normal and, thus, the density function of $x_t$ is given by:
\[\begin{aligned}
  f\left( {{x_t}} \right) &= \sqrt {\frac{1}{{2\pi {\sigma ^2}}}} \exp \left( { - \frac{1}{{2{\sigma ^2}}} \cdot {{\left[ {\left( {{x_t} - \mu } \right) - \phi \left( {{x_{t - 1}} - \mu } \right)} \right]}^2}} \right) \hfill \\
   &= {\left( {2\pi } \right)^{ - \frac{1}{2}}}{\left( {{\sigma ^2}} \right)^{ - \frac{1}{2}}}\exp \left( { - \frac{1}{{2{\sigma ^2}}} \cdot {{\left[ {\left( {{x_t} - \mu } \right) - \phi \left( {{x_{t - 1}} - \mu } \right)} \right]}^2}} \right) \textrm{           [2]} \\ 
\end{aligned} \]

And for this equation we'll call it [2].

### MLE for $\sigma ^2$ on $AR(1)$ with mean $\mu$

*Whew*, with all of the above said, we're now ready to obtain an MLE estimate on an $AR(1)$.

Let $\vec{\theta}  = \left[ {\begin{array}{*{20}{c}}
  \mu  \\ 
  \phi  \\ 
  {{\sigma ^2}} 
\end{array}} \right]$, then the likelihood of $\vec{\theta}$ is given by $x_1, \ldots , x_T$ is:

\[\begin{aligned}
  L\left( {\vec \theta |{x_1}, \ldots ,{x_T}} \right) &= f\left( {{x_1}, \ldots ,{x_T}|\vec \theta } \right) \hfill \\
   &= f\left( {{x_1}} \right) \cdot \prod\limits_{t = 2}^T {f\left( {{x_t}|{x_{t - 1}}} \right)}
\end{aligned} \]

The last equality is the result of us using a lag 1 of "memory." Also, note that $x_t | x_{t-1}$ must have $t > 1 \in \mathbb{N}$. Furthermore, we have dropped the parameters in the densities, e.g. $\vec{\theta}$ in $f(\cdot)$, to ease notation. 

Using equations [1] and [2], we have:

\[L\left( {\vec \theta |{x_1}, \ldots ,{x_T}} \right) = {\left( {2\pi } \right)^{ - \frac{T}{2}}}{\left( {{\sigma ^2}} \right)^{ - \frac{T}{2}}}{\left( {1 - {\phi ^2}} \right)^{\frac{1}{2}}}\exp \left( { - \frac{1}{{2{\sigma ^2}}}\left[ {\left( {1 - {\phi ^2}} \right){{\left( {{x_t} - \mu } \right)}^2} + \sum\limits_{t = 2}^T {{{\left[ {\left( {{x_t} - \mu } \right) - \phi \left( {{x_{t - 1}} - \mu } \right)} \right]}^2}} } \right]} \right)\]

For convenience, we'll define:

\[S\left( {\mu ,\phi } \right) = \left( {1 - {\phi ^2}} \right){\left( {{x_t} - \mu } \right)^2} + \sum\limits_{t = 2}^T {{{\left[ {\left( {{x_t} - \mu } \right) - \phi \left( {{x_{t - 1}} - \mu } \right)} \right]}^2}} \]

Fun fact, this is called the "**unconditional** sum of squares."

Thus, we will operate on:

\[L\left( {\vec \theta |{x_1}, \ldots ,{x_T}} \right) = {\left( {2\pi } \right)^{ - \frac{T}{2}}}{\left( {{\sigma ^2}} \right)^{ - \frac{T}{2}}}{\left( {1 - {\phi ^2}} \right)^{\frac{1}{2}}}\exp \left( { - \frac{1}{{2{\sigma ^2}}}S\left( {\mu ,\phi } \right)} \right)\]

Taking the log of this yields:

\[\begin{aligned}
  l\left( {\vec \theta |{x_1}, \ldots ,{x_T}} \right) &= \log \left( {L\left( {\vec \theta |{x_1}, \ldots ,{x_T}} \right)} \right) \hfill \\
   &=  - \frac{T}{2}\log \left( {2\pi } \right) - \frac{T}{2}\log \left( {{\sigma ^2}} \right) + \frac{1}{2}\left( {1 - {\phi ^2}} \right) - \frac{1}{{2{\sigma ^2}}}S\left( {\mu ,\phi } \right) \hfill \\ 
\end{aligned} \]

Now, taking the derivative and solving for the maximized point gives:

\[\begin{aligned}
  \frac{\partial }{{\partial {\sigma ^2}}}l\left( {\vec \theta |{x_1}, \ldots ,{x_T}} \right) &=  - \frac{T}{{2{\sigma ^2}}} + \frac{1}{{2{\sigma ^4}}}S\left( {\mu ,\phi } \right) \hfill \\
  0 &=  - \frac{T}{{2{\sigma ^2}}} + \frac{1}{{2{\sigma ^4}}}S\left( {\mu ,\phi } \right) \hfill \\
  \frac{T}{{2{\sigma ^2}}} &= \frac{1}{{2{\sigma ^4}}}S\left( {\mu ,\phi } \right) \hfill \\
  {{ \sigma }^2} &= \frac{1}{T}S\left( {\mu ,\phi } \right) \hfill \\ 
\end{aligned} \]

Thus, the MLE for ${\hat \sigma }^2 = \frac{1}{T}S\left( {\hat \mu ,\hat \phi } \right)$, where $\hat \mu$ and $\hat \phi$ are the MLEs for $\mu , \phi$ that are obtained numerically via either *Newton Raphson* or a *Scoring Algorithm*. (More details in a numerical recipe book.)

#### Conditional MLE on $AR(1)$ with mean $\mu$

A common strategy to reduce the dependency on numerical recipes is to simplify $l\left( {\vec \theta |{x_1}, \ldots ,{x_T}} \right)$ by using ${l^*}\left( {\vec \theta |{x_1}, \ldots ,{x_T}} \right)$:

\[\begin{aligned}
  {l^*}\left( {\vec \theta |{x_1}, \ldots ,{x_T}} \right) &= \prod\limits_{t = 2}^T {\log \left( {f\left( {{x_t}|{x_{t - 1}}} \right)} \right)}  \hfill \\
   &= \prod\limits_{t = 2}^T {\log \left( {{{\left( {2\pi } \right)}^{ - \frac{1}{2}}}{{\left( {{\sigma ^2}} \right)}^{ - \frac{1}{2}}}\exp \left( { - \frac{1}{{2{\sigma ^2}}} \cdot {{\left[ {\left( {{x_t} - \mu } \right) - \phi \left( {{x_{t - 1}} - \mu } \right)} \right]}^2}} \right)} \right)}  \hfill \\
   &=  - \frac{{\left( {T - 1} \right)}}{2}\log \left( {2\pi } \right) - \frac{{\left( {T - 1} \right)}}{2}\log \left( {{\sigma ^2}} \right) - \frac{1}{{2{\sigma ^2}}}\sum\limits_{t = 2}^T {{{\left[ {\left( {{x_t} - \mu } \right) - \phi \left( {{x_{t - 1}} - \mu } \right)} \right]}^2}}  \hfill \\ 
\end{aligned} \]

Again, for convenience, we'll define:

\[{S_c}\left( {\mu ,\phi } \right) = \sum\limits_{t = 2}^T {{{\left[ {\left( {{x_t} - \mu } \right) - \phi \left( {{x_{t - 1}} - \mu } \right)} \right]}^2}} \]

Fun fact, this is called the "**conditional** sum of squares."

So, we will use:

\[{l^*}\left( {\vec \theta |{x_1}, \ldots ,{x_T}} \right) =  - \frac{{\left( {T - 1} \right)}}{2}\log \left( {2\pi } \right) - \frac{{\left( {T - 1} \right)}}{2}\log \left( {{\sigma ^2}} \right) - \frac{1}{{2{\sigma ^2}}}{S_c}\left( {\mu ,\phi } \right)\]

Taking the derivative with respect to $\mu$ gives:

\[\begin{aligned}
  \frac{\partial }{{\partial \mu }}{l^*}\left( {\vec \theta |{x_1}, \ldots ,{x_T}} \right) &=  - \frac{1}{{2{\sigma ^2}}}\sum\limits_{t = 2}^T {2\left[ {\left( {{x_t} - \mu } \right) - \phi \left( {{x_{t - 1}} - \mu } \right)} \right]\left( {\phi  - 1} \right)}  \hfill \\
   &= \frac{{1 - \phi }}{{{\sigma ^2}}}\sum\limits_{t = 2}^T {\left[ {\left( {{x_t} - \mu } \right) - \phi \left( {{x_{t - 1}} - \mu } \right)} \right]}  \hfill \\
   &= \frac{{1 - \phi }}{{{\sigma ^2}}}\sum\limits_{t = 2}^T {\left( {{x_t} - \phi {x_{t - 1}} - \mu \left( {1 - \phi } \right)} \right)}  \hfill \\
   &= -\frac{{{{\left( {1 - \phi } \right)}^2}}}{{{\sigma ^2}}}\mu \left( {T - 1} \right) + \frac{{\left( {1 - \phi } \right)}}{{{\sigma ^2}}}\sum\limits_{t = 2}^T {\left( {{x_t} - \phi {x_{t - 1}}} \right)}  \hfill \\ 
\end{aligned} \]

Solving for $\mu^{*}$ gives:

\[\begin{aligned}
  0 &= \frac{\partial }{{\partial \mu }}{l^*}\left( {\vec \theta |{x_1}, \ldots ,{x_t}} \right) \hfill \\
  0 &=  - \frac{{{{\left( {1 - \phi } \right)}^2}}}{{{\sigma ^2}}}{\mu ^*}\left( {T - 1} \right) + \frac{{\left( {1 - {\phi ^*}} \right)}}{{\sigma _*^2}}\sum\limits_{t = 2}^T {\left( {{x_t} - {\phi ^*}{x_{t - 1}}} \right)}  \hfill \\
  \frac{{{{\left( {1 - {\phi ^*}} \right)}^2}}}{{\sigma _*^2}}{\mu ^*}\left( {T - 1} \right) &= \frac{{\left( {1 - {\phi ^*}} \right)}}{{\sigma _*^2}}\sum\limits_{t = 2}^T {\left( {{x_t} - {\phi ^*}{x_{t - 1}}} \right)}  \hfill \\
  {\mu ^*}\left( {1 - {\phi ^*}} \right)\left( {T - 1} \right) &= \sum\limits_{t = 2}^T {\left( {{x_t} - {\phi ^*}{x_{t - 1}}} \right)}  \hfill \\
  {\mu ^*} &= \frac{1}{{\left( {1 - {\phi ^*}} \right)\left( {T - 1} \right)}}\sum\limits_{t = 2}^T {\left( {{x_t} - {\phi ^*}{x_{t - 1}}} \right)}  \hfill \\
  {\mu ^*} &= \frac{1}{{1 - {\phi ^*}}}\left[ {\underbrace {\frac{1}{{T - 1}}\sum\limits_{t = 2}^T {{x_t}} }_{ = {{\bar x}_{\left( 2 \right)}}} - \underbrace {\frac{{{\phi ^*}}}{{T - 1}}\sum\limits_{t = 2}^T {{x_{t - 1}}} }_{ = {{\bar x}_{\left( 1 \right)}}}} \right] \hfill \\
  {{\hat \mu }^*} &= \frac{1}{{1 - {\phi ^*}}}\left( {{{\bar x}_{\left( 2 \right)}} - \phi {{\bar x}_{\left( 1 \right)}}} \right) \hfill \\ 
\end{aligned} \]

When $T$ is large, we have the following:

\[\begin{aligned}
  {{\bar x}_{\left( 1 \right)}} \approx \bar x &,{{\bar x}_{\left( 2 \right)}} \approx \bar x \hfill \\
   \hfill \\
  {{\hat \mu }^*} &= \frac{1}{{1 - {\phi ^*}}}\left( {\bar x - {\phi ^*}\bar x} \right) \hfill \\
   &= \frac{{\bar x}}{{1 - {\phi ^*}}}\left( {1 - {\phi ^*}} \right) \hfill \\
   &= \bar x \hfill \\ 
\end{aligned} \]


Taking the derivative with respect to $\sigma^2$ and solving for $\sigma^2$ gives:
\[\begin{aligned}
  \frac{\partial }{{\partial {\sigma ^2}}}{l^*}\left( {\vec \theta |{x_1}, \ldots ,{x_T}} \right) &=  - \frac{{\left( {T - 1} \right)}}{{2\sigma _*^2}} + \frac{1}{{2\sigma _*^4}}{S_c}\left( {\mu ,\phi } \right) \hfill \\
  0 &=  - \frac{{\left( {T - 1} \right)}}{{2\sigma _*^2}} + \frac{1}{{2\sigma _*^4}}{S_c}\left( {\mu ,\phi } \right) \hfill \\
  \frac{{\left( {T - 1} \right)}}{{2\sigma _*^2}} &= \frac{1}{{2\sigma _*^4}}{S_c}\left( {\mu ,\phi } \right) \hfill \\
  \hat \sigma _*^2 &= \frac{1}{{T - 1}}{S_c}\left( {{{\hat \mu }^*},{{\hat \phi }^*}} \right) \hfill \\ 
\end{aligned} \]


Taking the derivative with respect to $\phi$ gives:
\[\begin{aligned}
  \frac{\partial }{{\partial \phi }}{l^*}\left( {\vec \theta |{x_1}, \ldots ,{x_T}} \right) &=  - \frac{1}{{2{\sigma ^2}}}\sum\limits_{t = 2}^T { - 2\left[ {\left( {{x_t} - \mu } \right) - \phi \left( {{x_{t - 1}} - \mu } \right)} \right]\left( {{x_{t - 1}} - \mu } \right)}  \hfill \\
   &= \frac{1}{{{\sigma ^2}}}\sum\limits_{t = 2}^T {\left[ {{x_t} - \phi {x_{t - 1}} - \mu \left( {1 - \phi } \right)} \right]\left( {{x_{t - 1}} - \mu } \right)}  \hfill \\
   &= \frac{1}{{{\sigma ^2}}}\sum\limits_{t = 2}^T {\left[ {{x_t}{x_{t - 1}} - \phi x_{t - 1}^2 - \mu \left( {1 - \phi } \right){x_{t - 1}} - \mu {x_t} + \mu \phi {x_{t - 1}} + {\mu ^2}\left( {1 - \phi } \right)} \right]}  \hfill \\
   &= \frac{1}{{{\sigma ^2}}}\left[ \begin{gathered}
  \sum\limits_{t = 2}^T {{x_t}{x_{t - 1}}}  - \phi \sum\limits_{t = 2}^T {x_{t - 1}^2}  - \mu \left( {1 - \phi } \right)\left( {T - 1} \right){{\bar x}_{\left( 1 \right)}} \hfill \\
   - \mu \left( {T - 1} \right){{\bar x}_{\left( 2 \right)}} + \phi \mu \left( {T - 1} \right){{\bar x}_{\left( 1 \right)}} + {\mu ^2}\left( {1 - \phi } \right)\left( {T - 1} \right) \hfill \\ 
\end{gathered}  \right] 
\end{aligned}\]

Solving for $\phi$ gives:
\[\begin{aligned}
  0 &= \frac{\partial }{{\partial \phi }}{l^*}\left( {\vec \theta |{x_1}, \ldots ,{x_T}} \right) \hfill \\
  0 &= \sum\limits_{t = 2}^T {{x_t}{x_{t - 1}}}  - {{\hat \phi }^*}\sum\limits_{t = 2}^T {x_{t - 1}^2}  - \left( {{{\bar x}_{\left( 2 \right)}} - {{\hat \phi }^*}{{\bar x}_{\left( 1 \right)}}} \right)\left( {T - 1} \right){{\bar x}_{\left( 1 \right)}} - \frac{{{{\bar x}_{\left( 2 \right)}} - {{\hat \phi }^*}{{\bar x}_{\left( 1 \right)}}}}{{1 - {{\hat \phi }^*}}}\left( {T - 1} \right){{\bar x}_{\left( 2 \right)}} \\
  &+ {{\hat \phi }^*}\frac{{{{\bar x}_{\left( 2 \right)}} - {{\hat \phi }^*}{{\bar x}_{\left( 1 \right)}}}}{{1 - {{\hat \phi }^*}}}\left( {T - 1} \right){{\bar x}_{\left( 1 \right)}} + {\left( {\frac{{{{\bar x}_{\left( 2 \right)}} - {{\hat \phi }^*}{{\bar x}_{\left( 1 \right)}}}}{{1 - {{\hat \phi }^*}}}} \right)^2}\left( {1 - {{\hat \phi }^*}} \right)\left( {T - 1} \right)  \hfill \\
   &\vdots  \hfill \\
  &{\text{Magic}} \hfill \\
   &\vdots  \hfill \\
  {{\hat \phi }^*} &= \frac{{\sum\limits_{t = 2}^T {\left( {{x_t} - {{\bar x}_{\left( 2 \right)}}} \right)\left( {{x_{t-1}} - {{\bar x}_{\left( 1 \right)}}} \right)} }}{{\sum\limits_{t = 2}^T {{{\left( {{x_{t - 1}} - {{\bar x}_{\left( 1 \right)}}} \right)}^2}} }} \hfill \\ 
\end{aligned} \]

When $T$ is large, we have: 

\[\begin{aligned}
  \sum\limits_{t = 2}^T {\left( {{x_t} - {{\bar x}_{\left( 2 \right)}}} \right)\left( {{x_t} - {{\bar x}_{\left( 1 \right)}}} \right)}  &\approx \sum\limits_{t = 2}^T {\left( {{x_t} - \bar x} \right)\left( {{x_{t - 1}} - \bar x} \right)}  \hfill \\
  \sum\limits_{t = 2}^T {{{\left( {{x_{t - 1}} - {{\bar x}_{\left( 1 \right)}}} \right)}^2}}  &\approx \sum\limits_{t = 1}^T {{{\left( {{x_t} - \bar x} \right)}^2}}  \hfill \\
   \hfill \\
  {{\hat \phi }^*} &= \frac{{\sum\limits_{t = 2}^T {\left( {{x_t} - {{\bar x}_{\left( 2 \right)}}} \right)\left( {{x_t} - {{\bar x}_{\left( 1 \right)}}} \right)} }}{{\sum\limits_{t = 2}^T {{{\left( {{x_{t - 1}} - {{\bar x}_{\left( 1 \right)}}} \right)}^2}} }} \approx \frac{{\sum\limits_{t = 2}^T {\left( {{x_t} - \bar x} \right)\left( {{x_{t - 1}} - \bar x} \right)} }}{{\sum\limits_{t = 1}^T {{{\left( {{x_t} - \bar x} \right)}^2}} }} = \hat \rho \left( 1 \right) \hfill \\ 
\end{aligned} \]


## Method of Moments

The goal behind the estimation with Method of Moments is to match the theoretical moment (e.g. $E\left[ {x_t^k} \right]$) with the sample moment (e.g $\frac{1}{n}\sum\limits_{i = 1}^n {x_i^k}$), where $k$ denotes the moment.

This method often leads to suboptimal estimates for general ARMA models. However, it is quite optimal for $AR(p)$.

### Method of Moments - AR(p)

Consider an $AR(p)$ process represented by: 
\[{x_t} = {\phi _1}{x_{t - 1}} +  \cdots  + {\phi _p}{x_{t - p}} + {w_t}\] 
where $w_t \sim N(0,\sigma^2)$ 

To begin, we find the Covariance of the process when $h > 0$:
\[\begin{aligned}
  Cov\left( {{x_{t + h}},{x_t}} \right) &\mathop = \limits^{\left( {h > 0} \right)} Cov\left( {{\phi _1}{x_{t + h - 1}} +  \cdots  + {\phi _p}{x_{t + h - p}} + {w_{t + h}},{x_t}} \right) \hfill \\
   &= {\phi _1}Cov\left( {{x_{t + h - 1}},{x_t}} \right) +  \cdots  + {\phi _p}Cov\left( {{x_{t + h - p}},{x_t}} \right) + Cov\left( {{w_{t + h}},{x_t}} \right) \hfill \\
   &= {\phi _1}\gamma \left( {h - 1} \right) +  \cdots  + {\phi _p}\gamma \left( {h - p} \right) \hfill \\ 
\end{aligned} \]

Now, we turn our attention to the variance of the process:

\[\begin{aligned}
  Var\left( {{w_t}} \right) &= Cov\left( {{w_t},{w_t}} \right) \hfill \\
   &= Cov\left( {{w_t},{w_t}} \right) + \underbrace {Cov\left( {{\phi _1}{x_{t - 1}},{w_t}} \right)}_{ = 0} +  \cdots  + \underbrace {Cov\left( {{\phi _p}{x_{t - p}},{w_t}} \right)}_{ = 0} \hfill \\
   &= Cov\left( {\underbrace {{\phi _1}{x_{t - 1}} +  \cdots  + {\phi _p}{x_p} + {w_t}}_{ = {x_t}},{w_t}} \right) \hfill \\
   &= Cov\left( {{x_t},{w_t}} \right) \hfill \\
   &= Cov\left( {{x_t},{x_t} - {\phi _1}{x_{t - 1}} -  \cdots  - {\phi _p}{x_p}} \right) \hfill \\
   &= Cov\left( {{x_t},{x_t}} \right) - {\phi _1}Cov\left( {{x_t},{x_{t - 1}}} \right) -  \cdots  - {\phi _p}Cov\left( {{x_t},{x_{t - p}}} \right) \hfill \\
   &= \gamma \left( 0 \right) - {\phi _1}\gamma \left( 1 \right) -  \cdots  - {\phi _p}\gamma \left( p \right) \hfill \\ 
\end{aligned} \]

Together, these equations are known as the **Yule-Walker** equations.

### Yule-Walker

**Definition** 

Equation form:

\[\begin{aligned}
  \gamma \left( h \right) &= {\phi _1}\gamma \left( {h - 1} \right) -  \cdots  - {\phi _p}\gamma \left( {h - p} \right) \hfill \\
  {\sigma ^2} &= \gamma \left( 0 \right) - {\phi _1}\gamma \left( 1 \right) -  \cdots  - {\phi _p}\gamma \left( p \right) \hfill \\ 
  h & = 1, \ldots, p
\end{aligned} \]

Matrix form:
\[\begin{aligned}
  \Gamma \vec \phi  &= \vec \gamma  \hfill \\
  {\sigma ^2} &= \gamma \left( 0 \right) - {{\vec \phi }^T}\vec \gamma  \hfill \\
   \hfill \\
  \vec \phi  &= \left[ {\begin{array}{*{20}{c}}
  {{\phi _1}} \\ 
   \vdots  \\ 
  {{\phi _p}} 
\end{array}} \right]_{p \times 1},\vec \gamma = \left[ {\begin{array}{*{20}{c}}
  {\gamma \left( 1 \right)} \\ 
   \vdots  \\ 
  {\gamma \left( p \right)} 
\end{array}} \right]_{p \times 1},\Gamma  = \left\{ {\gamma \left( {k - j} \right)} \right\}_{j,k = 1}^p  \\ 
\end{aligned} \]

More aptly, the structure of $\Gamma$ looks like the following: 
$$\Gamma = {\left[ {\begin{array}{*{20}{c}}
  {\gamma \left( 0 \right)}&{\gamma \left( { - 1} \right)}&{\gamma \left( { - 2} \right)}& \cdots &{\gamma \left( {1 - p} \right)} \\ 
  {\gamma \left( 1 \right)}&{\gamma \left( 0 \right)}&{\gamma \left( { - 1} \right)}& \cdots &{\gamma \left( {2 - p} \right)} \\ 
  {\gamma \left( 2 \right)}&{\gamma \left( 1 \right)}&{\gamma \left( 0 \right)}& \cdots &{\gamma \left( {3 - p} \right)} \\ 
   \vdots & \vdots & \vdots & \ddots & \vdots  \\ 
  {\gamma \left( {p - 1} \right)}&{\gamma \left( {p - 2} \right)}&{\gamma \left( {p - 3} \right)}& \cdots &{\gamma \left( 0 \right)} 
\end{array}} \right]_{p \times p}}$$ 

Note, that we are able to use the above equations to effectively estimate $\vec \phi$ and $\sigma ^2$.

\[\left[ \begin{aligned}
  \hat{\vec{\phi}}  &= {{\hat \Gamma }^{ - 1}}\hat{\vec{\gamma}}  \hfill \\
  {{\hat \sigma }^2} &= \hat \gamma \left( 0 \right) - {{\hat{\vec{\gamma}}}^T}{{\hat \Gamma }^{ - 1}}\hat{\vec{\gamma}} \hfill \\ 
\end{aligned}  \right. \to {\text{Yule - Walker Estimates}}\]

For the second equation, we are effectively substituting in the first equation for $\hat{\vec{\phi}}$, hence the quadratic form ${{\hat{\vec{\gamma}}}^T}{{\hat \Gamma }^{ - 1}}\hat{\vec{\gamma}}$.

With this being said, there are a few nice asymptotic properties that we obtain for an $AR(p)$.

1. $\sqrt T \left( {\hat{\vec{\phi}}  - \vec \phi } \right)\mathop  \to \limits_{t \to \infty }^L N\left( {\vec 0,{\sigma ^2}{\Gamma ^{ - 1}}} \right)$
1. ${\hat \sigma ^2}\mathop  \to \limits^p {\sigma ^2}$

Yule-Walker estimates are optimal in the sense that they have the smallest asymptotic variance i.e. \[Var\left( {\sqrt{T} \hat{\vec{\phi}} } \right) = {\sigma ^2}{\Gamma ^{ - 1}}\] **However, they are not necessarily optimal with small sample sizes.**

Conceptually, the reason for this optimality result is a consequence from the linear dependence between moments and variables. 

This is not true for MA or ARMA, which are both nonlinear and suboptimal.

### Estimates

Consider $x_t$ as an $MA(1)$ process: ${x_t} = \theta {w_{t - 1}} + {w_t},{w_t}\mathop  \sim \limits^{i.i.d} N\left( {0,{\sigma ^2}} \right)$

Finding the covariance when $h = 1$ gives:
\[\begin{aligned}
  Cov\left( {{x_t},{x_{t - 1}}} \right) &= Cov\left( {\theta {w_{t - 1}} + {w_t},\theta {w_{t - 2}} + {w_{t - 1}}} \right) \hfill \\
   &= Cov\left( {\theta {w_{t - 1}},{w_{t - 1}}} \right) \hfill \\
   &= \theta {\sigma ^2} \hfill \\
\end{aligned} \]

Finding the variance (e.g. $h=0$) gives:
\[\begin{aligned}
  Cov\left( {{x_t},{x_t}} \right) &= Cov\left( {\theta {w_{t - 1}} + {w_t},\theta {w_{t - 1}} + {w_t}} \right) \hfill \\
   &= {\theta ^2}Cov\left( {{w_{t - 1}},{w_{t - 1}}} \right) + \underbrace {2\theta Cov\left( {{w_{t - 1}},{w_t}} \right)}_{ = 0} + Cov\left( {{w_t},{w_t}} \right) \hfill \\
   &= {\theta ^2}{\sigma ^2} + {\sigma ^2} \hfill \\
   &= {\sigma ^2}\left( {1 + {\theta ^2}} \right) \hfill \\ 
\end{aligned} \]

This gives us the MA(1) ACF of:
\[\rho \left( h \right) = \left\{ {\begin{array}{*{20}{c}}
  1&{h = 0} \\ 
  {\frac{\theta }{{{\theta ^2} + 1}}}&{h =  \pm 1} 
\end{array}} \right.\]

With this in mind, let's solve for possible $\theta$ values:
\[\begin{aligned}
  \rho \left( 1 \right) &= \frac{\theta }{{{\theta ^2} + 1}} \hfill \\
   \Rightarrow \theta  &= \left( {{\theta ^2} + 1} \right)\rho \left( 1 \right) \hfill \\
  \theta  &= \rho \left( 1 \right){\theta ^2} + \rho \left( 1 \right) \hfill \\
  0 &= \rho \left( 1 \right){\theta ^2} - \theta  + \rho \left( 1 \right) \hfill \\ 
\end{aligned} \]

Yuck, that looks nasty. Let's dig out an ol' friend from middle school known as the quadratic formula:

\[\theta  = \frac{{ - b \pm \sqrt {{b^2} - 4ac} }}{{2a}}\]

Applying the quadratic formula leads to:

\[\begin{aligned}
  a &= \rho \left( h \right), b = -1, c = \rho \left( h \right) \\
  \theta  &= \frac{{1 \pm \sqrt {{1^2} - 4\rho \left( h \right)\rho \left( h \right)} }}{{2\rho \left( h \right)}} \hfill \\
  \theta  &= \frac{{1 \pm \sqrt {1 - 4{{\left[ {\rho \left( h \right)} \right]}^2}} }}{{2\rho \left( h \right)}} \hfill \\
\end{aligned} \]

Thus, we have two possibilities:
\[\begin{aligned}
  {\theta _1} &= \frac{{1 + \sqrt {1 - 4{{\left[ {\rho \left( h \right)} \right]}^2}} }}{{2\rho \left( h \right)}} \hfill \\
  {\theta _2} &= \frac{{1 - \sqrt {1 - 4{{\left[ {\rho \left( h \right)} \right]}^2}} }}{{2\rho \left( h \right)}} \hfill \\ 
  \end{aligned}\]
  
To ensure invertibility, we mandate that $\left| {\rho \left( 1 \right)} \right| < \frac{1}{2}$. Thus, we opt for ${\theta _2}$.

So, our estimator is: \[\hat \theta  = \frac{{1 - \sqrt {1 - 4{{\left[ {\hat \rho \left( 1 \right)} \right]}^2}} }}{{2\hat \rho \left( 1 \right)}}\]

Furthermore, it can be shown that:

\[\sqrt T \left( {\hat \theta  - \theta } \right)\mathop  \to \limits_{T \to \infty }^L N\left( {0 ,\frac{{1 + {\theta ^2} + 4{\theta ^4} + {\theta ^6} + {\theta ^8}}}{{{{\left( {1 - {\theta ^2}} \right)}^2}}}} \right)\]

So, this is not a really optimal estimator...

## Prediction (Forecast)

