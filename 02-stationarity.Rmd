# Autocorrelation and Stationarity

> "I have seen the future and it is very much like the present, only longer."
>
> --- Kehlog Albran, The Profit

After reading this chapter you will be able to:

- Describe independent and dependent data
- Interpret a processes ACF and CCF.  
- Understand the notion of stationarity.
- Differentiate between Strong and Weak stationarity.
- Judge whether a process is stationary. 

## Dependency

Generally speaking, there is a dependence that within the sequence of random variables.

Recall the difference between independent and dependent data:

*Definition:* **Independence**

$X_1, X_2, \ldots, X_T$ are independent and identically distributed if and only if

\begin{equation}
P\left(X_1 \le x_1, X_2 \le x_2,\ldots, X_{T} \le x_T \right) = P\left(X_1 \le x_1\right) P\left(X_2 \le x_2\right) \cdots P\left(X_{T} \le x_T \right) \label{eq:independent}
\end{equation}

for any $T \ge 2$ and $x_1, \ldots, x_T \in \mathbb{R}$.

*Definition:* **Dependence**

$X_1, X_2, \ldots, X_T$ are identically distributed but dependent, then 

\begin{equation}
\left| {P\left( {{X_1} < {x_1},{X_2} < {x_2}, \ldots ,{X_T} < {x_T}} \right) - P\left( {{X_1} < {x_1}} \right)P\left( {{X_2} < {x_2}} \right) \cdots P\left( {{X_T} < {x_T}} \right)} \right| \ne 0 \label{eq:dependent}
\end{equation}

for some $x_1, \ldots, x_T \in \mathbb{R}$.

### Measuring (Linear) Dependence

There are many forms of dependency...

![dependency](images/1280px-Correlation_examples2.png)

However, the methods, covariance and correlation, that we will be using are specific to measuring linear dependence. As a result, these tools are less helpful to measure monotonic dependence and they are much less helpful to measure nonlinearly dependence.

#### Autocovariance Function 

Dependence between $T$ different RV is difficult to measure in one shot! So we consider just two random variables, $X_t$ and $X_{t+h}$. Then one
(linear) measure of dependence is the covariance between $\left(X_t , X_{t+h}\right)$. Since $X$ is the same RV observed at two different time
points, the covariance between $X_t$ and $X_{t+h}$ is defined as the Autocovariance.

*Definition:* **Autocovariance Function**

The **Autocovariance Function** is defined as the second moment product

\[{\gamma _x}\left( {t,t+h} \right) = \operatorname{cov} \left( {{x_t},{x_{t+h}}} \right) = E\left[ {\left( {{x_t} - {\mu _t}} \right)\left( {{x_{t+h}} - {\mu _{t+h}}} \right)} \right]\]

for all $t$ and $t+h$. 

The notation used above corresponds to:

\[\begin{aligned}
  \operatorname{cov} \left( {{X_t},{X_{t+h}}} \right) &= E\left[ {{X_t}{X_{t+h}}} \right] - E\left[ {{X_t}} \right]E\left[ {{X_{t+h}}} \right] \\
  E\left[ {{X_t}} \right] &= \int\limits_{ - \infty }^\infty  {x \cdot {f_x}\left( x \right)dx}  \\
  E\left[ {{X_t}{X_{t+h}}} \right] &= \int\limits_{ - \infty }^\infty  {\int\limits_{ - \infty }^\infty  {{x_1}{x_2} \cdot f\left( {{x_1},{x_2}} \right)d{x_1}d{x_2}} }  \\ 
\end{aligned} \]

We normally drop the subscript referring to the time series if it is clear to the time series the autocovariance function is referencing.
e.g. ${\gamma _x}\left( {t,t+h} \right) = {\gamma}\left( {t,t+h} \right)$.

The more commonly used formulation for weakly stationary processes (more next section) is:
\[\gamma \left( {{X_t},{X_{t + h}}} \right) = \operatorname{cov} \left( {{X_t},{X_{t+h}}} \right) = \gamma \left( {h} \right)\]

A few other notes: 

1. The covariance function is symmetric. That is, ${\gamma}\left( {t,t+h} \right) = {\gamma}\left( {t+h,t} \right)$
2. Just as any covariance, the ${\gamma}\left( {t,t+h} \right)$ is "scale dependent", ${\gamma}\left( {t,t+h} \right) \in \mathbb{R}$, or $-\infty \le {\gamma}\left( {t,t+h} \right) \le +\infty$
    1. If $\left| {\gamma}\left( {t,t+h} \right) \right|$ is "close" to 0, then they are "less dependent"
    2. If $\left| {\gamma}\left( {t,t+h} \right) \right|$ is "far" from 0, $X_t$ and $X_{t+h}$ are "more dependent".
3. ${\gamma}\left( {t,t+h} \right)=0$ does not imply $X_t$ and $X_{t+h}$ are independent.
4. If $X_t$ and $X_{t+h}$ are joint normally distributed then $X_t$ and $X_{t+h}$ are independent.


#### Autocorrelation Function (ACF)

A "simplified" $\gamma \left(t, t+h\right)$ is the Autocorrelation (AC) between $X_t$ and $X_{t+h}$, which is scale free! It is simply defined as
$$\rho \left( {{X_t},{X_{t + h}}} \right) = Corr\left( {{X_t},{X_{t + h}}} \right) = \frac{{Cov\left( {{X_t},{X_{t + h}}} \right)}}{{{\sigma _{{X_t}}}{\sigma _{{X_{t + h}}}}}}$$

The more commonly used formulation for weakly stationary processes (more next section) is:
\[\rho \left( {{X_t},{X_{t + h}}} \right) = \frac{{Cov\left( {{X_t},{X_{t + h}}} \right)}}{{{\sigma _{{X_t}}}{\sigma _{{X_{t + h}}}}}} = \frac{{\gamma \left( h \right)}}{{\gamma \left( 0 \right)}} = \rho \left( h \right)\]

Therefore, the autocorrelation function is only a function of the lag $h$ between observations.

Just as any correlation:

1. $\rho \left( {{X_t},{X_{t + h}}} \right)$ is scale free
2. $\rho \left( {{X_t},{X_{t + h}}} \right)$ is closer to $\pm 1 \Rightarrow \left({ X_t, X_{t+h} } \right)$ "more dependent."

Remember... When using correlation....

![correlation_sillies](images/correlation-does-not-imply-causation.jpg)

#### Cross dependency functions

Consider two time series: $\left(X_t \right)$ and $\left(Y_t \right)$.

Then the cross-covariance function between two series $\left(X_t \right)$ and $\left(Y_t \right)$ is:

\[{\gamma _{XY}}\left( {t,t + h} \right) = \operatorname{cov} \left( {{X_t},{Y_{t + h}}} \right) = E\left[ {\left( {{X_t} - E\left[ {{X_t}} \right]} \right)\left( {{Y_{t + h}} - E\left[ {{Y_{t + h}}} \right]} \right)} \right]\]

The cross-correlation function is given by
\[{\rho _{XY}}\left( {t,t + h} \right) = Corr\left( {{X_t},{Y_{t + h}}} \right) = \frac{{{\gamma _{XY}}\left( {t,t + h} \right)}}{{{\sigma _{{X_t}}}{\sigma _{{Y_{t + h}}}}}}\]

These ideas can extended beyond the bivariate case to a general multivariate setting.

#### Sample Autocovariance and Autocorrelation Functions

*Definition:* **Sample Autocovariance Function**

The **Sample Autocovariance Function** is defined as:

\[\hat \gamma \left( h \right) = \frac{1}{T}\sum\limits_{t = 1}^{T - h} {\left( {{X_t} - \bar X} \right)\left( {{X_{t + h}} - \bar X} \right)} \]

*Definition:* **Sample Autocorrelation function**

The **Sample Autocorrelation function** is defined as:

\[\hat \rho \left( h \right) = \frac{{\hat \gamma \left( h \right)}}{{\hat \gamma \left( 0 \right)}}\]


## Stationarity

There are two kinds of stationarity: *Strong* and *Weak* Stationarity. 
These types of stationarity are **not equivalent** and the presence of 
**one kind of stationarity does not imply the other**. 
That is, a time series can be strongly stationary but not weakly stationary and
vice versa. In very rare cases, a time series can be both strong and weakly
stationary. The most common form of stationarity is that of weakly stationarity.

### Weak Stationarity

*Definition:* **Weak Stationarity** or **Second-order Stationarity**

The mean and autocovariance of the stochastic process are finite and invariant under a shift in time, i.e.

$$\begin{aligned}
E\left[X_t \right] &= \mu_t = \mu < \infty \\
cov\left(X_t, X_{t+h} \right) &= cov\left(X_{t+k}, X_{t+h+k}\right)  \\
 &= \gamma \left(h\right)
\end{aligned}$$

### Strong Stationarity

*Definition:* **Strong Stationarity** or **Strict Stationarity**

The joint probability distribution of $\left(X_t \right)$, $t \in N$ is invariant under a shift in time, i.e.
$$P\left(X_t \le x_1, \ldots, X_{t+k} \le x_k \right) = P\left(X_{t+h} \le x_1, \ldots, X_{t+h+k} \le x_k \right)$$
for any time shift $h$ and any $x_1, x_2, \ldots, x_k$ belong to the domain of $X_{t_1}, X_{t_2}, \ldots, X_{t_k}$ respectively.

So to summarize, we have **weak stationarity** that relies on how *separated each observation* is rather than their location in time. On the flip side, we have **strict stationarity** that relies on the *location in time* instead of how separated observations are.

Stationarity of $X_t$ matters, because it provides the framework in which averaging makes sense. Unless properties like mean and covariance are either fixed or "evolve" in a known manner, we cannot average the observed data.

With this being said, here are a few examples of stationarity:

1. $X_t \sim Cauchy$ is strictly stationary but **NOT** weakly stationary.
    * The strong stationarity exists due to the symmetric properties of the distribution.
    * It cannot be weakly stationary because it has an infinite variance!
2. $X_{2t} = U_{2t}, X_{2t+1} = V_{2t+1} \forall t$ where ${U_t}\mathop \sim \limits^{iid} N\left( {1,1} \right)$ and ${V_t}\mathop \sim \limits^{iid} Exponential\left( 1 \right)$ is weakly stationary but **NOT** strictly stationary.
    * The weak stationary exists since the mean is constant ($\mu = 1$) and the variance does not depend on time ($\sigma ^2 = 1$).
    * It cannot be strongly stationary due to values not aligning in time. 

Regarding white noises, we  can obtain different levels of stationarity depending on the assumption:

1. If $X_t \sim WN$, e.g. **uncorrelated observations** with a finite variance, then it is weakly stationary but **NOT** strictly stationary.
2. If $X_t \mathop \sim \limits^{iid} NWN$, e.g. **normally distributed independent observations** with a finite variance, then it is weakly stationary *AND* strictly stationary.

The autocovariance of weakly stationary processes has the following properties:

1. $\gamma \left(0\right) = var\left[X_t \right] \ge 0$ (variance) 
2. $\gamma \left(h\right) = \gamma \left(-h\right)$ (function is even / symmetric)
3. $\left| \gamma \left(h\right) \right| \le \gamma \left( 0 \right) \forall h.$

We obtain these properties through:

1. \[\gamma \left( 0 \right) = Var\left( {{x_t}} \right) = E\left[ {{{\left( {{x_t} - \mu } \right)}^2}} \right] = \sum\limits_{t = 1}^T {{p_t}{{\left( {{x_t} - \mu } \right)}^2}}  = {p_1}{\left( {{x_1} - \mu } \right)^2} +  \cdots  + {p_T}{\left( {{x_T} - \mu } \right)^2} \ge 0\]
2. \[\begin{aligned}
  \gamma \left( h \right) &= \gamma \left( {t + h - t} \right) \\
   &= E\left[ {\left( {{x_{t + h}} - \mu } \right)\left( {{x_t} - \mu } \right)} \right] \\
   &= E\left[ {\left( {{x_t} - \mu } \right)\left( {{x_{t + h}} - \mu } \right)} \right] \\
   &= \gamma \left( {t - \left( {t + h} \right)} \right) \\
   &= \gamma \left( { - h} \right) 
\end{aligned}\]
3. Using the Cauchy-Schwarz Inequality, ${\left( {E\left[ {XY} \right]} \right)^2} \le E\left[ {{X^2}} \right]E\left[ {{Y^2}} \right]$, we have:
\[\begin{aligned}
  {\left( {\left| {\gamma \left( h \right)} \right|} \right)^2} &= {\left( {\gamma \left( h \right)} \right)^2}  \\
   &= {\left( {E\left[ {\left( {{x_t} - \mu } \right)\left( {{x_{t + h}} - \mu } \right)} \right]} \right)^2}  \\
   &\le E\left[ {{{\left( {{x_t} - \mu } \right)}^2}} \right]E\left[ {{{\left( {{x_{t + h}} - \mu } \right)}^2}} \right]  \\
   &= {\left( {\gamma \left( 0 \right)} \right)^2}  \\
  {\left( {\gamma \left( h \right)} \right)^2} &\le {\left( {\gamma \left( 0 \right)} \right)^2}  \\
  \left| {\gamma \left( h \right)} \right| &\le \gamma \left( 0 \right) 
\end{aligned}\]

### Is it weakly stationary?

In order to verify if it is weakly stationary, we must make sure the time series satisfies:

1. $E\left[y_t \right] = \mu_t = \mu < \infty$
2. $cov\left(y_t, Y_{t+h} \right) = \gamma \left(h\right)$


### Is a random walk weakly stationary?

First up, is a RW stationary? By intuition, the answer should be "no" since there is a randomness component that cannot be accounted for when looking for a pattern. But, we need to prove that.

1. \[\begin{aligned}
  E\left[ {{y_t}} \right] &= E\left[ {{y_{t - 1}} + {w_t}} \right] \\
   &= E\left[ {\sum\limits_{i = 1}^t {{w_t}}  + {Y_0}} \right] \\
   &= E\left[ {\sum\limits_{i = 1}^t {{w_t}} } \right] + {Y_0} \\
   &= 0 + c \\
   &= c \\ 
\end{aligned} \] Note, the mean here is constant since it depends only on the value of the first term in the sequence.
2. \[\begin{aligned}
  Var\left( {{y_t}} \right) &= Var\left( {\sum\limits_{i = 1}^t {{w_t}}  + {Y_0}} \right) \\
   &= Var\left( {\sum\limits_{i = 1}^t {{w_t}} } \right) + \underbrace {Var\left( {{Y_0}} \right)}_{ = 0{\text{ constant}}} \\
   &= \sum\limits_{i = 1}^t {Var\left( {{w_t}} \right)}  \\
   &= t\sigma _w^2 \\
   &\Rightarrow Cov\left( {{y_t},{y_{t + h}}} \right) \ne \gamma \left( h \right) \\ 
\end{aligned}\] Alas, the variance has a dependence on time. This causes the $Var\left(y_t\right) \ge \infty$ as $t \rightarrow \infty$. As a result, the process is not weakly stationary.

Continuing on just to obtain the covariance, we have:
\[\begin{aligned}
  \gamma \left( h \right) &= Cov\left( {{y_t},{y_{t + h}}} \right) \\
   &= Cov\left( {\sum\limits_{i = 1}^t {{w_i}} ,\sum\limits_{j = 1}^{t + h} {{w_j}} } \right) \\
   &= Cov\left( {\sum\limits_{i = 1}^t {{w_i}} ,\sum\limits_{j = 1}^t {{w_j}} } \right) \\
   &= \min \left( {t,t + h} \right)\sigma _w^2 \\
   &= \left( {t + \min \left( {0,h} \right)} \right)\sigma _w^2 \\ 
\end{aligned} \]

### Is an MA(1) Stationary?

1. \[\begin{aligned}
  E\left[ {{y_t}} \right] &= E\left[ {{\theta _1}{w_{t - 1}} + {w_t}} \right] \\
   &= {\theta _1}E\left[ {{w_{t - 1}}} \right] + E\left[ {{w_t}} \right] \\
   &= 0 \\
    \end{aligned}\] The mean is constant over time. So the first criterion is okay.
2. \[\begin{aligned}
  Cov\left( {{y_t},{y_{t + h}}} \right) &= E\left[ {\left( {{y_t} - E\left[ {{y_t}} \right]} \right)\left( {{y_{t + h}} - E\left[ {{y_{t + h}}} \right]} \right)} \right] \\
   &= E\left[ {{y_t}{y_{t + h}}} \right] - \underbrace {E\left[ {{y_t}} \right]}_{ = 0}\underbrace {E\left[ {{y_{t + h}}} \right]}_{ = 0} \\
   &= E\left[ {\left( {{\theta _1}{w_{t - 1}} + {w_t}} \right)\left( {{\theta _1}{w_{t + h - 1}} + {w_{t + h}}} \right)} \right] \\
   &= E\left[ {\theta _1^2{w_{t - 1}}{w_{t + h - 1}} + \theta {w_t}{w_{t + h}} + {\theta _1}{w_{t - 1}}{w_{t + h}} + {w_t}{w_{t + h}}} \right] \\
   &\\
  E\left[ {{w_t}{w_{t + h}}} \right] &= \operatorname{cov} \left( {{w_t},{w_{t + h}}} \right) + E\left[ {{w_t}} \right]E\left[ {{w_{t + h}}} \right] = {1_{\left\{ {h = 0} \right\}}}\sigma _w^2 \\
   \\
   &\Rightarrow Cov\left( {{y_t},{y_{t + h}}} \right) = \left( {\theta _1^2{1_{\left\{ {h = 0} \right\}}} + {\theta _1}{1_{\left\{ {h = 1} \right\}}} + {\theta _1}{1_{\left\{ {h =  - 1} \right\}}} + {1_{\left\{ {h = 0} \right\}}}} \right)\sigma _w^2 \\
  \gamma \left( h \right) &= \left\{ {\begin{array}{*{20}{c}}
  {\left( {\theta _1^2 + 1} \right)\sigma _w^2}&{h = 0} \\ 
  {{\theta _1}\sigma _w^2}&{\left| h \right| = 1} \\ 
  0&{\left| h \right| > 1} 
\end{array}} \right. 
\end{aligned} \] Note, the autocovariance function does not depend on time. Thus, the second weakly stationary criterion is satisfied.

The MA(1) process is weakly stationary since both the mean and variance are constant over time.

As a bonus, note that we also can easily obtain the autocorrelation function (ACF)
$$\Rightarrow \rho \left( h \right) = \left\{ {\begin{array}{*{20}{c}}
  1&{h = 0} \\ 
  {\frac{{{\theta _1}\sigma _w^2}}{{\left( {\theta _1^2 + 1} \right)\sigma _w^2}} = \frac{{{\theta _1}}}{{\theta _1^2 + 1}}}&{\left| h \right| = 1} \\ 
  0&{\left| h \right| > 1} 
\end{array}} \right.$$

### Is an AR(1) Stationary?

Consider the AR(1) process given as:
$${y_t} = {\phi _1}{y_{t - 1}} + {w_t} \text{, where } {w_t}\mathop \sim \limits^{iid} WN\left( {0,\sigma _w^2} \right)$$

This process was shown to simplify to: 

$$y_t = {\phi ^t}{y_0} + \sum\limits_{i = 0}^{t - 1} {\phi _1^i{w_{t - i}}}$$

In addition, we add the requirement that $\left| \phi _1 \right| < 1$. This requirement allows for the process to be stationary. If $\phi _1 \ge 1$, the process would not converge.  This way the process will be able to be written as a geometric series that converges:
\[\sum\limits_{k = 0}^\infty  {{r^k}}  = \frac{1}{{1 - r}},{\text{ }}\left| r \right| < 1\]

Next, we demonstrate how crucial this property is: 

\[\begin{aligned}
  \mathop {\lim }\limits_{t \to \infty } E\left[ {{y_t}} \right] &= \mathop {\lim }\limits_{t \to \infty } E\left[ {{\phi ^t}{y_0} + \sum\limits_{i = 0}^{t - 1} {\phi _1^i{w_{t - i}}} } \right] \\
   &= \mathop {\lim }\limits_{t \to \infty } \underbrace {{\phi ^t}{y_0}}_{\left| \phi  \right| < 1 \Rightarrow t \to \infty {\text{  = 0}}} + \sum\limits_{i = 0}^{t - 1} {\phi _1^i\underbrace {E\left[ {{w_{t - i}}} \right]}_{ = 0}}  \\
   &= 0 \\
  \mathop {\lim }\limits_{t \to \infty } Var\left( {{y_t}} \right) &= \mathop {\lim }\limits_{t \to \infty } Var\left( {{\phi ^t}{y_0} + \sum\limits_{i = 0}^{t - 1} {\phi _1^i{w_{t - i}}} } \right) \\
   &= \mathop {\lim }\limits_{t \to \infty } \underbrace {Var\left( {{\phi ^t}{y_0}} \right)}_{ = 0{\text{ since constant}}} + Var\left( {\sum\limits_{i = 0}^{t - 1} {\phi _1^i{w_{t - i}}} } \right) \\
   &= \mathop {\lim }\limits_{t \to \infty } \sum\limits_{i = 0}^{t - 1} {\phi _1^{2i}Var\left( {{w_{t - i}}} \right)}  \\
   &= \mathop {\lim }\limits_{t \to \infty } \sigma _w^2\sum\limits_{i = 0}^{t - 1} {\phi _1^{2i}}  \\
   &= \sigma _w^2 \cdot  
  \underbrace {\frac{1}{{1 - {\phi ^2}}}}_{\begin{subarray}{l} 
  {\text{Geometric Series}} 
\end{subarray}}
\end{aligned} \]

This leads us to being able to conclude the autocovariance function is:
\[\begin{aligned}
  Cov\left( {{y_t},{y_{t + h}}} \right) &= Cov\left( {{y_t},\phi {y_{t + h - 1}} + {w_{t + h}}} \right) \\
   &= Cov\left( {{y_t},\phi {y_{t + h - 1}}} \right) \\
   &= Cov\left( {{y_t},{\phi ^{\left| h \right|}}{y_t}} \right) \\
   &= {\phi ^{\left| h \right|}}Cov\left( {{y_t},{y_t}} \right) \\
   &= {\phi ^{\left| h \right|}}Var\left( {{y_t}} \right) \\
   &= {\phi ^{\left| h \right|}}\frac{{\sigma _w^2}}{{1 - \phi _1^2}} \\ 
\end{aligned} \]

Both the mean and autocovariance function do not depend on time and, thus, the AR(1) process is stationary if $\left| \phi _1 \right| < 1$. 

If we assume that the AR(1) process is stationary, we can derive the mean and variance in another way. Without a loss of generality, we'll assume $y_0 = 0$.

Therefore:

\[\begin{aligned}
  {y_t} &= {\phi _t}{y_{t - 1}} + {w_t} \\
   &= {\phi _1}\left( {{\phi _1}{y_{t - 2}} + {w_{t - 1}}} \right) + {w_t} \\
   &= \phi _1^2{y_{t - 2}} + {\phi _1}{w_{t - 1}} + {w_t} \\
   &\vdots  \\
   &= \sum\limits_{i = 0}^{t - 1} {\phi _1^i{w_{t - i}}}  \\
   & \\
  E\left[ {{y_t}} \right] &= E\left[ {\sum\limits_{i = 0}^{t - 1} {\phi _1^i{w_{t - i}}} } \right] \\
   &= \sum\limits_{i = 0}^{t - 1} {\phi _1^i\underbrace {E\left[ {{w_{t - i}}} \right]}_{ = 0}}  \\
   &= 0 \\
   &\\
  Var\left( {{y_t}} \right) &= E\left[ {{{\left( {{y_t} - E\left[ {{y_t}} \right]} \right)}^2}} \right] \\
   &= E\left[ {y_t^2} \right] - {\left( {E\left[ {{y_t}} \right]} \right)^2} \\
   &= E\left[ {y_t^2} \right] \\
   &= E\left[ {{{\left( {{\phi _1}{y_{t - 1}} + {w_t}} \right)}^2}} \right] \\
   &= E\left[ {\phi _1^2y_{t - 1}^2 + w_t^2 + 2{\phi _1}{y_t}{w_t}} \right] \\
   &= \phi _1^2E\left[ {y_{t - 1}^2} \right] + \underbrace {E\left[ {w_t^2} \right]}_{ = \sigma _w^2} + 2{\phi _1}\underbrace {E\left[ {{y_t}} \right]}_{ = 0}\underbrace {E\left[ {{w_t}} \right]}_{ = 0} \\
   &= \underbrace {\phi _1^2Var\left( {{y_{t - 1}}} \right) + \sigma _w^2 = \phi _1^2Var\left( {{y_t}} \right) + \sigma _w^2}_{{\text{Assume stationarity}}} \\
  Var\left( {{y_t}} \right) &= \phi _1^2Var\left( {{y_t}} \right) + \sigma _w^2 \\
  Var\left( {{y_t}} \right) - \phi _1^2Var\left( {{y_t}} \right) &= \sigma _w^2 \\
  Var\left( {{y_t}} \right)\left( {1 - \phi _1^2} \right) &= \sigma _w^2 \\
  Var\left( {{y_t}} \right) &= \frac{{\sigma _w^2}}{{1 - \phi _1^2}} \\ 
\end{aligned} \]

## Joint Stationarity

Two time series, say $\left(X_t \right)$ and $\left(Y_t\right)$, are said to be jointly stationary if they are each stationary, and the cross-covariance function

\[{\gamma _{XY}}\left( {t,t + h} \right) = Cov\left( {{X_t},{Y_{t + h}}} \right) = {\gamma _{XY}}\left( h \right)\]

is a function only of lag $h$.

The cross-correlation function for jointly stationary times can be expressed as:

\[{\rho _{XY}}\left( {t,t + h} \right) = \frac{{{\gamma _{XY}}\left( {t,t + h} \right)}}{{{\sigma _{{X_t}}}{\sigma _{{Y_{t + h}}}}}} = \frac{{{\gamma _{XY}}\left( h \right)}}{{{\sigma _{{X_t}}}{\sigma _{{Y_{t + h}}}}}} = {\rho _{XY}}\left( h \right)\]

