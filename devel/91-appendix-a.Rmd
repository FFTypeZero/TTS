# Proofs

## Proof of Theorem 1

Before provifing the proof of Theorem 1 we state and prove the following lemma:

**Lemma A1:** Let 
\begin{equation}
X_t = \mu + \sum\limits_{j = -\infty}^{\infty} \psi_j W_{t-j},
\end{equation}
where $(W_t)$ is a strong white process with variance $\sigma^2$, and the coefficients satisfying $\sum \, |\psi_j| < \infty$. Then, we have
    \begin{equation*}
        T^{\frac{1}{2}}[\tilde{\gamma} \left( h \right) - \hat \gamma \left( h \right)] = o_p(1).
    \end{equation*}

*Proof:* By Markov inequality, we have:

<!-- I changed Markov inequ from < to <=, correct? -->
\begin{equation*}
\mathbb{P}\left( |T^{\frac{1}{2}}[\tilde{\gamma} \left( h \right) - \hat \gamma \left( h \right)]| \geq \epsilon \right) \leq \frac{\mathbb{E}|T^{\frac{1}{2}}[\tilde{\gamma} \left( h \right) - \hat \gamma \left( h \right)]|}{\epsilon},
\end{equation*}
for any $\epsilon > 0$.

Thus, it is enough to show that $\mathbb{E}|T^{\frac{1}{2}}[\tilde{\gamma} \left( h \right) - \hat \gamma \left( h \right)]| \to 0$ as $T \to \infty$.

By the definitions of $\tilde{\gamma} \left( h \right)$ and $\hat \gamma \left( h \right)$,
\begin{equation*}
\begin{aligned}
T^{\frac{1}{2}}[\tilde{\gamma} \left( h \right) - \hat \gamma \left( h \right)] &= \frac{T^{\frac{1}{2}}}{T} \sum_{t = T-h+1}^{T}(X_t - \mu)(X_{t+h} - \mu) \\
&+ \frac{T^{\frac{1}{2}}}{T} \sum_{t = 1}^{T-h}\left[(X_t - \mu)(X_{t+h} - \mu) - (X_t - \bar{X})(X_{t+h} - \bar{X})\right]\\
&= \frac{T^{\frac{1}{2}}}{T} \sum_{t = T-h+1}^{T}(X_t - \mu)(X_{t+h} - \mu) \\
&+ \frac{T^{\frac{1}{2}}}{T} \sum_{t = 1}^{T-h}\left[(X_t - \mu)(X_{t+h} - \mu) - (X_t - \mu + \mu - \bar{X})(X_{t+h} - \mu + \mu - \bar{X})\right]\\
&= \frac{T^{\frac{1}{2}}}{T} \sum_{t = T-h+1}^{T}(X_t - \mu)(X_{t+h} - \mu) \\
&+ \frac{T^{\frac{1}{2}}}{T} \sum_{t = 1}^{T-h}\left[(\bar{X} - \mu)(X_{t+h} - \mu) + (\bar{X} - \mu)(X_t - \mu) - (\bar{X} - \mu)^2\right],
\end{aligned}
\end{equation*}
where $\bar{X} = \frac{1}{T}\sum_{t=1}^T X_t = \mu + \frac{1}{T}\sum_{t=1}^T\sum_{j=-\infty}^{\infty} \psi_j w_{t-j} = \mu + \frac{1}{T} \sum_{j = -\infty}^{\infty} \sum_{t=1}^T \psi_j w_{t-j}$.

Then,
\begin{equation*}
\begin{aligned}
\mathbb{E}|T^{\frac{1}{2}}[\tilde{\gamma} \left( h \right) - \hat \gamma \left( h \right)]| &\leq \frac{T^{\frac{1}{2}}}{T} \sum_{t = T-h+1}^{T} \mathbb{E}|(X_t - \mu)(X_{t+h} - \mu)|\\
&+ \frac{T^{\frac{1}{2}}}{T} \sum_{t = 1}^{T-h} \left[\mathbb{E} |(\bar{X} - \mu)(X_{t+h} - \mu)| + \mathbb{E} |(\bar{X} - \mu)(X_t - \mu)| - \mathbb{E}[(\bar{X} - \mu)^2]\right]
\end{aligned}
\end{equation*}

Next, we consider each term of the above equation. For the first term, since $(X_t - \mu)(X_{t+h} - \mu) = \sum_{j = -\infty}^{\infty} \psi_j w_{t-j} \sum_{i=-\infty}^{\infty} \psi_i w_{t+h-i}$, and $\mathbb{E}[w_iw_j] \neq 0$ only if $i = j$. Thus we have
\begin{equation*}
\mathbb{E}|(X_t - \mu)(X_{t+h} - \mu)| \leq \sigma^2 \sum_{i = -\infty}^{\infty}|\psi_i \psi_{i+h}|.
\end{equation*}

Similarly, for the second term we have
\begin{equation*}
\mathbb{E}|(\bar{X} - \mu)(X_{t+h} - \mu)| \leq \frac{\sigma^2}{T} \sum_{i = -\infty}^{\infty}|\psi_i \psi_{i+h}|.
\end{equation*}

The third term
\begin{equation*}
\mathbb{E}|(\bar{X} - \mu)(X_t - \mu)| = \frac{\sigma^2}{T} \sum_{i = -\infty}^{\infty}\psi_i^2.
\end{equation*}

The fourth term
\begin{equation*}
\mathbb{E}[(\bar{X} - \mu)^2] = \frac{\sigma^2}{T} \sum_{i = -\infty}^{\infty}\psi_i^2.
\end{equation*}

Then the third term and the fourth term will be cancelled out, we have 

\begin{equation*}
\begin{aligned}
\mathbb{E}|T^{\frac{1}{2}}[\tilde{\gamma} \left( h \right) - \hat \gamma \left( h \right)]| &\leq \frac{T^{\frac{1}{2}}}{T} \sum_{t = T-h+1}^{T} \mathbb{E}|(X_t - \mu)(X_{t+h} - \mu)|\\
&+ \frac{T^{\frac{1}{2}}}{T} \sum_{t = 1}^{T-h} \left[\mathbb{E} |(\bar{X} - \mu)(X_{t+h} - \mu)|\right]\\
&\leq \frac{T^{\frac{1}{2}}}{T} h \sigma^2 \sum_{i = -\infty}^{\infty}|\psi_i \psi_{i+h}| + \frac{T^{\frac{1}{2}}}{T^2} (T-h) \sigma^2 \sum_{i = -\infty}^{\infty}|\psi_i \psi_{i+h}|\\
&= \frac{T^{\frac{1}{2}}}{T^2} (Th + T - h) \sigma^2 \sum_{i = -\infty}^{\infty}|\psi_i \psi_{i+h}| \to 0,
\end{aligned}
\end{equation*}
as $T \to \infty$.

Thus we conclude that
\begin{equation*}
T^{\frac{1}{2}}[\tilde{\gamma} \left( h \right) - \hat \gamma \left( h \right)] = o_p(1).
\end{equation*}

Returning to the proof of Theorem 1, we let $X_t = W_t + \mu$, where $\mu < \infty$ and $(W_t)$ is a strong white noise process with variance $\sigma^2$ and finite fourth moment.

Next, we consider the sample autocovariance function computed on $(X_t)$, i.e.

\begin{equation*}
    \hat \gamma \left( h \right) = \frac{1}{T}\sum\limits_{t = 1}^{T - h} {\left( {{X_t} - \bar X} \right)\left( {{X_{t + h}} - \bar X} \right)}.
\end{equation*}

For this equation, it is clear that $\hat \gamma \left( 0 \right)$ and $\hat \gamma \left( h \right)$ (with $h > 0$) are two statistics involving sums of different lengths. As we will see, this prevents us from using directely the multivariate central limit theorem on the vector $[ \hat \gamma \left( h \right) \;\;\; \hat \gamma \left( h \right) ]^T$. However, the lag $h$ is fixed and therefore the difference in the number of elements of both sums is asymptotically negligible. Therefore, we define a new statistic 

\begin{equation*}
    \tilde{\gamma} \left( h \right) = \frac{1}{T}\sum\limits_{t = 1}^{T} {\left( {{X_t} - \mu} \right)\left( {{X_{t + h}} - \mu} \right)},
\end{equation*}

which is easier to "handle" and show that $\hat \gamma \left( h \right)$ and $\tilde{\gamma} \left( h \right)$ are asymptotically equivalent in the sense that:

\begin{equation*}
    T^{\frac{1}{2}}[\tilde{\gamma} \left( h \right) - \hat \gamma \left( h \right)] = o_p(1).
\end{equation*}

Therefore $\tilde{\gamma} \left( h \right)$ and $\hat \gamma \left( h \right)$ have the same asymptotic distribution, it is suffice to show the asymptotic distribution of $\tilde{\gamma} \left( h \right)$.

The expectation of $\tilde{\gamma} \left( h \right)$
\begin{equation*}
    \mathbb{E}[\tilde{\gamma} \left( h \right)] = \frac{1}{T}\mathbb{E}\left[\sum\limits_{t = 1}^{T} {\left( {{X_t} - \mu} \right)\left( {{X_{t + h}} - \mu} \right)}\right].
\end{equation*}

By Cauchy–Schwarz inequality and $\var(X_t) = \sigma^2$, we have
\begin{equation*}
    \sum\limits_{t = 1}^{T} {\left( {{X_t} - \mu} \right)\left( {{X_{t + h}} - \mu} \right)} \leq \sum\limits_{t = 1}^{T} {\left( {{X_t} - \mu} \right)^2} \sum\limits_{t = 1}^{T} {\left( {{X_{t + h}} - \mu} \right)^2} < \infty.
\end{equation*}

Then by bounded convergence theorem and $\{X_t\}_1^T$ are independent, we have

\begin{equation*}
    \mathbb{E}[\tilde{\gamma} \left( h \right)] = \frac{1}{T}\left[\sum\limits_{t = 1}^{T} { \mathbb{E}\left( {{X_t} - \mu} \right)\mathbb{E}\left( {{X_{t + h}} - \mu} \right)}\right] =
    \begin{cases}
        \sigma^2, & \text{for } h = 0\\
        0, & \text{for } h \neq 0
    \end{cases}.
\end{equation*}


The variance of $\tilde{\gamma} \left( h \right)$, 

when $h \neq 0$,
\begin{equation*}
    \begin{aligned}
        \var[\tilde{\gamma} \left( h \right)] &= \frac{1}{T^2}\mathbb{E}\left\{\left[\sum\limits_{t = 1}^{T} {\left( {{X_t} - \mu} \right)\left( {{X_{t + h}} - \mu} \right)}\right]^2\right\}\\
        &= \frac{1}{T^2}\mathbb{E}\left\{\left[\sum\limits_{i = 1}^{T} {\left( {{X_i} - \mu} \right)\left( {{X_{i + h}} - \mu} \right)}\right] \left[\sum\limits_{j = 1}^{T} {\left( {{X_j} - \mu} \right)\left( {{X_{j + h}} - \mu} \right)}\right]\right\}\\
        &= \frac{1}{T^2}\mathbb{E}\left[\sum\limits_{i = 1}^{T}\sum\limits_{j = 1}^{T} {\left( {{X_i} - \mu} \right)\left( {{X_{i + h}} - \mu} \right)}{\left( {{X_j} - \mu} \right)\left( {{X_{j + h}} - \mu} \right)}\right].
    \end{aligned}
\end{equation*}

Also by Cauchy–Schwarz inequality and the finite fourth moment assumption, we can use the bounded convergence theorem. And since $\{X_t\}_1^T$ is white noise process, we have 


\begin{equation*}
    \mathbb{E}\left[{\left( {{X_i} - \mu} \right)\left( {{X_{i + h}} - \mu} \right)}{\left( {{X_j} - \mu} \right)\left( {{X_{j + h}} - \mu} \right)}\right] \neq 0
\end{equation*}
only when $i = j$.

Therefore,
\begin{equation*}
    \begin{aligned}
        \var[\tilde{\gamma} \left( h \right)] &= \frac{1}{T^2}\sum\limits_{i = 1}^{T} \mathbb{E}\left[ {\left( {{X_i} - \mu} \right)^2\left( {{X_{i + h}} - \mu} \right)^2}\right]\\
        &= \frac{1}{T^2}\sum\limits_{i = 1}^{T} \mathbb{E}{\left( {{X_i} - \mu} \right)^2\mathbb{E}\left( {{X_{i + h}} - \mu} \right)^2}\\
        &= \frac{1}{T}\sigma^4, \text{for } (h \neq 0).
    \end{aligned}
\end{equation*}

Similarly, when $h = 0$, we have

\begin{equation*}
    \begin{aligned}
        \var[\tilde{\gamma} \left( 0 \right)] &= \frac{1}{T^2}\mathbb{E}\left\{\left[\sum\limits_{t = 1}^{T} {\left( {{X_t} - \mu} \right)^2}\right]^2\right\} - \frac{1}{T^2}\left[\mathbb{E}\sum\limits_{t = 1}^{T} {\left( {{X_t} - \mu} \right)^2}\right]^2\\
        &= \frac{2}{T}\sigma^4, \text{for } (h = 0).
    \end{aligned}
\end{equation*}

Next, since $\{\left( {{X_t} - \mu} \right)\left( {{X_{t + h}} - \mu} \right)\}_1^T$ are iid, we can apply CLT to it, and have
\begin{equation*}
    \sqrt{T}\tilde{\gamma} \left( h \right) = \frac{1}{\sqrt{T}}\sum\limits_{t = 1}^{T} {\left( {{X_t} - \mu} \right)\left( {{X_{t + h}} - \mu} \right)} \overset{\mathcal{D}}{\to} 
    \begin{cases}
        N(\sigma^2, 2\sigma^4), & \text{for } h = 0\\
        N(0, \sigma^4), & \text{for } h \neq 0
    \end{cases}.
\end{equation*}


Therefore, we also have
\begin{equation*}
    \sqrt{T}\hat \gamma \left( h \right) \overset{\mathcal{D}}{\to} 
    \begin{cases}
        N(\sigma^2, 2\sigma^4), & \text{for } h = 0\\
        N(0, \sigma^4), & \text{for } h \neq 0
    \end{cases}.
\end{equation*}

Since we can show that $\hat \gamma \left( 0 \right) \overset{p}{\to} \sigma^2$, by Slutsky Theorem, for $h \neq 0$
\begin{equation*}
    \hat \rho \left( h \right) = \frac{ \hat \gamma \left( h \right)}{\hat \gamma \left( 0 \right)} \overset{\mathcal{D}}{\to} N(0, \frac{1}{T}).
\end{equation*}


