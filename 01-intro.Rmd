# Introduction

```{r introduction_code, echo = FALSE, cache = FALSE}
knitr::read_chunk('code/chapter/01_introduction.R')
```

<!--
> "One damned thing after another." ~ R. A. Fisher
>
-->

>
> *Prévoir consiste à projeter dans l’avenir ce qu’on a perçu dans le passé.* Henri Bergson
>

<!--
>
> *Hâtons-nous; le temps fuit, et nous entraîne avec soi : le moment où je parle est déjà loin de moi*. Nicolas Boileau
>
-->

**Objectives TODO JAMES**

Generally speaking a *time series* (or stochastic process) corresponds to set of "repeated" observations of the same variable such as price of a financial asset or temperature in a given location. In terms of notation a time series is often writen as

 \[\left(X_1, X_2, ..., X_T \right) \;\;\; \text{ or } \;\;\; \left(X_t\right)_{t = 1,...,T}.\]
 
The time index $t$ generally the set $\mathbb{N}$ or $\mathbb{Z}$. When $t \in \mathbb{R}$, a time series becomes a *continuous-time* stochastic process such a Brownian motion. In this class we limit our selves to *discrete-time* processes where a variable is measured sequentially at fixed and equally spaced intervals in time. This implies that we will assume $t$ is not random (the time at which each observation is measure is know) and the time between two consequtive observation is constant. 

Moreover, the term "time series" can, as we discussed, denote a sample or a set of observations but also a probability model for that sample. For example, on of the simplest probability model used in time series analysis is called a *white noise* process and is defined as
\[W_t \mathop \sim \limits^{iid} N(0, \sigma^2).\]
This statement simply means that $(X_t)$ is normally distribtued and independent over time. This model is quite unintersting but as we will very usefull to construct other (more interesting) models. Unlike white noise process, time series are typically *not* independent over time. Suppose that the temperature in Champaign is unusually low, then it is reasonable to assume that tomorrow's temperature will also be low. Such behaviour suggest a dependent over time. The time series methods we will discuss in this class consists parametric models used to characterize (or at least approximate) the joint distribution of $(X_t)$. Often, time series models can be decomposed in what we called a *signal*, say $(Y_t)$ and a *noise*, say $(W_t)$, leading to the model
 \[X_t = Y_t + W_t.\]
Typically, we have $E[Y_t] \neq 0$ while $E[W_t] = 0$ (although we may have $E[W_t | W_{t-1}, ..., W_1] \neq 0$). Such models impose some parametric structure which represent a convenient and flexible way of studying time series and evalute to which extent *future* value of the series can be forecasted. As we will see, predicting future values is one of the main aspects of time series analysis. However, making predictions is often a daunting taks or as famously stated by Nils Bohr:

> 
> "*Prediction is very difficult, especially about the future.*"
>

There are plenty of examples predictions which releved to be completely erroneous. For example, Irving Fisher, Professor of Economics at Yale University, famously predicted three days before the 1929 crash:

>
> "*Stock prices have reached what looks like a permanently high plateau*". 
>

Another example is Thomas Watson, president of IBM, who said in 1943:

>
> "*I think there is a world market for maybe five computers.*"
>


## Exploratory Data Analysis (EDA) for Time Series

When dealing with relatively small time series (e.g. a few thousands), it is often useful to look at a graph of the original data. Such graphs can be informative to "detect" some features of a time series such as trends and the presence of outliers.

Indeed, a trend is typically deamed present in a time series when the data exhibit some form of long term increase or decrease or combination of increases or decreases. Such trends could be linear or non-linear and represent a important part of the "signal" of a model. Here are few examples of non-linear trends:

1. **Seasonal trends** (periodic): These are the cyclical patterns which repeat after a fixed/regular time period. This could be due to business cycles (e.g. bust/recession, recovery).
    
2. **Non-seasonal trends** (periodic): These patterns cannot be associated to seasonal variation and can for example to external variable. For example, impact of economic indicators on stock returns. Such trends are often hard to detect based on a graphical anaylsis of the data.
    
3. **"Other" trends**: These trends have typically no regular patterns and change statistical properties of a time series over a segment of time ("window"). A typicall example of such trends corresponds to vibrations observed before, during and after an earthquake.

**Example:** An example of a time series is, for example, the quarterly earnings of the company Johnson and Johson. In the figure below we present these earnings between 1960 and 1980:

**JAMES: can we "gmwm" this graph? Thanks!**

```{r example_jj, echo=FALSE, fig.height=4, fig.width=7, cache=TRUE}
```

It can clearly be observed that the data present a non-linear increasing trend as well as a yearly seasonal component. In addition, on can note that the *variability* of the data seems to increase with time. As we will such observations provide some valuable guilines to select a suitable models for such data.



<!-- Model = Pattern + Noise

In Time Series, the pattern represents the association between values observed over time (e.g. autocorrelation). 

***Since these patterns are correlated in time, methods that assume independence are unable to be used.***





In essence, we seek to be able to predict, classify, and associate observed data with a theoretical backend.

 ## Objective of Time Series

1. View a set of observations made sequentially in "time."
    * "One damned thing after another." ~ R. A. Fisher
2. Find a suitable model to describe an observed process 
    * "All models are wrong, but some are useful" ~ George Box
3. Forecast future observations 
    * "Prediction is very difficult, especially if it's about the future." ~ Niels Bohr 


## What is a Time Series and can I eat it?

The definition of ***Time Series (TS)*** is as follows: 

A ***Time Series*** is a stochastic process, a sequence of random variables (RV) defined on a common probability space denoted as $\left(X_t\right)_{t=1, \ldots, T}$ (i.e. $X_1, X_2, \ldots, X_T$).

-->


Moreover, when observing "raw" time series data is it also intersting to evaluate if some the following phenomeon occur:

1. **Change in Means:** Does the mean of the process seems to evaluate in time?
2. **Change in Variance:** Does the variance of the process evolves with time?
3. **Change in State:** Does the time series seems to change between to "states" having distinct statistical properties?
4. **Outliers** Does the time series contain some "extreme" observations? Note that this is typically difficult to assess visually.


**Example:** In the figure below we present an example of displacement recorded during an earthquake as well as explosion (**JAMES MORE DETAILS, also what are the units for the displacement?**).

**JAMES: can we "gmwm" this graph? Thanks!**

```{r example_eq, echo=FALSE, fig.height=4, fig.width=7, cache=TRUE}
```

It can clearly be observed the statistical properties of the time series appears to change in time. **JAMES ADD DETAILS also talk about 4 things (e.g. change in variance) about.**

Finally, we consider an example about high-frequency finance to illustate to limitation our current framework.

**Example:** The figure below present the returns (i.e. informally speaking the changes in price) for Starbuck's stock on the first of July 2011 during about 150 seconds (left panel) and about 400 minutes (right panel). 

```{r example_highfreq, fig.height=4, fig.width=7, cache=TRUE}
```

It can be observed on the left panel that points are not equally spaced. Indeed, in high-frequency data interval between two points is typically not constant and is, even worst, a random variable. This implies that when a new observation will be available is in general unknown. On the right panel, one can observed that that the varibility of the data seems to change dring the course of the trading day. Such phenomon is well know in the finance community as a lot variation typically happends at the start (and the end) of the day while the middle of the day is associated with small changes. Moreover, clear extrem observations can also be noted in this graph. 


## Basic Time Series Models

In this section, we introduce some simple time series models. Before doing so it is useful to define $\Omega_t$ as all the information avaiable up to time $t-1$, i.e.

\[\Omega_t = \left(X_{t-1}, X_{t-2}, ..., X_0 \right).\]

As we will see this compact notation is quite useful.

### White noise processes

The building block for most time series models is the Gaussian white noise process, which can be defined as

\[{W_t}\mathop \sim \limits^{iid} N\left( {0,\sigma _w^2} \right).\]

This definition implies that:

1. $E[W_t | \Omega_t] = 0$ for all $t$,
2. $\operatorname{cov}\left(W_t, W_{t-h} \right) = \boldsymbol{1}_{h = 0} \; \sigma^2$ for all $t, h$.

Therefore, this process present an absence of temporal (or serial) dependence and is homoskedastic (i.e it has a constant variance). This definition can be generalized in two sorts of processes, the *weak* and *strong* white noise. The process $(W_t)$ is a weak white noise if

1. $E[W_t] = 0$ for all $t$,
2. $\operatorname{var}\left(W_t\right) = \sigma^2$ for all $t$,
3. $\operatorname{cov} \left(W_t, W_{t-h}\right) = 0$, for all $t$, and for all $h \neq 0$.

Note that this definition does not imply that $W_t$ and $W_{t-h}$ are independent (for $h \neq 0$) but simply uncorrelated. However, the notion of indepence is used to define a *strong* white noise as

1. $E[W_t] = 0$ and $\operatorname{var}(W_t) = \sigma^2 < \infty$, for all $t$,
2. $F(W_t) = F(W_{t-h})$, for all $t,h$ (where $F(W_t)$ denotes the distribution of $W_t$),
3. $W_t$ and $W_{t-h}$ are independent for all $t$ and for all $h \neq 0$.

It is clear from these definitions that if a process is a strong white noise it is also a weak white noise. However, the converse is not true a shown in the following example:

**Example**: Let $X_t \mathop \sim \limits^{iid} F_t$, where $F_t$ denote a Student distribution with $t$ degrees of freedom. Such process is a weak but not a strong white noise.

The code below presents an example of how to simulate a Gaussian white noise process

```{r example_WN, fig.height=4, fig.width=7, cache=TRUE}
```

### Random Walk Processes

The term *random walk* was first introduce by Karl Pearson in the early 19 hunders. As for the white noise, there exist a large range of random walk processes. For example, one of the simplest form of random walk are be explained as follows: suppose that you are walking on campus and your next step can either be on your left, your right, forward or backward (each with equal probability). Two realizations of such processes are represented below:

```{r RW2d, fig.height=4, fig.width=7, cache=TRUE}
```

Such processes inspired Karl Pearson famous quote that

>
> "*the most likely place to find a drunken walker is somewhere near his starting point.*"
> 

Empirical evidence of this phenomenon is not too hard to find on a Friday night in Champaign. In this class, we only consider one very specific form of randon walk, namely the Gaussian random walk which can be defined as:

$$X_t = X_{t-1} + W_t,$$

where $W_t$ is a Gaussian white noise and with initial condition $X_0 = c$ (typically $c = 0$). This process can be expressed differently by *backsubstitution* as follows:

\[\begin{aligned}
  {X_t} &= {X_{t - 1}} + {W_t} 
   = \left( {{X_{t - 2}} + {W_{t - 1}}} \right) + {W_t} \\
  {X_t} &= \sum\limits_{i = 1}^t {{W_i}} + X_0 =  \sum\limits_{i = 1}^t {{W_i}} + c \\ 
\end{aligned} \]

The code below presents an example of how to simulate a such process

```{r generate_rw, cache=TRUE}
```

### Autoregressive Process of Order 1

An autoregressive process of order 1 or AR(1) is a generalization of both the white noise and random walk process which are both special case of an AR(1). A (Gaussian) AR(1) process can be defined as

\[{X_t} = {\phi}{X_{t - 1}} + {W_t},\]

where $W_t$ is a Gaussian white noise. Clearly, an AR(1) with $\phi = 0$ is a Gaussian white noise and when $\phi = 1$ the process becomes a random walk. 

**Remark:** We generally assume that an AR(1) (as well as other time series models) have zero mean. The reason for this assumption is only to simplfy the notation but it is easy to consider an AR(1) process around an arbitrary mean $\mu$, i.e.

\[\left(X_t - \mu\right) = \phi \left(X_{t-1} - \mu \right) + W_t,\]

which is of course equivalent to

\[X_t = \left(1 - \phi \right) \mu + \phi X_{t-1} + W_t.\]

Thus, we will generally onyl work with zero mean processes since adding means is simple.

**Remark:** An AR(1) is in fact a linear combination of the past realisations of the white noise $W_t$. Indeed, we have

\[\begin{aligned}
 {X_t} &= {\phi_t}{X_{t - 1}} + {W_t} 
   = {\phi}\left( {{\phi}{X_{t - 2}} + {W_{t - 1}}} \right) + {W_t} \\
   &= \phi^2{X_{t - 2}} + {\phi}{W_{t - 1}} + {W_t} 
   = {\phi^t}{X_0} + \sum\limits_{i = 0}^{t - 1} {\phi^i{W_{t - i}}}.
\end{aligned}\]

Under the assumption of infinite past (i.e. $t \in \mathbb{Z}$) and $|\phi| < 1$, we obtain

\[X_t = \sum\limits_{i = 0}^{\infty} {\phi^i {W_{t - i}}},\]

since $\operatorname{lim}_{i \to \infty} \; {\phi^i}{X_{t-i}} = 0$.


The code below presents an example of how an AR(1) can be simulated

```{r example_AR1, cache=TRUE}
```

### Moving Average Process of Order 1

As we have seen in the previous example, an AR(1) can be expressed as a linear combination of all past observation of $(W_t)$, the next process, called a moving average process of order 1 or MA(1) is (in some sense) a "truncated" version of an AR(1). It is defined as

\begin{equation} 
  X_t = \theta W_{t-1} + W_t,
 \label{eq:defMA1}
\end{equation}
 
where (again) $W_t$ denotes a Gaussian white noise process. An example on how generate an MA(1) is given below:

```{r example_MA1, cache=TRUE}
```

  
### Linear Drift

A linear drift is a very simple detemrinistic time series model which can be expressed as 

\[X_t = X_{t+1} + \omega, \]

where $\omega$ is a constant and with the initial condition $X_0 = c$, an arbitrary constant (typically zero). This process can be expressed in a more familiar form as follows:

\[
  {X_t} = {X_{t - 1}} + \omega 
   = \left( {{X_{t - 2}} + \omega} \right) + \omega 
   = t{\delta} + c  \]

Therefore, a (linear) drift corresponds to a simple linear model with slope $\omega$ and intercept $c$.

A drift can simply be generated used the code below:

```{r example_DR, cache=TRUE}
```


### Composite Stochastic Processes

A composite stochastic processes can be defined as the sum of underlying (or latent) stochastic processes. In this text, we will use the term *latent time series* as a synomym to composite stochastic processes. A simple example of such process is for example

\[\begin{aligned}
Y_t &= Y_{t-1} + W_t + \delta\\
X_t &= Y_t + Z_t,
\end{aligned}\]

where $W_t$ and $Z_t$ are two independent Gaussian white noise processes. This model often used as first tool to approximate the number of individuals in the context ecological population dynamic. For example, suppose we want to study the popiulation of Chamois in the Swiss Alpes so let $Y_t$ denote the "true" number of individual in this population at time $t$. It is reasonable that $Y_t$ is (approximately) the population at the previous time $t-1$ (e.g the previous year) plus a random variation and a drift. This random variation is due to the natural random in ecological population and reflects changes in number of predators, in aboundance of food or weather condition. On the other hand, the drift is often of particular interest for ecologist as it can used to determine the "long" term trends for the population (e.g. is the population increasing, stable or decreasing). Of course, $Y_t$ (the number of individauls) is typically unknown and we observed a noisy version of it, denotes as $X_t$. This process corresponds to the true population plus a measruementr error as some Chamois may not be observed individuals and some counted several times. Interestingly, this process can clearly be expressed as a *latent time series model* (or composite stochastic process) as follows:

\[\begin{aligned}
R_t &= R_{t-1} + W_t \\
S_t &= \delta t \\
X_t &= R_t + S_t + Z_t,
\end{aligned}\]

where $R_t$, $S_t$ and $Z_t$ denote, respectively, a radnom walk, a drift and a white noise. The code below can be used to simulate such data:

```{r example_ecolo, cache=TRUE}
```

In the above graph, the three latent (unobserved) processes are first depicted (i.e. white noise, random walk and drift) and then the sum of the three is present (i.e. $(X_t)$).
<!--
*Definition:* **Autoregressive Process of Order p = 1**

This process is generally denoted as **AR(1)** and is defined as:
${y_t} = {\phi _1}{y_{t - 1}} + {w_t},$

where ${w_t}\mathop \sim \limits^{iid} WN\left( {0,\sigma _w^2} \right)$

If $\phi _1 = 1$, then the process is equivalent to a random walk.

The process can be simplified using **backsubstitution** to being:

```{r generate_ar1, cache=TRUE}
```
-->
<!--
white noise can actually be generalize

The process name of white noise has meaning in the notion of colors of noise. Specifically, the white noise is a process that mirrors white light's flat frequency spectrum. So, the process has equal frequencies in any interval of time.

*Definition:* **White Noise**

$w_t$ or $\varepsilon _t$ is a **white noise process** if $w_t$ are uncorrelated identically distributed random variables with
$E\left[w_t\right] = 0$ and $Var\left[w_t\right] = \sigma ^2$, for all $t$. We can represent this algebraically as:
$$y_t = w_t,$$
where ${w_t}\mathop \sim \limits^{id} WN\left( {0,\sigma _w^2} \right)$

Now, if the $w_t$ are **Normally (Gaussian) distributed**, then the process is known as a **Gaussian White Noise** e.g. ${w_t}\mathop \sim \limits^{iid} N\left( {0,{\sigma ^2}} \right)$ -->




