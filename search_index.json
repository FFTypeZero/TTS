[
["index.html", "A Tour of Time Series Analysis with R Preface Bibliographic Note Rendering Mathematical Formulae R Code Conventions License", " A Tour of Time Series Analysis with R James Balamuta, Stéphane Guerrier and Roberto Molinari 2016-08-17 Preface This text is designed as an introduction to time series analysis and is used as a support document for the class STAT 429 (Time Series Analysis) given at the University of Illinois at Urbana-Champaign. It preferable to always access the text online rather than a printed to be sure you are using the latest version. The online version so affords additional features over the traditional PDF copy such as a scaling text, variety of font faces, and themed backgrounds. However, if you are in need of a local copy, a pdf version is also available. This document is under active development and as a result is likely to contains many errors. As Montesquieu puts it: “La nature semblait avoir sagement pourvu à ce que les sottises des hommes fussent passagères, et les livres les immortalisent.” If you notice any errors, we would be grateful if you would let us know. For that there are two options: If you are familiar with GitHub and know RMarkdown, make a pull request and fix the issue yourself! (fastest resolution) In the Online version, click the edit button in the top-left corner. Send an email to balamut2 AT illinois DOT edu and we will address issue. Bibliographic Note This text is heavily inspired by the following three execellent references: “Time Series Analysis and Its Applications”, Third Edition, Robert H. Shumway &amp; David S. Stoffer. “Time Series for Macroeconomics and Finance”, John H. Cochrane. “Cours de Séries Temporelles: Théorie et Applications”, Volume 1, Arthur Charpentier. Rendering Mathematical Formulae Throughout the book, there will be mathematical symbols used to express the material. Depending on the version of the book, there are two different render engines. For the online version, the text uses MathJax to render mathematical notation for the web. In the event the formulae does not load for a specific chapter, first try to refresh the page. 9 times out of 10 the issue is related to the software library not loading quickly. For the pdf version, the text is built using the recommended AMS LaTeX symbolic packages. As a result, there should be no issue displaying equations. An example of a mathematical rendering capabilities would be given as: \\[ a^2 + b^2 = c^2 \\] R Code Conventions The code used throughout the book will predominately be R code. To obtain a copy of R, go to the Comprehensive R Archive Network (CRAN) and download the appropriate installer for your operating system. When R code is displayed it will be typeset using a monospace font with syntax highlighting enabled to ensure the differentiation of functions, variables, and so on. For example, the following adds 1 to 1 a = 1L + 1L a Each code segment may contain actual output from R. Such output will appear in grey font prefixed by ##. For example, the output of the above code segment would look like so: ## [1] 2 Alongside the PDF download of the book, you should find the R code used within each chapter. License This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],
["introduction.html", "Chapter 1 Introduction 1.1 Exploratory Data Analysis (EDA) for Time Series 1.2 Basic Time Series Models", " Chapter 1 Introduction \"One damned thing after another.\" ~ R. A. Fisher > --> Prévoir consiste à projeter dans l’avenir ce qu’on a perçu dans le passé. Henri Bergson > *Hâtons-nous; le temps fuit, et nous entraîne avec soi : le moment où je parle est déjà loin de moi*. Nicolas Boileau > --> Objectives TODO JAMES Generally speaking a time series (or stochastic process) corresponds to set of “repeated” observations of the same variable such as price of a financial asset or temperature in a given location. In terms of notation a time series is often writen as \\[\\left(X_1, X_2, ..., X_T \\right) \\;\\;\\; \\text{ or } \\;\\;\\; \\left(X_t\\right)_{t = 1,...,T}.\\] The time index \\(t\\) generally the set \\(\\mathbb{N}\\) or \\(\\mathbb{Z}\\). When \\(t \\in \\mathbb{R}\\), a time series becomes a continuous-time stochastic process such a Brownian motion. In this class we limit our selves to discrete-time processes where a variable is measured sequentially at fixed and equally spaced intervals in time. This implies that we will assume \\(t\\) is not random (the time at which each observation is measure is know) and the time between two consequtive observation is constant. Moreover, the term “time series” can, as we discussed, denote a sample or a set of observations but also a probability model for that sample. For example, on of the simplest probability model used in time series analysis is called a white noise process and is defined as \\[W_t \\mathop \\sim \\limits^{iid} N(0, \\sigma^2).\\] This statement simply means that \\((X_t)\\) is normally distribtued and independent over time. This model is quite unintersting but as we will very usefull to construct other (more interesting) models. Unlike white noise process, time series are typically not independent over time. Suppose that the temperature in Champaign is unusually low, then it is reasonable to assume that tomorrow’s temperature will also be low. Such behaviour suggest a dependent over time. The time series methods we will discuss in this class consists parametric models used to characterize (or at least approximate) the joint distribution of \\((X_t)\\). Often, time series models can be decomposed in what we called a signal, say \\((Y_t)\\) and a noise, say \\((W_t)\\), leading to the model \\[X_t = Y_t + W_t.\\] Typically, we have \\(E[Y_t] \\neq 0\\) while \\(E[W_t] = 0\\) (although we may have \\(E[W_t | W_{t-1}, ..., W_1] \\neq 0\\)). Such models impose some parametric structure which represent a convenient and flexible way of studying time series and evalute to which extent future value of the series can be forecasted. As we will see, predicting future values is one of the main aspects of time series analysis. However, making predictions is often a daunting taks or as famously stated by Nils Bohr: “Prediction is very difficult, especially about the future.” There are plenty of examples predictions which releved to be completely erroneous. For example, Irving Fisher, Professor of Economics at Yale University, famously predicted three days before the 1929 crash: “Stock prices have reached what looks like a permanently high plateau”. Another example is Thomas Watson, president of IBM, who said in 1943: “I think there is a world market for maybe five computers.” 1.1 Exploratory Data Analysis (EDA) for Time Series When dealing with relatively small time series (e.g. a few thousands), it is often useful to look at a graph of the original data. Such graphs can be informative to “detect” some features of a time series such as trends and the presence of outliers. Indeed, a trend is typically deamed present in a time series when the data exhibit some form of long term increase or decrease or combination of increases or decreases. Such trends could be linear or non-linear and represent a important part of the “signal” of a model. Here are few examples of non-linear trends: Seasonal trends (periodic): These are the cyclical patterns which repeat after a fixed/regular time period. This could be due to business cycles (e.g. bust/recession, recovery). Non-seasonal trends (periodic): These patterns cannot be associated to seasonal variation and can for example to external variable. For example, impact of economic indicators on stock returns. Such trends are often hard to detect based on a graphical anaylsis of the data. “Other” trends: These trends have typically no regular patterns and change statistical properties of a time series over a segment of time (“window”). A typicall example of such trends corresponds to vibrations observed before, during and after an earthquake. Example: An example of a time series is, for example, the quarterly earnings of the company Johnson and Johson. In the figure below we present these earnings between 1960 and 1980: JAMES: can we “gmwm” this graph? Thanks! It can clearly be observed that the data present a non-linear increasing trend as well as a yearly seasonal component. In addition, on can note that the variability of the data seems to increase with time. As we will such observations provide some valuable guilines to select a suitable models for such data. Moreover, when observing “raw” time series data is it also intersting to evaluate if some the following phenomeon occur: Change in Means: Does the mean of the process seems to evaluate in time? Change in Variance: Does the variance of the process evolves with time? Change in State: Does the time series seems to change between to “states” having distinct statistical properties? Outliers Does the time series contain some “extreme” observations? Note that this is typically difficult to assess visually. Example: In the figure below we present an example of displacement recorded during an earthquake as well as explosion (JAMES MORE DETAILS, also what are the units for the displacement?). JAMES: can we “gmwm” this graph? Thanks! It can clearly be observed the statistical properties of the time series appears to change in time. JAMES ADD DETAILS also talk about 4 things (e.g. change in variance) about. Finally, we consider an example about high-frequency finance to illustate to limitation our current framework. Example: The figure below present the returns (i.e. informally speaking the changes in price) for Starbuck’s stock on the first of July 2011 during about 150 seconds (left panel) and about 400 minutes (right panel). # Load packages library(highfrequency) ## Loading required package: xts library(timeDate) # Load &quot;high-frequency&quot; Starbucks returns for Jul 01 2011 data(sbux.xts) # Plot returns par(mfrow = c(1,2)) plot(sbux.xts[1:89], main = &quot; &quot;, ylab = &quot;Returns&quot;) plot(sbux.xts, main = &quot; &quot;, ylab = &quot;Returns&quot;) It can be observed on the left panel that points are not equally spaced. Indeed, in high-frequency data interval between two points is typically not constant and is, even worst, a random variable. This implies that when a new observation will be available is in general unknown. On the right panel, one can observed that that the varibility of the data seems to change dring the course of the trading day. Such phenomon is well know in the finance community as a lot variation typically happends at the start (and the end) of the day while the middle of the day is associated with small changes. Moreover, clear extrem observations can also be noted in this graph. 1.2 Basic Time Series Models In this section, we introduce some simple time series models. Before doing so it is useful to define \\(\\Omega_t\\) as all the information avaiable up to time \\(t-1\\), i.e. \\[\\Omega_t = \\left(X_{t-1}, X_{t-2}, ..., X_0 \\right).\\] As we will see this compact notation is quite useful. 1.2.1 White noise processes The building block for most time series models is the Gaussian white noise process, which can be defined as \\[{W_t}\\mathop \\sim \\limits^{iid} N\\left( {0,\\sigma _w^2} \\right).\\] This definition implies that: \\(E[W_t | \\Omega_t] = 0\\) for all \\(t\\), \\(\\operatorname{cov}\\left(W_t, W_{t-h} \\right) = \\boldsymbol{1}_{h = 0} \\; \\sigma^2\\) for all \\(t, h\\). Therefore, this process present an absence of temporal (or serial) dependence and is homoskedastic (i.e it has a constant variance). This definition can be generalized in two sorts of processes, the weak and strong white noise. The process \\((W_t)\\) is a weak white noise if \\(E[W_t] = 0\\) for all \\(t\\), \\(\\operatorname{var}\\left(W_t\\right) = \\sigma^2\\) for all \\(t\\), \\(\\operatorname{cov} \\left(W_t, W_{t-h}\\right) = 0\\), for all \\(t\\), and for all \\(h \\neq 0\\). Note that this definition does not imply that \\(W_t\\) and \\(W_{t-h}\\) are independent (for \\(h \\neq 0\\)) but simply uncorrelated. However, the notion of indepence is used to define a strong white noise as \\(E[W_t] = 0\\) and \\(\\operatorname{var}(W_t) = \\sigma^2 &lt; \\infty\\), for all \\(t\\), \\(F(W_t) = F(W_{t-h})\\), for all \\(t,h\\) (where \\(F(W_t)\\) denotes the distribution of \\(W_t\\)), \\(W_t\\) and \\(W_{t-h}\\) are independent for all \\(t\\) and for all \\(h \\neq 0\\). It is clear from these definitions that if a process is a strong white noise it is also a weak white noise. However, the converse is not true a shown in the following example: Example: Let \\(X_t \\mathop \\sim \\limits^{iid} F_t\\), where \\(F_t\\) denote a Student distribution with \\(t\\) degrees of freedom. Such process is a weak but not a strong white noise. The code below presents an example of how to simulate a Gaussian white noise process # This code simulate a gaussian white noise process n = 100 # process length sigma2 = 1 # process variance Xt = gen.gts(WN(sigma2 = sigma2), n = n) plot(Xt) 1.2.2 Random Walk Processes The term random walk was first introduce by Karl Pearson in the early 19 hunders. As for the white noise, there exist a large range of random walk processes. For example, one of the simplest form of random walk are be explained as follows: suppose that you are walking on campus and your next step can either be on your left, your right, forward or backward (each with equal probability). Two realizations of such processes are represented below: RW2dimension = function(steps = 100){ # Initial matrix step_direction = matrix(0,steps+1,2) # Start random walk for (i in 2:steps+1){ # Draw a random number from U(0,1) rn = runif(1) # Go right if rn \\in [0,0.25) if (rn &lt; 0.25) {step_direction[i,1] = 1} # Go left if rn \\in [0.25,0.5) if (rn &gt;= 0.25 &amp;&amp; rn &lt; 0.5) {step_direction[i,1] = -1} # Go forward if rn \\in [0.5,0.75) if (rn &gt;= 0.5 &amp;&amp; rn &lt; 0.75) {step_direction[i,2] = 1} # Go backward if rn \\in [0.75,1] if (rn &gt;= 0.75) {step_direction[i,2] = -1} } # Cumulate steps position = cbind(cumsum(step_direction[,1]),cumsum(step_direction[,2])) # Plot results plot(NA, xlim = range(position[,1]), ylim = range(position[,2]), xlab = &quot;X-position&quot;, ylab = &quot;Y-position&quot;, main = paste(&quot;2D random walk with&quot;, steps, &quot;steps&quot;)) grid() lines(position, type = &quot;l&quot;) points(c(0,position[steps,1]),c(0,position[steps,2]), pch = 16, cex= 3, col = c(&quot;red&quot;,&quot;blue&quot;)) legend(&quot;topleft&quot;, c(&quot;Start point&quot;,&quot;End point&quot;), pch = 16, pt.cex = 3, col = c(&quot;red&quot;,&quot;blue&quot;), bty = &quot;n&quot;, bg = &quot;white&quot;, box.col = &quot;white&quot;, cex = 1.2) } # Plot 2D random walk with 10^2 and 10^5 steps set.seed(2) par(mfrow = c(1,2)) RW2dimension(steps = 10^2) RW2dimension(steps = 10^5) Such processes inspired Karl Pearson famous quote that “the most likely place to find a drunken walker is somewhere near his starting point.” Empirical evidence of this phenomenon is not too hard to find on a Friday night in Champaign. In this class, we only consider one very specific form of randon walk, namely the Gaussian random walk which can be defined as: \\[X_t = X_{t-1} + W_t,\\] where \\(W_t\\) is a Gaussian white noise and with initial condition \\(X_0 = c\\) (typically \\(c = 0\\)). This process can be expressed differently by backsubstitution as follows: \\[\\begin{aligned} {X_t} &amp;= {X_{t - 1}} + {W_t} = \\left( {{X_{t - 2}} + {W_{t - 1}}} \\right) + {W_t} \\\\ {X_t} &amp;= \\sum\\limits_{i = 1}^t {{W_i}} + X_0 = \\sum\\limits_{i = 1}^t {{W_i}} + c \\\\ \\end{aligned} \\] The code below presents an example of how to simulate a such process 1.2.3 Autoregressive Process of Order 1 An autoregressive process of order 1 or AR(1) is a generalization of both the white noise and random walk process which are both special case of an AR(1). A (Gaussian) AR(1) process can be defined as \\[{X_t} = {\\phi}{X_{t - 1}} + {W_t},\\] where \\(W_t\\) is a Gaussian white noise. Clearly, an AR(1) with \\(\\phi = 0\\) is a Gaussian white noise and when \\(\\phi = 1\\) the process becomes a random walk. Remark: We generally assume that an AR(1) (as well as other time series models) have zero mean. The reason for this assumption is only to simplfy the notation but it is easy to consider an AR(1) process around an arbitrary mean \\(\\mu\\), i.e. \\[\\left(X_t - \\mu\\right) = \\phi \\left(X_{t-1} - \\mu \\right) + W_t,\\] which is of course equivalent to \\[X_t = \\left(1 - \\phi \\right) \\mu + \\phi X_{t-1} + W_t.\\] Thus, we will generally onyl work with zero mean processes since adding means is simple. Remark: An AR(1) is in fact a linear combination of the past realisations of the white noise \\(W_t\\). Indeed, we have \\[\\begin{aligned} {X_t} &amp;= {\\phi_t}{X_{t - 1}} + {W_t} = {\\phi}\\left( {{\\phi}{X_{t - 2}} + {W_{t - 1}}} \\right) + {W_t} \\\\ &amp;= \\phi^2{X_{t - 2}} + {\\phi}{W_{t - 1}} + {W_t} = {\\phi^t}{X_0} + \\sum\\limits_{i = 0}^{t - 1} {\\phi^i{W_{t - i}}}. \\end{aligned}\\] Under the assumption of infinite past (i.e. \\(t \\in \\mathbb{Z}\\)) and \\(|\\phi| &lt; 1\\), we obtain \\[X_t = \\sum\\limits_{i = 0}^{\\infty} {\\phi^i {W_{t - i}}},\\] since \\(\\operatorname{lim}_{i \\to \\infty} \\; {\\phi^i}{X_{t-i}} = 0\\). The code below presents an example of how an AR(1) can be simulated # This code simulate a gaussian random walk process n = 100 # process length phi = 0.5 # phi parameter sigma2 = 1 # innovation variance Xt = gen.gts(AR1(phi = phi, sigma2 = sigma2), n = n) plot(Xt) 1.2.4 Moving Average Process of Order 1 As we have seen in the previous example, an AR(1) can be expressed as a linear combination of all past observation of \\((W_t)\\), the next process, called a moving average process of order 1 or MA(1) is (in some sense) a “truncated” version of an AR(1). It is defined as \\[X_t = \\theta W_{t-1} + W_t,\\] where (again) \\(W_t\\) denotes a Gaussian white noise process. An example on how generate an MA(1) is given below: # This code simulate a gaussian white noise process n = 100 # process length sigma2 = 1 # innovation variance theta = 0.5 # theta parameter Xt = gen.gts(MA1(theta = theta, sigma2 = sigma2), n = n) plot(Xt) 1.2.5 Linear Drift A linear drift is a very simple detemrinistic time series model which can be expressed as \\[X_t = X_{t+1} + \\omega, \\] where \\(\\omega\\) is a constant and with the initial condition \\(X_0 = c\\), an arbitrary constant (typically zero). This process can be expressed in a more familiar form as follows: \\[ {X_t} = {X_{t - 1}} + \\omega = \\left( {{X_{t - 2}} + \\omega} \\right) + \\omega = t{\\delta} + c \\] Therefore, a (linear) drift corresponds to a simple linear model with slope \\(\\omega\\) and intercept \\(c\\). A drift can simply be generated used the code below: # This code simulate a linear drift with 0 intercept n = 100 # process length omega = 0.5 # slope parameter Xt = gen.gts(DR(omega = omega), n = n) plot(Xt) 1.2.6 Composite Stochastic Processes A composite stochastic processes can be defined as the sum of underlying (or latent) stochastic processes. In this text, we will use the term latent time series as a synomym to composite stochastic processes. A simple example of such process is for example \\[\\begin{aligned} Y_t &amp;= Y_{t-1} + W_t + \\delta\\\\ X_t &amp;= Y_t + Z_t, \\end{aligned}\\] where \\(W_t\\) and \\(Z_t\\) are two independent Gaussian white noise processes. This model often used as first tool to approximate the number of individuals in the context ecological population dynamic. For example, suppose we want to study the popiulation of Chamois in the Swiss Alpes so let \\(Y_t\\) denote the “true” number of individual in this population at time \\(t\\). It is reasonable that \\(Y_t\\) is (approximately) the population at the previous time \\(t-1\\) (e.g the previous year) plus a random variation and a drift. This random variation is due to the natural random in ecological population and reflects changes in number of predators, in aboundance of food or weather condition. On the other hand, the drift is often of particular interest for ecologist as it can used to determine the “long” term trends for the population (e.g. is the population increasing, stable or decreasing). Of course, \\(Y_t\\) (the number of individauls) is typically unknown and we observed a noisy version of it, denotes as \\(X_t\\). This process corresponds to the true population plus a measruementr error as some Chamois may not be observed individuals and some counted several times. Interestingly, this process can clearly be expressed as a latent time series model (or composite stochastic process) as follows: \\[\\begin{aligned} R_t &amp;= R_{t-1} + W_t \\\\ S_t &amp;= \\delta t \\\\ X_t &amp;= R_t + S_t + Z_t, \\end{aligned}\\] where \\(R_t\\), \\(S_t\\) and \\(Z_t\\) denote, respectively, a radnom walk, a drift and a white noise. The code below can be used to simulate such data: n = 1000 # process length delta = 0.005 # delta parameter (drift) sigma2 = 10 # variance parameter (white noise) gamma2 = 0.1 # innovation variance (random walk) model = WN(sigma2 = sigma2) + RW(gamma2 = gamma2) + DR(omega = delta) Xt = gen.lts(model, n = n) plot(Xt) In the above graph, the three latent (unobserved) processes are first depicted (i.e. white noise, random walk and drift) and then the sum of the three is present (i.e. \\((X_t)\\)). "],
["autocorrelation-and-stationarity.html", "Chapter 2 Autocorrelation and Stationarity 2.1 Dependency 2.2 The Autocorrelation and Autocovariance Functions 2.3 Stationarity 2.4 Joint Stationarity", " Chapter 2 Autocorrelation and Stationarity \"I have seen the future and it is very much like the present, only longer.\" > > --- Kehlog Albran, The Profit --> After reading this chapter you will be able to: Describe independent and dependent data Interpret a processes ACF and CCF. Understand the notion of stationarity. Differentiate between Strong and Weak stationarity. Judge whether a process is stationary. 2.1 Dependency Generally speaking, there is a dependence that within the sequence of random variables. Recall the difference between independent and dependent data: Definition: Independence \\(X_1, X_2, \\ldots, X_T\\) are independent and identically distributed if and only if \\begin{equation} P\\left(X_1 \\le x_1, X_2 \\le x_2,\\ldots, X_{T} \\le x_T \\right) = P\\left(X_1 \\le x_1\\right) P\\left(X_2 \\le x_2\\right) \\cdots P\\left(X_{T} \\le x_T \\right) \\label{eq:independent} \\end{equation} for any \\(T \\ge 2\\) and \\(x_1, \\ldots, x_T \\in \\mathbb{R}\\). Definition: Dependence \\(X_1, X_2, \\ldots, X_T\\) are identically distributed but dependent, then \\begin{equation} \\left| {P\\left( {{X_1} &lt; {x_1},{X_2} &lt; {x_2}, \\ldots ,{X_T} &lt; {x_T}} \\right) - P\\left( {{X_1} &lt; {x_1}} \\right)P\\left( {{X_2} &lt; {x_2}} \\right) \\cdots P\\left( {{X_T} &lt; {x_T}} \\right)} \\right| \\ne 0 \\label{eq:dependent} \\end{equation} for some \\(x_1, \\ldots, x_T \\in \\mathbb{R}\\). 2.1.1 Measuring (Linear) Dependence There are many forms of dependency… dependency However, the methods, covariance and correlation, that we will be using are specific to measuring linear dependence. As a result, these tools are less helpful to measure monotonic dependence and they are much less helpful to measure nonlinearly dependence. 2.2 The Autocorrelation and Autocovariance Functions Dependence between \\(T\\) different RV is difficult to measure in one shot! So we consider just two random variables, \\(X_t\\) and \\(X_{t+h}\\). Then one (linear) measure of dependence is the covariance between \\(\\left(X_t , X_{t+h}\\right)\\). Since \\(X\\) is the same RV observed at two different time points, the covariance between \\(X_t\\) and \\(X_{t+h}\\) is defined as the Autocovariance. 2.2.1 Definitions The Autocovariance Function of a series \\(X_t\\) is defined as \\[{\\gamma _x}\\left( {t,t+h} \\right) = \\operatorname{cov} \\left( {{x_t},{x_{t+h}}} \\right).\\] Since we generally consider stochastic processes with constant zero mean we often have \\[{\\gamma _x}\\left( {t,t+h} \\right) = E\\left[X_t X_{t+h} \\right]. \\] We normally drop the subscript referring to the time series if it is clear to the time series the autocovariance function is referencing. For example, we generally use \\({\\gamma}\\left( {t,t+h} \\right)\\) instead of \\({\\gamma _x}\\left( {t,t+h} \\right)\\). Moreover, the notation is even further simplify when the covariance of \\(X_t\\) and \\(X_{t+h}\\) is the same as that of \\(X_{t+j}\\) and \\(X_{t+h+j}\\) (for \\(j \\in \\mathbb{Z}\\)), i.e. that the covariance depends only on the time between observations and not the absolute date \\(t\\). This is an important property call stationarity, which will be discuss in the next section. In this case, we simply use to following notation: \\[\\gamma \\left( {h} \\right) = \\operatorname{cov} \\left( X_t , X_{t+h} \\right). \\] A few other remarks: The covariance function is symmetric. That is, \\({\\gamma}\\left( {h} \\right) = {\\gamma}\\left( -h \\right)\\) since \\(\\operatorname{cov} \\left( {{X_t},{X_{t+h}}} \\right) = \\operatorname{cov} \\left( X_{t+h},X_{t} \\right)\\). Note that \\(\\operatorname{var} \\left( X_{t} \\right) = {\\gamma}\\left( 0 \\right)\\). We have that \\(|\\gamma(h)| \\leq \\gamma(0)\\) for all \\(h\\). The proof of this inequality follows from Cauchy-Schwarz inequality, i.e. \\[ \\begin{aligned} \\left(|\\gamma(h)| \\right)^2 &amp;= \\gamma(h)^2 = \\left(E\\left[\\left(X_t - E[X_t] \\right)\\left(X_{t+h} - E[X_{t+h}] \\right)\\right]\\right)^2\\\\ &amp;\\leq E\\left[\\left(X_t - E[X_t] \\right)^2 \\right] E\\left[\\left(X_{t+h} - E[X_{t+h}] \\right)^2 \\right] = \\gamma(0)^2. \\end{aligned} \\] Just as any covariance, the \\({\\gamma}\\left( {h} \\right)\\) is “scale dependent”, \\({\\gamma}\\left( {h} \\right) \\in \\mathbb{R}\\), or \\(-\\infty \\le {\\gamma}\\left( {h} \\right) \\le +\\infty\\) If \\(\\left| {\\gamma}\\left( {h} \\right) \\right|\\) is “close” to 0, then they are “less dependent”. If \\(\\left| {\\gamma}\\left( {h} \\right) \\right|\\) is “far” from 0, \\(X_t\\) and \\(X_{t+h}\\) are “more dependent”. \\({\\gamma}\\left( {h} \\right)=0\\) does not imply \\(X_t\\) and \\(X_{t+h}\\) are independent. This is only true in joint Gaussian case. An important related statistic is the correlation of \\(X_t\\) with \\(X_{t+h}\\) or autocorrelation which is defined (for stationary processes) as \\[\\rho \\left( h \\right) = \\operatorname{corr}\\left( {{X_t},{X_{t + h}}} \\right) = \\frac{\\gamma(h) }{\\gamma(0)}.\\] It is important to note that the above notation implies that the autocorrelation function is only a function of the lag \\(h\\) between observations. Thus, autocovariances and autocorrelations are one possible way to describe the joint distribution of a time series. Indeed, the correlation of \\(X_t\\) with \\(X_{t+1}\\) is an obvious measure of how persistent a time series is. Remeber that just as with any correlation: \\(\\rho \\left( h \\right)\\) is scale free. \\(\\rho \\left( {{X_t},{X_{t + h}}} \\right)\\) is closer to \\(\\pm 1 \\Rightarrow \\left({ X_t, X_{t+h} } \\right)\\) “more dependent.” \\(|\\rho \\left( h \\right)| \\leq 1\\) since \\(|\\gamma(h)| \\leq \\gamma(0)\\). Causation and correlation are two very different things! 2.2.2 A Fundamental Representation Autocovariances and autocorrelation also turn out to be a very useful tool because they are one of fundamental representations of time series. Indeed, if we consider a zero mean normally distrbuted process it is clear that its joint distribution is fully characterized by the autocariances \\(E[X_t X_{t+h}]\\) (since the joint probability density only depends of these covariances). Once we know the autocovariances we know everything there is to know about the process and therefore: If two processes have the same autocovariance function, then they are the same process. 2.2.3 Admissible autocorrelation functions Since the autocorrelation is related to a fundamental representation of time series it implies that one might be able to define a stochastic process by picking a set autocorrelation values. However, it turns out not every collection of numbers such as \\(\\{\\rho_1, \\rho_2, ...\\}\\) is the autocorrelation of a process. Two conditions are required to ensure the validity of an autocorrelation sequence: \\(\\operatorname{max}_j \\; | \\rho_j| \\leq 1\\). \\(\\operatorname{var} \\left[\\sum_{j = 0}^\\infty \\alpha_j X_{t-j} \\right] \\geq 0\\) for all \\(\\{\\alpha_0, \\alpha_1, ...\\}\\). The first condition is obvious and simply relects the fact that \\(|\\rho \\left( h \\right)| \\leq 1\\) but the second is more difficult to verify. Let \\(\\alpha_j = 0, \\; j &gt; 1\\), then conditon 2 implies that \\[\\operatorname{var} \\left[ \\alpha_0 X_{t} + \\alpha_1 X_{t-1} \\right] = \\gamma_0 \\begin{bmatrix} \\alpha_0 &amp; \\alpha_1 \\end{bmatrix} \\begin{bmatrix} 1 &amp; \\rho_1\\\\ \\rho_1 &amp; 1 \\end{bmatrix} \\begin{bmatrix} \\alpha_0 \\\\ \\alpha_1 \\end{bmatrix} \\geq 0. \\] Thus, the matrix \\[ \\boldsymbol{A}_1 = \\begin{bmatrix} 1 &amp; \\rho_1\\\\ \\rho_1 &amp; 1 \\end{bmatrix} \\] must be positive semi-definite. Therefore, \\[\\operatorname{det} \\left(\\boldsymbol{A}_1\\right) = 1 - \\rho_1^2 \\] implying that \\(|\\rho_1| &lt; 1\\). Next, let \\(\\alpha_j = 0, \\; j &gt; 2\\), then we must verify that: \\[\\operatorname{var} \\left[ \\alpha_0 X_{t} + \\alpha_1 X_{t-1} + \\alpha_2 X_{t-2} \\right] = \\gamma_0 \\begin{bmatrix} \\alpha_0 &amp; \\alpha_1 &amp;\\alpha_2 \\end{bmatrix} \\begin{bmatrix} 1 &amp; \\rho_1 &amp; \\rho_2\\\\ \\rho_1 &amp; 1 &amp; \\rho_1 \\\\ \\rho_2 &amp; \\rho_1 &amp; 1 \\end{bmatrix} \\begin{bmatrix} \\alpha_0 \\\\ \\alpha_1 \\\\ \\alpha_2 \\end{bmatrix} \\geq 0. \\] Similarly, this implies that the matrix \\[ \\boldsymbol{A}_2 = \\begin{bmatrix} 1 &amp; \\rho_1 &amp; \\rho_2\\\\ \\rho_1 &amp; 1 &amp; \\rho_1 \\\\ \\rho_2 &amp; \\rho_1 &amp; 1 \\end{bmatrix} \\] must be positive semi-definite. It is easy to verify that \\[\\operatorname{det} \\left(\\boldsymbol{A}_2\\right) = \\left(1 - \\rho_2 \\right)\\left(- 2 \\rho_1^2 + \\rho_2 + 1\\right). \\] It implies that \\(|\\rho_2| &lt; 1\\) as well as \\[\\begin{aligned} &amp;- 2 \\rho_1^2 + \\rho_2 + 1 \\geq 0 \\Rightarrow 1 &gt; \\rho_2 \\geq 2 \\rho_1^2 - 1 \\\\ &amp;\\Rightarrow 1 - \\rho_1^2 &gt; \\rho_2 - \\rho_1^2 \\geq -(1 - \\rho_1^2)\\\\ &amp;\\Rightarrow 1 &gt; \\frac{\\rho_2 - \\rho_1^2 }{1 - \\rho_1^2} \\geq 1, \\end{aligned}\\] imlying that \\(\\rho_1\\) and \\(\\rho_2\\) must lie in a parabolic shaped region defined by the above inequalities. Therefore, the restrictions on the autocorrelation are very complicated providing a motivation for other form of fundamental representation. 2.3 Stationarity 2.3.1 Definitions There are two kinds of stationarity which are commonly used. They are defined below: A process \\(\\{X_t\\}\\) is strongly stationary or strictly stationary if the joint probability distribution of \\(\\{X_{t-h}, ..., X_t, ..., X_{t+h}\\}\\) is independent of \\(t\\) for all \\(h\\). A process \\(\\{X_t\\}\\) is weakly stationary, covariance stationary or second order stationary if \\(E[X_t]\\), \\(E[X_t^2]\\) are finite and \\(E[X_t X_{t-h}]\\) depends only on \\(h\\) and not on \\(t\\). These types of stationarity are not equivalent and the presence of one kind of stationarity does not imply the other. That is, a time series can be strongly stationary but not weakly stationary and vice versa. In some cases, a time series can be both strong and weakly stationary, this is happends for example in the (joint) Gaussian case. Stationarity of \\(X_t\\) matters, because it provides the framework in which averaging dependent data makes sense. A few remarks: Strong stationarity \\(\\notimplies\\) weak stationarity. Example: an iid Cauchy process is strongly but not weakly stationary. Weak stationarity \\(\\notimplies\\) strong stationarity. Example: \\(X_{2t} = U_{2t}, X_{2t+1} = V_{2t+1} \\forall t\\) where \\({U_t}\\mathop \\sim \\limits^{iid} N\\left( {1,1} \\right)\\) and \\({V_t}\\mathop \\sim \\limits^{iid} Exponential\\left( 1 \\right)\\) is weakly stationary but NOT strongly stationary. Strong stationarity + \\(E[X_t]\\), \\(E[X_t^2] &lt; \\infty\\) \\(\\implies\\) weak stationarity Weak stationarity + normality \\(\\implies\\) strong stationarity. 2.3.2 Assessing Weak Stationarity of Time Series Models In order to verify if a process is weakly stationary, we must make sure the process satisfies: \\(E\\left[X_t \\right] = \\mu_t = \\mu &lt; \\infty\\), \\(\\operatorname{var}\\left[X_t \\right] = \\sigma^2_t = \\sigma^2 &lt; \\infty\\), \\(\\operatorname{cov}\\left(X_t, X_{t+h} \\right) = \\gamma \\left(h\\right)\\). 2.3.2.1 Example: Gaussian White Noise It is easy to verify that a Gaussian white noise is stationary. Indeed, we have: \\(E\\left[ {{X_t}} \\right] = 0\\), \\(\\gamma(0) = \\sigma^2 &lt; \\infty\\), \\(\\gamma(h) = 0\\) for \\(|h| &gt; 0\\). 2.3.2.2 Example: Random Walk To evaluate the stationarity of a random walk we first derive its properties: \\[\\begin{aligned} E\\left[ {{X_t}} \\right] &amp;= E\\left[ {{X_{t - 1}} + {W_t}} \\right] = E\\left[ {\\sum\\limits_{i = 1}^t {{W_t}} + {X_0}} \\right] \\\\ &amp;= E\\left[ {\\sum\\limits_{i = 1}^t {{W_t}} } \\right] + {X_0} = X_0 \\\\ \\end{aligned} \\] Note, the mean here is constant since it depends only on the value of the first term in the sequence. \\[\\begin{aligned} \\operatorname{var}\\left( {{X_t}} \\right) &amp;= \\operatorname{var}\\left( {\\sum\\limits_{i = 1}^t {{W_t}} + {X_0}} \\right) = \\operatorname{var}\\left( {\\sum\\limits_{i = 1}^t {{w_t}} } \\right) + \\underbrace {\\operatorname{var}\\left( {{X_0}} \\right)}_{= 0} \\\\ &amp;= \\sum\\limits_{i = 1}^t {Var\\left( {{w_t}} \\right)} = t \\sigma_w^2. \\end{aligned}\\] where \\(\\sigma_w^2 = \\operatorname{var}(W_t)\\). Therefore, the variance has a dependence on time and we have: \\[\\mathop {\\lim }\\limits_{t \\to \\infty } \\; \\operatorname{var}\\left(X_t\\right) = \\infty.\\] As a result, the process is not weakly stationary. Continuing on just to obtain the covariance, we have: \\[\\begin{aligned} \\gamma \\left( h \\right) &amp;= Cov\\left( {{y_t},{y_{t + h}}} \\right) = Cov\\left( {\\sum\\limits_{i = 1}^t {{w_i}} ,\\sum\\limits_{j = 1}^{t + h} {{w_j}} } \\right) \\\\ &amp;= Cov\\left( {\\sum\\limits_{i = 1}^t {{w_i}} ,\\sum\\limits_{j = 1}^t {{w_j}} } \\right) = \\min \\left( {t,t + h} \\right)\\sigma _w^2 \\\\ &amp;= \\left( {t + \\min \\left( {0,h} \\right)} \\right)\\sigma _w^2, \\end{aligned} \\] which also illustrates that non-stationarity of a random walk. In the following simulated example, we illustrate the non-stationary feature of such process: # In this example, we simulate a large number of random walks # Number of simulated processes B = 200 # Length of random walks n = 1000 # Output matrix out = matrix(NA,B,n) for (i in 1:B){ # Simulate random walk Xt = cumsum(rnorm(n)) # Store process out[i,] = Xt } # Plot random walks plot(NA, xlim = c(1,n), ylim = range(out), xlab = &quot;Time&quot;, ylab = &quot; &quot;) color = sample(topo.colors(B, alpha = 0.5)) for (i in 1:B){ lines(out[i,], col = color[i]) } # Add 95% confidence region lines(1:n, 1.96*sqrt(1:n), col = 2, lwd = 2, lty = 2) lines(1:n, -1.96*sqrt(1:n), col = 2, lwd = 2, lty = 2) The relationship between time and variance can clearly be observed in the above graph. 2.3.2.3 Example: MA(1) To evaluate the stationarity of an MA(1) process we first derive its properties: \\[\\begin{aligned} E\\left[ {{y_t}} \\right] &amp;= E\\left[ {{\\theta _1}{w_{t - 1}} + {w_t}} \\right] \\\\ &amp;= {\\theta _1}E\\left[ {{w_{t - 1}}} \\right] + E\\left[ {{w_t}} \\right] = 0 \\\\ \\end{aligned}\\] \\[\\begin{aligned} Cov\\left( {{y_t},{y_{t + h}}} \\right) &amp;= E\\left[ {\\left( {{y_t} - E\\left[ {{y_t}} \\right]} \\right)\\left( {{y_{t + h}} - E\\left[ {{y_{t + h}}} \\right]} \\right)} \\right] \\\\ &amp;= E\\left[ {{y_t}{y_{t + h}}} \\right] - \\underbrace {E\\left[ {{y_t}} \\right]}_{ = 0}\\underbrace {E\\left[ {{y_{t + h}}} \\right]}_{ = 0} \\\\ &amp;= E\\left[ {\\left( {{\\theta _1}{w_{t - 1}} + {w_t}} \\right)\\left( {{\\theta _1}{w_{t + h - 1}} + {w_{t + h}}} \\right)} \\right] \\\\ &amp;= E\\left[ {\\theta _1^2{w_{t - 1}}{w_{t + h - 1}} + \\theta {w_t}{w_{t + h}} + {\\theta _1}{w_{t - 1}}{w_{t + h}} + {w_t}{w_{t + h}}} \\right] \\\\ &amp;\\\\ E\\left[ {{w_t}{w_{t + h}}} \\right] &amp;= \\operatorname{cov} \\left( {{w_t},{w_{t + h}}} \\right) + E\\left[ {{w_t}} \\right]E\\left[ {{w_{t + h}}} \\right] = {1_{\\left\\{ {h = 0} \\right\\}}}\\sigma _w^2 \\\\ \\\\ &amp;\\Rightarrow Cov\\left( {{y_t},{y_{t + h}}} \\right) = \\left( {\\theta _1^2{1_{\\left\\{ {h = 0} \\right\\}}} + {\\theta _1}{1_{\\left\\{ {h = 1} \\right\\}}} + {\\theta _1}{1_{\\left\\{ {h = - 1} \\right\\}}} + {1_{\\left\\{ {h = 0} \\right\\}}}} \\right)\\sigma _w^2 \\\\ \\gamma \\left( h \\right) &amp;= \\left\\{ {\\begin{array}{*{20}{c}} {\\left( {\\theta _1^2 + 1} \\right)\\sigma _w^2}&amp;{h = 0} \\\\ {{\\theta _1}\\sigma _w^2}&amp;{\\left| h \\right| = 1} \\\\ 0&amp;{\\left| h \\right| &gt; 1} \\end{array}} \\right. \\end{aligned} \\] Therefore, an MA(1) process is weakly stationary since both the mean and variance are constant over time. In addition, we can easily obtain the autocorrelation function which is given by \\[\\Rightarrow \\rho \\left( h \\right) = \\left\\{ {\\begin{array}{*{20}{c}} 1&amp;{h = 0} \\\\ {\\frac{{{\\theta _1}\\sigma _w^2}}{{\\left( {\\theta _1^2 + 1} \\right)\\sigma _w^2}} = \\frac{{{\\theta _1}}}{{\\theta _1^2 + 1}}}&amp;{\\left| h \\right| = 1} \\\\ 0&amp;{\\left| h \\right| &gt; 1} \\end{array}} \\right.\\] Interestingly, we can note that \\(|\\rho(1)| \\leq 0.5\\). 2.3.2.4 Example: MA(1) Consider the AR(1) process given as: \\[{y_t} = {\\phi _1}{y_{t - 1}} + {w_t} \\text{, where } {w_t}\\mathop \\sim \\limits^{iid} WN\\left( {0,\\sigma _w^2} \\right)\\] This process was shown to simplify to: \\[y_t = {\\phi ^t}{y_0} + \\sum\\limits_{i = 0}^{t - 1} {\\phi _1^i{w_{t - i}}}\\] In addition, we add the requirement that \\(\\left| \\phi _1 \\right| &lt; 1\\). This requirement allows for the process to be stationary. If \\(\\phi _1 \\ge 1\\), the process would not converge. This way the process will be able to be written as a geometric series that converges: \\[\\sum\\limits_{k = 0}^\\infty {{r^k}} = \\frac{1}{{1 - r}},{\\text{ }}\\left| r \\right| &lt; 1\\] Next, we demonstrate how crucial this property is: \\[\\begin{aligned} \\mathop {\\lim }\\limits_{t \\to \\infty } E\\left[ {{y_t}} \\right] &amp;= \\mathop {\\lim }\\limits_{t \\to \\infty } E\\left[ {{\\phi ^t}{y_0} + \\sum\\limits_{i = 0}^{t - 1} {\\phi _1^i{w_{t - i}}} } \\right] \\\\ &amp;= \\mathop {\\lim }\\limits_{t \\to \\infty } \\underbrace {{\\phi ^t}{y_0}}_{\\left| \\phi \\right| &lt; 1 \\Rightarrow t \\to \\infty {\\text{ = 0}}} + \\sum\\limits_{i = 0}^{t - 1} {\\phi _1^i\\underbrace {E\\left[ {{w_{t - i}}} \\right]}_{ = 0}} \\\\ &amp;= 0 \\\\ \\mathop {\\lim }\\limits_{t \\to \\infty } Var\\left( {{y_t}} \\right) &amp;= \\mathop {\\lim }\\limits_{t \\to \\infty } Var\\left( {{\\phi ^t}{y_0} + \\sum\\limits_{i = 0}^{t - 1} {\\phi _1^i{w_{t - i}}} } \\right) \\\\ &amp;= \\mathop {\\lim }\\limits_{t \\to \\infty } \\underbrace {Var\\left( {{\\phi ^t}{y_0}} \\right)}_{ = 0{\\text{ since constant}}} + Var\\left( {\\sum\\limits_{i = 0}^{t - 1} {\\phi _1^i{w_{t - i}}} } \\right) \\\\ &amp;= \\mathop {\\lim }\\limits_{t \\to \\infty } \\sum\\limits_{i = 0}^{t - 1} {\\phi _1^{2i}Var\\left( {{w_{t - i}}} \\right)} \\\\ &amp;= \\mathop {\\lim }\\limits_{t \\to \\infty } \\sigma _w^2\\sum\\limits_{i = 0}^{t - 1} {\\phi _1^{2i}} \\\\ &amp;= \\sigma _w^2 \\cdot \\underbrace {\\frac{1}{{1 - {\\phi ^2}}}}_{\\begin{subarray}{l} {\\text{Geometric Series}} \\end{subarray}} \\end{aligned} \\] This leads us to being able to conclude the autocovariance function is: \\[\\begin{aligned} Cov\\left( {{y_t},{y_{t + h}}} \\right) &amp;= Cov\\left( {{y_t},\\phi {y_{t + h - 1}} + {w_{t + h}}} \\right) \\\\ &amp;= Cov\\left( {{y_t},\\phi {y_{t + h - 1}}} \\right) \\\\ &amp;= Cov\\left( {{y_t},{\\phi ^{\\left| h \\right|}}{y_t}} \\right) \\\\ &amp;= {\\phi ^{\\left| h \\right|}}Cov\\left( {{y_t},{y_t}} \\right) \\\\ &amp;= {\\phi ^{\\left| h \\right|}}Var\\left( {{y_t}} \\right) \\\\ &amp;= {\\phi ^{\\left| h \\right|}}\\frac{{\\sigma _w^2}}{{1 - \\phi _1^2}} \\\\ \\end{aligned} \\] Both the mean and autocovariance function do not depend on time and, thus, the AR(1) process is stationary if \\(\\left| \\phi _1 \\right| &lt; 1\\). If we assume that the AR(1) process is stationary, we can derive the mean and variance in another way. Without a loss of generality, we’ll assume \\(y_0 = 0\\). Therefore: \\[\\begin{aligned} {y_t} &amp;= {\\phi _t}{y_{t - 1}} + {w_t} \\\\ &amp;= {\\phi _1}\\left( {{\\phi _1}{y_{t - 2}} + {w_{t - 1}}} \\right) + {w_t} \\\\ &amp;= \\phi _1^2{y_{t - 2}} + {\\phi _1}{w_{t - 1}} + {w_t} \\\\ &amp;\\vdots \\\\ &amp;= \\sum\\limits_{i = 0}^{t - 1} {\\phi _1^i{w_{t - i}}} \\\\ &amp; \\\\ E\\left[ {{y_t}} \\right] &amp;= E\\left[ {\\sum\\limits_{i = 0}^{t - 1} {\\phi _1^i{w_{t - i}}} } \\right] \\\\ &amp;= \\sum\\limits_{i = 0}^{t - 1} {\\phi _1^i\\underbrace {E\\left[ {{w_{t - i}}} \\right]}_{ = 0}} \\\\ &amp;= 0 \\\\ &amp;\\\\ Var\\left( {{y_t}} \\right) &amp;= E\\left[ {{{\\left( {{y_t} - E\\left[ {{y_t}} \\right]} \\right)}^2}} \\right] \\\\ &amp;= E\\left[ {y_t^2} \\right] - {\\left( {E\\left[ {{y_t}} \\right]} \\right)^2} \\\\ &amp;= E\\left[ {y_t^2} \\right] \\\\ &amp;= E\\left[ {{{\\left( {{\\phi _1}{y_{t - 1}} + {w_t}} \\right)}^2}} \\right] \\\\ &amp;= E\\left[ {\\phi _1^2y_{t - 1}^2 + w_t^2 + 2{\\phi _1}{y_t}{w_t}} \\right] \\\\ &amp;= \\phi _1^2E\\left[ {y_{t - 1}^2} \\right] + \\underbrace {E\\left[ {w_t^2} \\right]}_{ = \\sigma _w^2} + 2{\\phi _1}\\underbrace {E\\left[ {{y_t}} \\right]}_{ = 0}\\underbrace {E\\left[ {{w_t}} \\right]}_{ = 0} \\\\ &amp;= \\underbrace {\\phi _1^2Var\\left( {{y_{t - 1}}} \\right) + \\sigma _w^2 = \\phi _1^2Var\\left( {{y_t}} \\right) + \\sigma _w^2}_{{\\text{Assume stationarity}}} \\\\ Var\\left( {{y_t}} \\right) &amp;= \\phi _1^2Var\\left( {{y_t}} \\right) + \\sigma _w^2 \\\\ Var\\left( {{y_t}} \\right) - \\phi _1^2Var\\left( {{y_t}} \\right) &amp;= \\sigma _w^2 \\\\ Var\\left( {{y_t}} \\right)\\left( {1 - \\phi _1^2} \\right) &amp;= \\sigma _w^2 \\\\ Var\\left( {{y_t}} \\right) &amp;= \\frac{{\\sigma _w^2}}{{1 - \\phi _1^2}} \\\\ \\end{aligned} \\] 2.3.3 Esimtation of the Mean Function If a time series is stationary, the mean function is constant and a possible estimator of this quantity is given by \\[\\bar{X} = {\\frac{1}{n}\\sum\\limits_{t = 1}^n {{X_t}} }.\\] This estimator is clearly unbiased and has the following variance: \\[\\begin{aligned} \\operatorname{var} \\left( {\\bar X} \\right) &amp;= \\operatorname{var} \\left( {\\frac{1}{n}\\sum\\limits_{t = 1}^n {{X_t}} } \\right) \\\\ &amp;= \\frac{1}{{{n^2}}}\\operatorname{var} \\left( {{{\\left[ {\\begin{array}{*{20}{c}} 1&amp; \\cdots &amp;1 \\end{array}} \\right]}_{1 \\times n}}{{\\left[ {\\begin{array}{*{20}{c}} {{X_1}} \\\\ \\vdots \\\\ {{X_n}} \\end{array}} \\right]}_{n \\times 1}}} \\right) \\\\ &amp;= \\frac{1}{{{n^2}}}{\\left[ {\\begin{array}{*{20}{c}} 1&amp; \\cdots &amp;1 \\end{array}} \\right]_{1 \\times n}}\\left[ {\\begin{array}{*{20}{c}} {\\gamma \\left( 0 \\right)}&amp;{\\gamma \\left( 1 \\right)}&amp; \\cdots &amp;{\\gamma \\left( {n - 1} \\right)} \\\\ {\\gamma \\left( 1 \\right)}&amp;{\\gamma \\left( 0 \\right)}&amp;{}&amp; \\vdots \\\\ \\vdots &amp;{}&amp; \\ddots &amp; \\vdots \\\\ {\\gamma \\left( {n - 1} \\right)}&amp; \\cdots &amp; \\cdots &amp;{\\gamma \\left( 0 \\right)} \\end{array}} \\right]{\\left[ {\\begin{array}{*{20}{c}} 1 \\\\ \\vdots \\\\ 1 \\end{array}} \\right]_{n \\times 1}} \\\\ &amp;= \\frac{1}{{{n^2}}}\\left( {n\\gamma \\left( 0 \\right) + 2\\left( {n - 1} \\right)\\gamma \\left( 1 \\right) + 2\\left( {n - 2} \\right)\\gamma \\left( 2 \\right) + \\cdots + 2\\gamma \\left( {n - 1} \\right)} \\right) \\\\ &amp;= \\frac{1}{n}\\sum\\limits_{h = - n}^n {\\left( {1 - \\frac{{\\left| h \\right|}}{n}} \\right)\\gamma \\left( h \\right)} \\\\ \\end{aligned}. \\] In the white noise case, the above formula reduces to the usual \\(\\operatorname{var} \\left( {\\bar X} \\right) = \\operatorname{var}(X_t)/n\\). 2.3.4 Sample Autocovariance and Autocorrelation Functions A natural estimator of the autocovariance function is given as: \\[\\hat \\gamma \\left( h \\right) = \\frac{1}{T}\\sum\\limits_{t = 1}^{T - h} {\\left( {{X_t} - \\bar X} \\right)\\left( {{X_{t + h}} - \\bar X} \\right)} \\] leading the following “plug-in” estimator of the autocorrelation function \\[\\hat \\rho \\left( h \\right) = \\frac{{\\hat \\gamma \\left( h \\right)}}{{\\hat \\gamma \\left( 0 \\right)}}.\\] A graphical representation of the autocorrelation function is often the first step of any time series analysis (assuming the process to be stationary). Consider the following simulated example: # Simulate iid gaussian RV (i.e. white noise) Xt = rnorm(100) # Compute autocorrelation acf_Xt = acf(Xt) # Plot autocorrelation plot(acf_Xt) In this example, the true autocorrelation at lag \\(h\\) (\\(|h|\\) &gt; 0 ) is equal 0 but obviously the estimated autocorrelations are random variables and are not equal to their true value. It would therefore be usefull to have have some knowledge about the variability of the sample autocorrelations (under some conditions) to assess whether the data comes from a completely random series or presents some significant correlation at some lags. The following result provide an asymptotic solution to this problem: If \\(X_t\\) is white noise with finite fourth moment, then \\(\\hat{\\rho}(h)\\) is approximately normally distributed with mean \\(0\\) and variance \\(T^{-1}\\) for all fixed \\(h\\). Using on this result, we now have an approximate method to assess whether peaks in sample autocorrelation are significant by determining whether the observed peak lies outside the interval \\(+/- 2/\\sqrt{T}\\) (i.e. an approximate 95% confidence interval). Returning to our previous example: # Plot autocorrelation with confidence bands plot(acf_Xt) It can now be observed that most peaks lies within the interval \\(+/- 2/\\sqrt{T}\\) suggesting that the true data generating process is completely random (in the linear sense). Unfortunately, this method is asymptotic (it relies on the central limit theorem) and there no “exact” tools that can be used in this case. In the simulation study below consider the “quality” of this result for \\(h = 3\\) considering different sample sizes: # Number of Monte Carlo replications B = 10000 # Define considered lag h = 3 # Sample size considered T = c(5,10,30,300) # Initialisation result = matrix(NA,B,length(T)) # Set seed set.seed(1) # Start Monte Carlo for (i in 1:B){ for (j in 1:length(T)){ # Simluate process Xt = rnorm(T[j]) # Save autocorrelation at lag h result[i,j] = acf(Xt, plot = FALSE)$acf[h+1] } } # Plot results par(mfrow = c(1,length(T))) for (i in 1:length(T)){ # Estimated empirical distribution hist(result[,i], col = &quot;lightgrey&quot;, main = paste(&quot;Sample size T =&quot;,T[i]), probability = TRUE, xlim = c(-1,1), xlab = &quot; &quot;) # Asymptotic distribution xx = seq(from = -10, to = 10, length.out = 10^3) yy = dnorm(xx,0,1/sqrt(T[i])) lines(xx,yy, col = &quot;red&quot;) } It can clearly be observed that asymptotic approximation is quite poor when \\(T = 5\\) but as the sample size increases the approximation becomes more appropriate and is nearly perfect with \\(T = 300\\). 2.4 Joint Stationarity Two time series, say \\(\\left(X_t \\right)\\) and \\(\\left(Y_t\\right)\\), are said to be jointly stationary if they are each stationary, and the cross-covariance function \\[{\\gamma _{XY}}\\left( {t,t + h} \\right) = Cov\\left( {{X_t},{Y_{t + h}}} \\right) = {\\gamma _{XY}}\\left( h \\right)\\] is a function only of lag \\(h\\). The cross-correlation function for jointly stationary times can be expressed as: \\[{\\rho _{XY}}\\left( {t,t + h} \\right) = \\frac{{{\\gamma _{XY}}\\left( {t,t + h} \\right)}}{{{\\sigma _{{X_t}}}{\\sigma _{{Y_{t + h}}}}}} = \\frac{{{\\gamma _{XY}}\\left( h \\right)}}{{{\\sigma _{{X_t}}}{\\sigma _{{Y_{t + h}}}}}} = {\\rho _{XY}}\\left( h \\right)\\] "],
["basic-models.html", "Chapter 3 Basic Models 3.1 The Backshift Operator 3.2 White Noise 3.3 Moving Average Process of Order q = 1 a.k.a MA(1) 3.4 Drift 3.5 Random Walk 3.6 Random Walk with Drift 3.7 Autoregressive Process of Order p = 1 a.k.a AR(1)", " Chapter 3 Basic Models 3.1 The Backshift Operator Definition: Backshift Operator The Backshift Operator is helpful when manipulating time series. When we backshift, we are changing the indices of the time series. e.g. \\(t \\rightarrow t-1\\). The operator is defined as: \\[B{x_t} = {x_{t - 1}}\\] If we were to repeatedly apply the backshift operator, we would receive: \\[\\begin{aligned} {B^2}{x_t} &amp;= B\\left( {B{x_t}} \\right) \\\\ &amp;= B\\left( {{x_{t - 1}}} \\right) \\\\ &amp;= {x_{t - 2}} \\\\ \\end{aligned}\\] We can generalize this behavior as: \\[{B^k}{x_t} = {x_{t - k}}\\] The backshift operator is helpful for later decompositions in addition to making differencing operations more straightforward. 3.2 White Noise The process name of white noise has meaning in the notion of colors of noise. Specifically, the white noise is a process that mirrors white light’s flat frequency spectrum. So, the process has equal frequencies in any interval of time. Definition: White Noise \\(w_t\\) or \\(\\varepsilon _t\\) is a white noise process if \\(w_t\\) are uncorrelated identically distributed random variables with \\(E\\left[w_t\\right] = 0\\) and \\(Var\\left[w_t\\right] = \\sigma ^2\\), for all \\(t\\). We can represent this algebraically as: \\[y_t = w_t,\\] where \\({w_t}\\mathop \\sim \\limits^{id} WN\\left( {0,\\sigma _w^2} \\right)\\) Now, if the \\(w_t\\) are Normally (Gaussian) distributed, then the process is known as a Gaussian White Noise e.g. \\({w_t}\\mathop \\sim \\limits^{iid} N\\left( {0,{\\sigma ^2}} \\right)\\) To generate gaussian white noise use: set.seed(1336) # Set seed to reproduce the results n = 200 # Number of observations to generate wn = ts(rnorm(n,0,1)) # Generate Guassian white noise. autoplot(wn) + ggtitle(&quot;White Noise Process&quot;) + ylab(&quot;Displacement&quot;) + xlab(&quot;Time (seconds)&quot;) 3.3 Moving Average Process of Order q = 1 a.k.a MA(1) Definition: Moving Average Process of Order (q = 1) The concept of a Moving Average Process of Order q is a way to remove “noise” and emphasize the signal. The moving average achieves this by taking the local averages of the data to produce a new smoother time series series. The newly created time series is more descriptive, but it does influence the dependence within the time series. This process is generally denoted as MA(1) and is defined as: \\[{y_t} = {\\theta _1}{w_{t - 1}} + {w_t},\\] where \\({w_t}\\mathop \\sim \\limits^{iid} WN\\left( {0,\\sigma _w^2} \\right)\\) set.seed(1345) # Set seed to reproduce the results n = 200 # Number of observations to generate sigma2 = 2 # Controls variance of Guassian white noise. theta = 0.3 # Handles the theta component of MA(1) # Generate a white noise wn = rnorm(n+1, sd = sqrt(sigma2)) # Simulate the MA(1) process ma = rep(0, n+1) for(i in 2:(n+1)) { ma[i] = theta*wn[i-1] + wn[i] } ma = ts(ma[2:(n+1)]) # Remove first item autoplot(ma) + ggtitle(&quot;Moving Average Order 1 Process&quot;) + ylab(&quot;Displacement&quot;) + xlab(&quot;Time (seconds)&quot;) 3.4 Drift Definition: Drift A drift process has two components: time and a slope. As more points are accumlated over time, the drift will match the common slope form. Specifically, the drift process has the following form: \\[y_t = y_{t-1} + \\delta \\] with the initial condition \\(y_0 = c\\). The process can be simplified using backsubstitution to being: \\[\\begin{aligned} {y_t} &amp;= {y_{t - 1}} + \\delta \\\\ &amp;= \\left( {{y_{t - 2}} + \\delta} \\right) + \\delta \\\\ &amp;\\vdots \\\\ &amp;= \\sum\\limits_{i = 1}^t {\\delta} + y_0 \\\\ {y_t} &amp;= t{\\delta} + c \\\\ \\end{aligned} \\] Again, note that a drift is similar to the slope-intercept form a linear line. e.g. \\(y = mx + b\\). To generate a drift use: n = 200 # Number of observations to generate drift = .3 # Drift Control dr = ts(drift*(1:n)) # Generate drift sequence (e.g. y = drift*x + 0) autoplot(dr) + ggtitle(&quot;Drift Process&quot;) + ylab(&quot;Displacement&quot;) + xlab(&quot;Time (seconds)&quot;) 3.5 Random Walk In 1906, Karl Pearson coined the term ‘random walk’ and demonstrated that “the most likely place to find a drunken walker is somewhere near his starting point.” Empirical evidence of this phenomenon is not too hard to find on a Friday night in Champaign. Definition: Random Walk A random walk is defined as a process where the current value of a variable is composed of the past value plus an error term that is a white noise. In algebraic form, \\[y_t = y_{t-1} + w_t\\] with the initial condition \\(y_0 = c\\). The process can be simplified using backsubstitution to being: \\[\\begin{aligned} {y_t} &amp;= {y_{t - 1}} + {w_t} \\\\ &amp;= \\left( {{y_{t - 2}} + {w_{t - 1}}} \\right) + {w_t} \\\\ &amp;\\vdots \\\\ {y_t} &amp;= \\sum\\limits_{i = 1}^t {{w_i}} + y_0 = \\sum\\limits_{i = 1}^t {{w_i}} + c \\\\ \\end{aligned} \\] To generate a random walk, we use: set.seed(1336) # Set seed to reproduce the results n = 200 # Number of observations to generate w = rnorm(n,0,1) # Generate Guassian white noise. rw = ts(cumsum(w)) # Cumulative sum # Create a data.frame to graph in ggplot2 autoplot(rw) + ggtitle(&quot;Random Walk&quot;) + ylab(&quot;Displacement&quot;) + xlab(&quot;Time (seconds)&quot;) 3.6 Random Walk with Drift In the previous case of a random walk, we assumed that drift, \\(\\delta\\), was equal to 0. What happens to the random walk if the drift is not equal to zero? That is, what happens with the initial condition \\(y_0 = c\\)? \\[\\begin{aligned} {y_t} &amp;= {y_{t - 1}} + {w_t} + \\delta \\\\ &amp;= \\left( {{y_{t - 2}} + {w_{t - 1}} + \\delta} \\right) + {w_t} + \\delta \\\\ &amp;\\vdots \\\\ {y_t} &amp;= \\sum\\limits_{i = 1}^t {\\left({w_{i} + \\delta}\\right)} + y_0 = \\sum\\limits_{i = 1}^t {{w_i}} + t\\delta + c \\\\ \\end{aligned} \\] To generate a random walk with drift we use: set.seed(1336) # Set seed to reproduce the results n = 200 # Number of observations to generate drift = .3 # Drift Control w = rnorm(n,0,1) # Generate Guassian white noise. wd = w + drift # Add a drift rwd = ts(cumsum(wd)) # Cumulative sum # Create a data.frame to graph in ggplot2 autoplot(rwd) + ggtitle(&quot;Random Walk with Drift&quot;) + ylab(&quot;Displacement&quot;) + xlab(&quot;Time (seconds)&quot;) Notice the difference the drift makes upon the random walk: # Add identifiers drift.df = data.frame(Index = 1:n, Data = drift*(1:n), Type = &quot;Drift&quot;) rw.df = data.frame(Index = 1:n, Data = rw, Type = &quot;Random Walk&quot;) rwd.df = data.frame(Index = 1:n, Data = rwd, Type = &quot;Random Walk with Drift&quot;) combined.df = rbind(drift.df, rw.df, rwd.df) ggplot(data = combined.df, aes(x = Index, y = Data, colour = Type)) + geom_line() + ggtitle(&quot;Comparisons of Random Walk&quot;) + ylab(&quot;Displacement&quot;) + xlab(&quot;Time (seconds)&quot;) 3.7 Autoregressive Process of Order p = 1 a.k.a AR(1) Definition: Autoregressive Process of Order p = 1 This process is generally denoted as AR(1) and is defined as: \\({y_t} = {\\phi _1}{y_{t - 1}} + {w_t},\\) where \\({w_t}\\mathop \\sim \\limits^{iid} WN\\left( {0,\\sigma _w^2} \\right)\\) If \\(\\phi _1 = 1\\), then the process is equivalent to a random walk. The process can be simplified using backsubstitution to being: \\[\\begin{aligned} {y_t} &amp;= {\\phi _t}{y_{t - 1}} + {w_t} \\\\ &amp;= {\\phi _1}\\left( {{\\phi _1}{y_{t - 2}} + {w_{t - 1}}} \\right) + {w_t} \\\\ &amp;= \\phi _1^2{y_{t - 2}} + {\\phi _1}{w_{t - 1}} + {w_t} \\\\ &amp;\\vdots \\\\ &amp;= {\\phi ^t}{y_0} + \\sum\\limits_{i = 0}^{t - 1} {\\phi _1^i{w_{t - i}}} \\end{aligned}\\] set.seed(1345) # Set seed to reproduce the results n = 200 # Number of observations to generate sigma2 = 2 # Controls variance of Guassian white noise. phi = 0.3 # Handles the phi component of AR(1) wn = rnorm(n+1, sd = sqrt(sigma2)) # Simulate the MA(1) process ar = rep(0,n+1) for(i in 2:n) { ar[i] = phi*ar[i-1] + wn[i] } ar = ts(ar[2:(n+1)]) autoplot(ar) + ggtitle(&quot;Autoregressive Order 1 Process&quot;) + ylab(&quot;Displacement&quot;) + xlab(&quot;Time (seconds)&quot;) "],
["arma.html", "Chapter 4 ARMA 4.1 Definition 4.2 MA / AR Operators 4.3 Redundancy 4.4 Causal + Invertible 4.5 Estimation of Parameters 4.6 Method of Moments 4.7 Prediction (Forecast)", " Chapter 4 ARMA 4.1 Definition 4.2 MA / AR Operators 4.3 Redundancy 4.4 Causal + Invertible 4.5 Estimation of Parameters Consider a time series given by \\(x_t \\sim ARMA(p,q)\\). This gives us with a paramter space \\(\\Omega\\) that looks like so: \\[\\vec \\varphi = \\left[ {\\begin{array}{*{20}{c}} {{\\phi _1}} \\\\ \\vdots \\\\ {{\\phi _p}} \\\\ {{\\theta _1}} \\\\ \\vdots \\\\ {{\\theta _q}} \\\\ {{\\sigma ^2}} \\end{array}} \\right]\\] In order to estimate this parameter space, we must assume the following three conditions: The process is casual The process is invertible The process has Gaussian innovations. Innovations are a time series equivalent to residuals. That is, an innovation is given by \\({x_t} - \\hat x_t^{t - 1}\\), where \\(\\hat x_t^{t - 1}\\) is the prediction at time \\(t\\) given \\(t-1\\) observations and \\({x_t}\\) is the true value observed at time \\(t\\). There are two main ways of performing such an estimation of the parameter space. Maximum Likelihood / Least Squares Estimation [MLE / LSE] Method of Moments (MoM) To begin, we’ll explore using the MLE to perform the estimation. 4.5.1 Maximum Likelihood Estimation Definition Consider \\(X_n = (X_1, X_2, \\ldots, X_n)\\) with the joint density \\(f(X_1, X_2, \\ldots, X_n ; \\theta)\\) where \\(\\theta \\in \\Theta\\). Given \\(X_1 = x_1, X_2 = x_2, \\ldots, X_n = x_n\\) is observed, we have the likelihood function of \\(\\theta\\) as \\[L(\\theta) = L(\\theta|x_1,x_2, \\ldots, x_n) = f(x_1,x_2, \\ldots, x_n | \\theta)\\] If the \\(X_i\\) are iid, then the likelihood simplifies to: \\[L(\\theta) = \\prod\\limits_{i = 1}^n {f\\left( {{x_i}|\\theta } \\right)} \\] However, that’s a bit painful to maximize with calculus. So, we opt to use the log of the function since derivatives are easier and the logarithmic function is always increasing. Thus, we traditionally use: \\[l\\left( \\theta \\right) = \\log \\left( {L\\left( \\theta \\right)} \\right) = \\sum\\limits_{i = 1}^n {\\log \\left( {f\\left( {{x_i}|\\theta } \\right)} \\right)} \\] From maximizes the likelihood function \\(L(\\theta)\\), we get the maximum likelihood estimate (MLE) of \\(\\theta\\). So, we end up with a value that makes the observed data the “most probable.” Note: The likelihood function is not a probability density function. 4.5.1.1 \\(AR(1)\\) with mean \\(\\mu\\) Consider an \\(AR(1)\\) process given as \\(y_t = \\phi y_{t-1} + w_t\\), \\({w_t}\\mathop \\sim \\limits^{iid} N\\left( {0,{\\sigma ^2}} \\right)\\), with \\(E\\left[ {{y_t}} \\right] = 0\\), \\(\\left| \\phi \\right| &lt; 1\\). Let \\(x_t = y_t + \\mu\\), so that \\(E\\left[ {{x_t}} \\right] = \\mu\\). Then, \\({x_t} - \\mu = {y_t}\\). Substituting in for \\(y_t\\), we get: \\[\\begin{aligned} y_t &amp;= \\phi y_{t-1} + w_t \\\\ \\underbrace {\\left( {{x_t} - \\mu } \\right)}_{ = {y_t}} &amp;= \\phi \\underbrace {\\left( {{x_{t - 1}} - \\mu } \\right)}_{ = {y_t}} + {w_t} \\\\ {x_t} &amp;= \\mu + \\phi \\left( {{x_{t - 1}} - \\mu } \\right) + {w_t} \\end{aligned}\\] In this case, \\(x_t\\) is an \\(AR(1)\\) process with mean \\(\\mu\\). This means that we have: \\(E\\left[ {{x_t}} \\right] = \\mu\\) \\[\\begin{aligned} Var\\left( {{x_t}} \\right) &amp;= Var\\left( {{x_t} - \\mu } \\right) \\hfill \\\\ &amp;= Var\\left( {{y_t}} \\right) \\hfill \\\\ &amp;= Var\\left( {\\sum\\limits_{j = 0}^\\infty {{\\phi ^j}{w_{t - j}}} } \\right) \\hfill \\\\ &amp;= \\sum\\limits_{j = 0}^\\infty {{\\phi ^{2j}}Var\\left( {{w_{t - j}}} \\right)} \\hfill \\\\ &amp;= {\\sigma ^2}\\sum\\limits_{j = 0}^\\infty {{\\phi ^{2j}}} \\hfill \\\\ &amp;= \\frac{{{\\sigma ^2}}}{{1 - {\\phi ^2}}},{\\text{ since }}\\left| \\phi \\right| &lt; 1{\\text{ and }}\\sum\\limits_{k = 0}^n {a{r^k}} = \\frac{a}{{1 - r}} \\hfill \\\\ \\end{aligned} \\] So, \\(x_t \\sim N\\left({ \\mu, \\frac{{{\\sigma ^2}}}{{1 - {\\phi ^2}}} }\\right)\\). Note that the distribution of \\(x_t\\) is normal and, thus, the density function of \\(x_t\\) is given by: \\[\\begin{aligned} f\\left( {{x_t}} \\right) &amp;= \\sqrt {\\frac{{1 - {\\phi ^2}}}{{2\\pi {\\sigma ^2}}}} \\exp \\left( { - \\frac{1}{2} \\cdot \\frac{{1 - {\\phi ^2}}}{{{\\sigma ^2}}} \\cdot {{\\left( {{x_t} - \\mu } \\right)}^2}} \\right) \\hfill \\\\ &amp;= {\\left( {2\\pi } \\right)^{ - \\frac{1}{2}}}{\\left( {{\\sigma ^2}} \\right)^{ - \\frac{1}{2}}}{\\left( {1 - {\\phi ^2}} \\right)^{\\frac{1}{2}}}\\exp \\left( { - \\frac{1}{2} \\cdot \\frac{{1 - {\\phi ^2}}}{{{\\sigma ^2}}} \\cdot {{\\left( {{x_t} - \\mu } \\right)}^2}} \\right) \\textrm{ [1]} \\\\ \\end{aligned} \\] We’ll call the last equation [1]. 4.5.1.2 Conditioning time \\(x_t | x_{t-1}\\) Now, consider \\(x_t | x_{t-1}\\) for \\(t &gt; 1\\). The mean is given by: \\[\\begin{aligned} E\\left[ {{x_t}|{x_{t - 1}}} \\right] &amp;= E\\left[ {\\mu + \\phi \\left( {{x_{t - 1}} - \\mu } \\right) + {w_t}|{x_{t - 1}}} \\right] \\nonumber \\\\ &amp;= \\mu + \\phi \\left( {{x_{t - 1}} - \\mu } \\right) \\end{aligned} \\] This is the case since \\(E\\left[ {{x_{t - 1}}|{x_{t - 1}}} \\right] = {x_{t - 1}}\\) and \\(E\\left[ {{w_t}|{x_{t - 1}}} \\right] = 0\\) Now, the variance is: \\[\\begin{aligned} Var\\left( {{x_t}|{x_{t - 1}}} \\right) &amp;= Var\\left( {\\mu + \\phi \\left( {{x_{t - 1}} - \\mu } \\right) + {w_t}|{x_{t - 1}}} \\right) \\hfill \\\\ &amp;= \\underbrace {Var\\left( {\\mu + \\phi \\left( {{x_{t - 1}} - \\mu } \\right)|{x_{t - 1}}} \\right)}_{ = 0} + Var\\left( {{w_t}|{x_{t - 1}}} \\right) \\hfill \\\\ &amp;= Var\\left( {{w_t}} \\right) \\hfill \\\\ &amp;= {\\sigma ^2} \\hfill \\\\ \\end{aligned} \\] Thus, we have: \\({x_t}\\sim N\\left( {\\mu + \\phi \\left( {{x_{t - 1}} - \\mu } \\right),{\\sigma ^2}} \\right)\\). Again, note that the distribution of \\(x_t\\) is normal and, thus, the density function of \\(x_t\\) is given by: \\[\\begin{aligned} f\\left( {{x_t}} \\right) &amp;= \\sqrt {\\frac{1}{{2\\pi {\\sigma ^2}}}} \\exp \\left( { - \\frac{1}{{2{\\sigma ^2}}} \\cdot {{\\left[ {\\left( {{x_t} - \\mu } \\right) - \\phi \\left( {{x_{t - 1}} - \\mu } \\right)} \\right]}^2}} \\right) \\hfill \\\\ &amp;= {\\left( {2\\pi } \\right)^{ - \\frac{1}{2}}}{\\left( {{\\sigma ^2}} \\right)^{ - \\frac{1}{2}}}\\exp \\left( { - \\frac{1}{{2{\\sigma ^2}}} \\cdot {{\\left[ {\\left( {{x_t} - \\mu } \\right) - \\phi \\left( {{x_{t - 1}} - \\mu } \\right)} \\right]}^2}} \\right) \\textrm{ [2]} \\\\ \\end{aligned} \\] And for this equation we’ll call it [2]. 4.5.2 MLE for \\(\\sigma ^2\\) on \\(AR(1)\\) with mean \\(\\mu\\) Whew, with all of the above said, we’re now ready to obtain an MLE estimate on an \\(AR(1)\\). Let \\(\\vec{\\theta} = \\left[ {\\begin{array}{*{20}{c}} \\mu \\\\ \\phi \\\\ {{\\sigma ^2}} \\end{array}} \\right]\\), then the likelihood of \\(\\vec{\\theta}\\) is given by \\(x_1, \\ldots , x_T\\) is: \\[\\begin{aligned} L\\left( {\\vec \\theta |{x_1}, \\ldots ,{x_T}} \\right) &amp;= f\\left( {{x_1}, \\ldots ,{x_T}|\\vec \\theta } \\right) \\hfill \\\\ &amp;= f\\left( {{x_1}} \\right) \\cdot \\prod\\limits_{t = 2}^T {f\\left( {{x_t}|{x_{t - 1}}} \\right)} \\end{aligned} \\] The last equality is the result of us using a lag 1 of “memory.” Also, note that \\(x_t | x_{t-1}\\) must have \\(t &gt; 1 \\in \\mathbb{N}\\). Furthermore, we have dropped the parameters in the densities, e.g. \\(\\vec{\\theta}\\) in \\(f(\\cdot)\\), to ease notation. Using equations [1] and [2], we have: \\[L\\left( {\\vec \\theta |{x_1}, \\ldots ,{x_T}} \\right) = {\\left( {2\\pi } \\right)^{ - \\frac{T}{2}}}{\\left( {{\\sigma ^2}} \\right)^{ - \\frac{T}{2}}}{\\left( {1 - {\\phi ^2}} \\right)^{\\frac{1}{2}}}\\exp \\left( { - \\frac{1}{{2{\\sigma ^2}}}\\left[ {\\left( {1 - {\\phi ^2}} \\right){{\\left( {{x_t} - \\mu } \\right)}^2} + \\sum\\limits_{t = 2}^T {{{\\left[ {\\left( {{x_t} - \\mu } \\right) - \\phi \\left( {{x_{t - 1}} - \\mu } \\right)} \\right]}^2}} } \\right]} \\right)\\] For convenience, we’ll define: \\[S\\left( {\\mu ,\\phi } \\right) = \\left( {1 - {\\phi ^2}} \\right){\\left( {{x_t} - \\mu } \\right)^2} + \\sum\\limits_{t = 2}^T {{{\\left[ {\\left( {{x_t} - \\mu } \\right) - \\phi \\left( {{x_{t - 1}} - \\mu } \\right)} \\right]}^2}} \\] Fun fact, this is called the “unconditional sum of squares.” Thus, we will operate on: \\[L\\left( {\\vec \\theta |{x_1}, \\ldots ,{x_T}} \\right) = {\\left( {2\\pi } \\right)^{ - \\frac{T}{2}}}{\\left( {{\\sigma ^2}} \\right)^{ - \\frac{T}{2}}}{\\left( {1 - {\\phi ^2}} \\right)^{\\frac{1}{2}}}\\exp \\left( { - \\frac{1}{{2{\\sigma ^2}}}S\\left( {\\mu ,\\phi } \\right)} \\right)\\] Taking the log of this yields: \\[\\begin{aligned} l\\left( {\\vec \\theta |{x_1}, \\ldots ,{x_T}} \\right) &amp;= \\log \\left( {L\\left( {\\vec \\theta |{x_1}, \\ldots ,{x_T}} \\right)} \\right) \\hfill \\\\ &amp;= - \\frac{T}{2}\\log \\left( {2\\pi } \\right) - \\frac{T}{2}\\log \\left( {{\\sigma ^2}} \\right) + \\frac{1}{2}\\left( {1 - {\\phi ^2}} \\right) - \\frac{1}{{2{\\sigma ^2}}}S\\left( {\\mu ,\\phi } \\right) \\hfill \\\\ \\end{aligned} \\] Now, taking the derivative and solving for the maximized point gives: \\[\\begin{aligned} \\frac{\\partial }{{\\partial {\\sigma ^2}}}l\\left( {\\vec \\theta |{x_1}, \\ldots ,{x_T}} \\right) &amp;= - \\frac{T}{{2{\\sigma ^2}}} + \\frac{1}{{2{\\sigma ^4}}}S\\left( {\\mu ,\\phi } \\right) \\hfill \\\\ 0 &amp;= - \\frac{T}{{2{\\sigma ^2}}} + \\frac{1}{{2{\\sigma ^4}}}S\\left( {\\mu ,\\phi } \\right) \\hfill \\\\ \\frac{T}{{2{\\sigma ^2}}} &amp;= \\frac{1}{{2{\\sigma ^4}}}S\\left( {\\mu ,\\phi } \\right) \\hfill \\\\ {{ \\sigma }^2} &amp;= \\frac{1}{T}S\\left( {\\mu ,\\phi } \\right) \\hfill \\\\ \\end{aligned} \\] Thus, the MLE for \\({\\hat \\sigma }^2 = \\frac{1}{T}S\\left( {\\hat \\mu ,\\hat \\phi } \\right)\\), where \\(\\hat \\mu\\) and \\(\\hat \\phi\\) are the MLEs for \\(\\mu , \\phi\\) that are obtained numerically via either Newton Raphson or a Scoring Algorithm. (More details in a numerical recipe book.) 4.5.2.1 Conditional MLE on \\(AR(1)\\) with mean \\(\\mu\\) A common strategy to reduce the dependency on numerical recipes is to simplify \\(l\\left( {\\vec \\theta |{x_1}, \\ldots ,{x_T}} \\right)\\) by using \\({l^*}\\left( {\\vec \\theta |{x_1}, \\ldots ,{x_T}} \\right)\\): \\[\\begin{aligned} {l^*}\\left( {\\vec \\theta |{x_1}, \\ldots ,{x_T}} \\right) &amp;= \\prod\\limits_{t = 2}^T {\\log \\left( {f\\left( {{x_t}|{x_{t - 1}}} \\right)} \\right)} \\hfill \\\\ &amp;= \\prod\\limits_{t = 2}^T {\\log \\left( {{{\\left( {2\\pi } \\right)}^{ - \\frac{1}{2}}}{{\\left( {{\\sigma ^2}} \\right)}^{ - \\frac{1}{2}}}\\exp \\left( { - \\frac{1}{{2{\\sigma ^2}}} \\cdot {{\\left[ {\\left( {{x_t} - \\mu } \\right) - \\phi \\left( {{x_{t - 1}} - \\mu } \\right)} \\right]}^2}} \\right)} \\right)} \\hfill \\\\ &amp;= - \\frac{{\\left( {T - 1} \\right)}}{2}\\log \\left( {2\\pi } \\right) - \\frac{{\\left( {T - 1} \\right)}}{2}\\log \\left( {{\\sigma ^2}} \\right) - \\frac{1}{{2{\\sigma ^2}}}\\sum\\limits_{t = 2}^T {{{\\left[ {\\left( {{x_t} - \\mu } \\right) - \\phi \\left( {{x_{t - 1}} - \\mu } \\right)} \\right]}^2}} \\hfill \\\\ \\end{aligned} \\] Again, for convenience, we’ll define: \\[{S_c}\\left( {\\mu ,\\phi } \\right) = \\sum\\limits_{t = 2}^T {{{\\left[ {\\left( {{x_t} - \\mu } \\right) - \\phi \\left( {{x_{t - 1}} - \\mu } \\right)} \\right]}^2}} \\] Fun fact, this is called the “conditional sum of squares.” So, we will use: \\[{l^*}\\left( {\\vec \\theta |{x_1}, \\ldots ,{x_T}} \\right) = - \\frac{{\\left( {T - 1} \\right)}}{2}\\log \\left( {2\\pi } \\right) - \\frac{{\\left( {T - 1} \\right)}}{2}\\log \\left( {{\\sigma ^2}} \\right) - \\frac{1}{{2{\\sigma ^2}}}{S_c}\\left( {\\mu ,\\phi } \\right)\\] Taking the derivative with respect to \\(\\mu\\) gives: \\[\\begin{aligned} \\frac{\\partial }{{\\partial \\mu }}{l^*}\\left( {\\vec \\theta |{x_1}, \\ldots ,{x_T}} \\right) &amp;= - \\frac{1}{{2{\\sigma ^2}}}\\sum\\limits_{t = 2}^T {2\\left[ {\\left( {{x_t} - \\mu } \\right) - \\phi \\left( {{x_{t - 1}} - \\mu } \\right)} \\right]\\left( {\\phi - 1} \\right)} \\hfill \\\\ &amp;= \\frac{{1 - \\phi }}{{{\\sigma ^2}}}\\sum\\limits_{t = 2}^T {\\left[ {\\left( {{x_t} - \\mu } \\right) - \\phi \\left( {{x_{t - 1}} - \\mu } \\right)} \\right]} \\hfill \\\\ &amp;= \\frac{{1 - \\phi }}{{{\\sigma ^2}}}\\sum\\limits_{t = 2}^T {\\left( {{x_t} - \\phi {x_{t - 1}} - \\mu \\left( {1 - \\phi } \\right)} \\right)} \\hfill \\\\ &amp;= -\\frac{{{{\\left( {1 - \\phi } \\right)}^2}}}{{{\\sigma ^2}}}\\mu \\left( {T - 1} \\right) + \\frac{{\\left( {1 - \\phi } \\right)}}{{{\\sigma ^2}}}\\sum\\limits_{t = 2}^T {\\left( {{x_t} - \\phi {x_{t - 1}}} \\right)} \\hfill \\\\ \\end{aligned} \\] Solving for \\(\\mu^{*}\\) gives: \\[\\begin{aligned} 0 &amp;= \\frac{\\partial }{{\\partial \\mu }}{l^*}\\left( {\\vec \\theta |{x_1}, \\ldots ,{x_t}} \\right) \\hfill \\\\ 0 &amp;= - \\frac{{{{\\left( {1 - \\phi } \\right)}^2}}}{{{\\sigma ^2}}}{\\mu ^*}\\left( {T - 1} \\right) + \\frac{{\\left( {1 - {\\phi ^*}} \\right)}}{{\\sigma _*^2}}\\sum\\limits_{t = 2}^T {\\left( {{x_t} - {\\phi ^*}{x_{t - 1}}} \\right)} \\hfill \\\\ \\frac{{{{\\left( {1 - {\\phi ^*}} \\right)}^2}}}{{\\sigma _*^2}}{\\mu ^*}\\left( {T - 1} \\right) &amp;= \\frac{{\\left( {1 - {\\phi ^*}} \\right)}}{{\\sigma _*^2}}\\sum\\limits_{t = 2}^T {\\left( {{x_t} - {\\phi ^*}{x_{t - 1}}} \\right)} \\hfill \\\\ {\\mu ^*}\\left( {1 - {\\phi ^*}} \\right)\\left( {T - 1} \\right) &amp;= \\sum\\limits_{t = 2}^T {\\left( {{x_t} - {\\phi ^*}{x_{t - 1}}} \\right)} \\hfill \\\\ {\\mu ^*} &amp;= \\frac{1}{{\\left( {1 - {\\phi ^*}} \\right)\\left( {T - 1} \\right)}}\\sum\\limits_{t = 2}^T {\\left( {{x_t} - {\\phi ^*}{x_{t - 1}}} \\right)} \\hfill \\\\ {\\mu ^*} &amp;= \\frac{1}{{1 - {\\phi ^*}}}\\left[ {\\underbrace {\\frac{1}{{T - 1}}\\sum\\limits_{t = 2}^T {{x_t}} }_{ = {{\\bar x}_{\\left( 2 \\right)}}} - \\underbrace {\\frac{{{\\phi ^*}}}{{T - 1}}\\sum\\limits_{t = 2}^T {{x_{t - 1}}} }_{ = {{\\bar x}_{\\left( 1 \\right)}}}} \\right] \\hfill \\\\ {{\\hat \\mu }^*} &amp;= \\frac{1}{{1 - {\\phi ^*}}}\\left( {{{\\bar x}_{\\left( 2 \\right)}} - \\phi {{\\bar x}_{\\left( 1 \\right)}}} \\right) \\hfill \\\\ \\end{aligned} \\] When \\(T\\) is large, we have the following: \\[\\begin{aligned} {{\\bar x}_{\\left( 1 \\right)}} \\approx \\bar x &amp;,{{\\bar x}_{\\left( 2 \\right)}} \\approx \\bar x \\hfill \\\\ \\hfill \\\\ {{\\hat \\mu }^*} &amp;= \\frac{1}{{1 - {\\phi ^*}}}\\left( {\\bar x - {\\phi ^*}\\bar x} \\right) \\hfill \\\\ &amp;= \\frac{{\\bar x}}{{1 - {\\phi ^*}}}\\left( {1 - {\\phi ^*}} \\right) \\hfill \\\\ &amp;= \\bar x \\hfill \\\\ \\end{aligned} \\] Taking the derivative with respect to \\(\\sigma^2\\) and solving for \\(\\sigma^2\\) gives: \\[\\begin{aligned} \\frac{\\partial }{{\\partial {\\sigma ^2}}}{l^*}\\left( {\\vec \\theta |{x_1}, \\ldots ,{x_T}} \\right) &amp;= - \\frac{{\\left( {T - 1} \\right)}}{{2\\sigma _*^2}} + \\frac{1}{{2\\sigma _*^4}}{S_c}\\left( {\\mu ,\\phi } \\right) \\hfill \\\\ 0 &amp;= - \\frac{{\\left( {T - 1} \\right)}}{{2\\sigma _*^2}} + \\frac{1}{{2\\sigma _*^4}}{S_c}\\left( {\\mu ,\\phi } \\right) \\hfill \\\\ \\frac{{\\left( {T - 1} \\right)}}{{2\\sigma _*^2}} &amp;= \\frac{1}{{2\\sigma _*^4}}{S_c}\\left( {\\mu ,\\phi } \\right) \\hfill \\\\ \\hat \\sigma _*^2 &amp;= \\frac{1}{{T - 1}}{S_c}\\left( {{{\\hat \\mu }^*},{{\\hat \\phi }^*}} \\right) \\hfill \\\\ \\end{aligned} \\] Taking the derivative with respect to \\(\\phi\\) gives: \\[\\begin{aligned} \\frac{\\partial }{{\\partial \\phi }}{l^*}\\left( {\\vec \\theta |{x_1}, \\ldots ,{x_T}} \\right) &amp;= - \\frac{1}{{2{\\sigma ^2}}}\\sum\\limits_{t = 2}^T { - 2\\left[ {\\left( {{x_t} - \\mu } \\right) - \\phi \\left( {{x_{t - 1}} - \\mu } \\right)} \\right]\\left( {{x_{t - 1}} - \\mu } \\right)} \\hfill \\\\ &amp;= \\frac{1}{{{\\sigma ^2}}}\\sum\\limits_{t = 2}^T {\\left[ {{x_t} - \\phi {x_{t - 1}} - \\mu \\left( {1 - \\phi } \\right)} \\right]\\left( {{x_{t - 1}} - \\mu } \\right)} \\hfill \\\\ &amp;= \\frac{1}{{{\\sigma ^2}}}\\sum\\limits_{t = 2}^T {\\left[ {{x_t}{x_{t - 1}} - \\phi x_{t - 1}^2 - \\mu \\left( {1 - \\phi } \\right){x_{t - 1}} - \\mu {x_t} + \\mu \\phi {x_{t - 1}} + {\\mu ^2}\\left( {1 - \\phi } \\right)} \\right]} \\hfill \\\\ &amp;= \\frac{1}{{{\\sigma ^2}}}\\left[ \\begin{gathered} \\sum\\limits_{t = 2}^T {{x_t}{x_{t - 1}}} - \\phi \\sum\\limits_{t = 2}^T {x_{t - 1}^2} - \\mu \\left( {1 - \\phi } \\right)\\left( {T - 1} \\right){{\\bar x}_{\\left( 1 \\right)}} \\hfill \\\\ - \\mu \\left( {T - 1} \\right){{\\bar x}_{\\left( 2 \\right)}} + \\phi \\mu \\left( {T - 1} \\right){{\\bar x}_{\\left( 1 \\right)}} + {\\mu ^2}\\left( {1 - \\phi } \\right)\\left( {T - 1} \\right) \\hfill \\\\ \\end{gathered} \\right] \\end{aligned}\\] Solving for \\(\\phi\\) gives: \\[\\begin{aligned} 0 &amp;= \\frac{\\partial }{{\\partial \\phi }}{l^*}\\left( {\\vec \\theta |{x_1}, \\ldots ,{x_T}} \\right) \\hfill \\\\ 0 &amp;= \\sum\\limits_{t = 2}^T {{x_t}{x_{t - 1}}} - {{\\hat \\phi }^*}\\sum\\limits_{t = 2}^T {x_{t - 1}^2} - \\left( {{{\\bar x}_{\\left( 2 \\right)}} - {{\\hat \\phi }^*}{{\\bar x}_{\\left( 1 \\right)}}} \\right)\\left( {T - 1} \\right){{\\bar x}_{\\left( 1 \\right)}} - \\frac{{{{\\bar x}_{\\left( 2 \\right)}} - {{\\hat \\phi }^*}{{\\bar x}_{\\left( 1 \\right)}}}}{{1 - {{\\hat \\phi }^*}}}\\left( {T - 1} \\right){{\\bar x}_{\\left( 2 \\right)}} \\\\ &amp;+ {{\\hat \\phi }^*}\\frac{{{{\\bar x}_{\\left( 2 \\right)}} - {{\\hat \\phi }^*}{{\\bar x}_{\\left( 1 \\right)}}}}{{1 - {{\\hat \\phi }^*}}}\\left( {T - 1} \\right){{\\bar x}_{\\left( 1 \\right)}} + {\\left( {\\frac{{{{\\bar x}_{\\left( 2 \\right)}} - {{\\hat \\phi }^*}{{\\bar x}_{\\left( 1 \\right)}}}}{{1 - {{\\hat \\phi }^*}}}} \\right)^2}\\left( {1 - {{\\hat \\phi }^*}} \\right)\\left( {T - 1} \\right) \\hfill \\\\ &amp;\\vdots \\hfill \\\\ &amp;{\\text{Magic}} \\hfill \\\\ &amp;\\vdots \\hfill \\\\ {{\\hat \\phi }^*} &amp;= \\frac{{\\sum\\limits_{t = 2}^T {\\left( {{x_t} - {{\\bar x}_{\\left( 2 \\right)}}} \\right)\\left( {{x_{t-1}} - {{\\bar x}_{\\left( 1 \\right)}}} \\right)} }}{{\\sum\\limits_{t = 2}^T {{{\\left( {{x_{t - 1}} - {{\\bar x}_{\\left( 1 \\right)}}} \\right)}^2}} }} \\hfill \\\\ \\end{aligned} \\] When \\(T\\) is large, we have: \\[\\begin{aligned} \\sum\\limits_{t = 2}^T {\\left( {{x_t} - {{\\bar x}_{\\left( 2 \\right)}}} \\right)\\left( {{x_t} - {{\\bar x}_{\\left( 1 \\right)}}} \\right)} &amp;\\approx \\sum\\limits_{t = 2}^T {\\left( {{x_t} - \\bar x} \\right)\\left( {{x_{t - 1}} - \\bar x} \\right)} \\hfill \\\\ \\sum\\limits_{t = 2}^T {{{\\left( {{x_{t - 1}} - {{\\bar x}_{\\left( 1 \\right)}}} \\right)}^2}} &amp;\\approx \\sum\\limits_{t = 1}^T {{{\\left( {{x_t} - \\bar x} \\right)}^2}} \\hfill \\\\ \\hfill \\\\ {{\\hat \\phi }^*} &amp;= \\frac{{\\sum\\limits_{t = 2}^T {\\left( {{x_t} - {{\\bar x}_{\\left( 2 \\right)}}} \\right)\\left( {{x_t} - {{\\bar x}_{\\left( 1 \\right)}}} \\right)} }}{{\\sum\\limits_{t = 2}^T {{{\\left( {{x_{t - 1}} - {{\\bar x}_{\\left( 1 \\right)}}} \\right)}^2}} }} \\approx \\frac{{\\sum\\limits_{t = 2}^T {\\left( {{x_t} - \\bar x} \\right)\\left( {{x_{t - 1}} - \\bar x} \\right)} }}{{\\sum\\limits_{t = 1}^T {{{\\left( {{x_t} - \\bar x} \\right)}^2}} }} = \\hat \\rho \\left( 1 \\right) \\hfill \\\\ \\end{aligned} \\] 4.6 Method of Moments The goal behind the estimation with Method of Moments is to match the theoretical moment (e.g. \\(E\\left[ {x_t^k} \\right]\\)) with the sample moment (e.g \\(\\frac{1}{n}\\sum\\limits_{i = 1}^n {x_i^k}\\)), where \\(k\\) denotes the moment. This method often leads to suboptimal estimates for general ARMA models. However, it is quite optimal for \\(AR(p)\\). 4.6.1 Method of Moments - AR(p) Consider an \\(AR(p)\\) process represented by: \\[{x_t} = {\\phi _1}{x_{t - 1}} + \\cdots + {\\phi _p}{x_{t - p}} + {w_t}\\] where \\(w_t \\sim N(0,\\sigma^2)\\) To begin, we find the Covariance of the process when \\(h &gt; 0\\): \\[\\begin{aligned} Cov\\left( {{x_{t + h}},{x_t}} \\right) &amp;\\mathop = \\limits^{\\left( {h &gt; 0} \\right)} Cov\\left( {{\\phi _1}{x_{t + h - 1}} + \\cdots + {\\phi _p}{x_{t + h - p}} + {w_{t + h}},{x_t}} \\right) \\hfill \\\\ &amp;= {\\phi _1}Cov\\left( {{x_{t + h - 1}},{x_t}} \\right) + \\cdots + {\\phi _p}Cov\\left( {{x_{t + h - p}},{x_t}} \\right) + Cov\\left( {{w_{t + h}},{x_t}} \\right) \\hfill \\\\ &amp;= {\\phi _1}\\gamma \\left( {h - 1} \\right) + \\cdots + {\\phi _p}\\gamma \\left( {h - p} \\right) \\hfill \\\\ \\end{aligned} \\] Now, we turn our attention to the variance of the process: \\[\\begin{aligned} Var\\left( {{w_t}} \\right) &amp;= Cov\\left( {{w_t},{w_t}} \\right) \\hfill \\\\ &amp;= Cov\\left( {{w_t},{w_t}} \\right) + \\underbrace {Cov\\left( {{\\phi _1}{x_{t - 1}},{w_t}} \\right)}_{ = 0} + \\cdots + \\underbrace {Cov\\left( {{\\phi _p}{x_{t - p}},{w_t}} \\right)}_{ = 0} \\hfill \\\\ &amp;= Cov\\left( {\\underbrace {{\\phi _1}{x_{t - 1}} + \\cdots + {\\phi _p}{x_p} + {w_t}}_{ = {x_t}},{w_t}} \\right) \\hfill \\\\ &amp;= Cov\\left( {{x_t},{w_t}} \\right) \\hfill \\\\ &amp;= Cov\\left( {{x_t},{x_t} - {\\phi _1}{x_{t - 1}} - \\cdots - {\\phi _p}{x_p}} \\right) \\hfill \\\\ &amp;= Cov\\left( {{x_t},{x_t}} \\right) - {\\phi _1}Cov\\left( {{x_t},{x_{t - 1}}} \\right) - \\cdots - {\\phi _p}Cov\\left( {{x_t},{x_{t - p}}} \\right) \\hfill \\\\ &amp;= \\gamma \\left( 0 \\right) - {\\phi _1}\\gamma \\left( 1 \\right) - \\cdots - {\\phi _p}\\gamma \\left( p \\right) \\hfill \\\\ \\end{aligned} \\] Together, these equations are known as the Yule-Walker equations. 4.6.2 Yule-Walker Definition Equation form: \\[\\begin{aligned} \\gamma \\left( h \\right) &amp;= {\\phi _1}\\gamma \\left( {h - 1} \\right) - \\cdots - {\\phi _p}\\gamma \\left( {h - p} \\right) \\hfill \\\\ {\\sigma ^2} &amp;= \\gamma \\left( 0 \\right) - {\\phi _1}\\gamma \\left( 1 \\right) - \\cdots - {\\phi _p}\\gamma \\left( p \\right) \\hfill \\\\ h &amp; = 1, \\ldots, p \\end{aligned} \\] Matrix form: \\[\\begin{aligned} \\Gamma \\vec \\phi &amp;= \\vec \\gamma \\hfill \\\\ {\\sigma ^2} &amp;= \\gamma \\left( 0 \\right) - {{\\vec \\phi }^T}\\vec \\gamma \\hfill \\\\ \\hfill \\\\ \\vec \\phi &amp;= \\left[ {\\begin{array}{*{20}{c}} {{\\phi _1}} \\\\ \\vdots \\\\ {{\\phi _p}} \\end{array}} \\right]_{p \\times 1},\\vec \\gamma = \\left[ {\\begin{array}{*{20}{c}} {\\gamma \\left( 1 \\right)} \\\\ \\vdots \\\\ {\\gamma \\left( p \\right)} \\end{array}} \\right]_{p \\times 1},\\Gamma = \\left\\{ {\\gamma \\left( {k - j} \\right)} \\right\\}_{j,k = 1}^p \\\\ \\end{aligned} \\] More aptly, the structure of \\(\\Gamma\\) looks like the following: \\[\\Gamma = {\\left[ {\\begin{array}{*{20}{c}} {\\gamma \\left( 0 \\right)}&amp;{\\gamma \\left( { - 1} \\right)}&amp;{\\gamma \\left( { - 2} \\right)}&amp; \\cdots &amp;{\\gamma \\left( {1 - p} \\right)} \\\\ {\\gamma \\left( 1 \\right)}&amp;{\\gamma \\left( 0 \\right)}&amp;{\\gamma \\left( { - 1} \\right)}&amp; \\cdots &amp;{\\gamma \\left( {2 - p} \\right)} \\\\ {\\gamma \\left( 2 \\right)}&amp;{\\gamma \\left( 1 \\right)}&amp;{\\gamma \\left( 0 \\right)}&amp; \\cdots &amp;{\\gamma \\left( {3 - p} \\right)} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ {\\gamma \\left( {p - 1} \\right)}&amp;{\\gamma \\left( {p - 2} \\right)}&amp;{\\gamma \\left( {p - 3} \\right)}&amp; \\cdots &amp;{\\gamma \\left( 0 \\right)} \\end{array}} \\right]_{p \\times p}}\\] Note, that we are able to use the above equations to effectively estimate \\(\\vec \\phi\\) and \\(\\sigma ^2\\). \\[\\left[ \\begin{aligned} \\hat{\\vec{\\phi}} &amp;= {{\\hat \\Gamma }^{ - 1}}\\hat{\\vec{\\gamma}} \\hfill \\\\ {{\\hat \\sigma }^2} &amp;= \\hat \\gamma \\left( 0 \\right) - {{\\hat{\\vec{\\gamma}}}^T}{{\\hat \\Gamma }^{ - 1}}\\hat{\\vec{\\gamma}} \\hfill \\\\ \\end{aligned} \\right. \\to {\\text{Yule - Walker Estimates}}\\] For the second equation, we are effectively substituting in the first equation for \\(\\hat{\\vec{\\phi}}\\), hence the quadratic form \\({{\\hat{\\vec{\\gamma}}}^T}{{\\hat \\Gamma }^{ - 1}}\\hat{\\vec{\\gamma}}\\). With this being said, there are a few nice asymptotic properties that we obtain for an \\(AR(p)\\). \\(\\sqrt T \\left( {\\hat{\\vec{\\phi}} - \\vec \\phi } \\right)\\mathop \\to \\limits_{t \\to \\infty }^L N\\left( {\\vec 0,{\\sigma ^2}{\\Gamma ^{ - 1}}} \\right)\\) \\({\\hat \\sigma ^2}\\mathop \\to \\limits^p {\\sigma ^2}\\) Yule-Walker estimates are optimal in the sense that they have the smallest asymptotic variance i.e. \\[Var\\left( {\\sqrt{T} \\hat{\\vec{\\phi}} } \\right) = {\\sigma ^2}{\\Gamma ^{ - 1}}\\] However, they are not necessarily optimal with small sample sizes. Conceptually, the reason for this optimality result is a consequence from the linear dependence between moments and variables. This is not true for MA or ARMA, which are both nonlinear and suboptimal. 4.6.3 Estimates Consider \\(x_t\\) as an \\(MA(1)\\) process: \\({x_t} = \\theta {w_{t - 1}} + {w_t},{w_t}\\mathop \\sim \\limits^{i.i.d} N\\left( {0,{\\sigma ^2}} \\right)\\) Finding the covariance when \\(h = 1\\) gives: \\[\\begin{aligned} Cov\\left( {{x_t},{x_{t - 1}}} \\right) &amp;= Cov\\left( {\\theta {w_{t - 1}} + {w_t},\\theta {w_{t - 2}} + {w_{t - 1}}} \\right) \\hfill \\\\ &amp;= Cov\\left( {\\theta {w_{t - 1}},{w_{t - 1}}} \\right) \\hfill \\\\ &amp;= \\theta {\\sigma ^2} \\hfill \\\\ \\end{aligned} \\] Finding the variance (e.g. \\(h=0\\)) gives: \\[\\begin{aligned} Cov\\left( {{x_t},{x_t}} \\right) &amp;= Cov\\left( {\\theta {w_{t - 1}} + {w_t},\\theta {w_{t - 1}} + {w_t}} \\right) \\hfill \\\\ &amp;= {\\theta ^2}Cov\\left( {{w_{t - 1}},{w_{t - 1}}} \\right) + \\underbrace {2\\theta Cov\\left( {{w_{t - 1}},{w_t}} \\right)}_{ = 0} + Cov\\left( {{w_t},{w_t}} \\right) \\hfill \\\\ &amp;= {\\theta ^2}{\\sigma ^2} + {\\sigma ^2} \\hfill \\\\ &amp;= {\\sigma ^2}\\left( {1 + {\\theta ^2}} \\right) \\hfill \\\\ \\end{aligned} \\] This gives us the MA(1) ACF of: \\[\\rho \\left( h \\right) = \\left\\{ {\\begin{array}{*{20}{c}} 1&amp;{h = 0} \\\\ {\\frac{\\theta }{{{\\theta ^2} + 1}}}&amp;{h = \\pm 1} \\end{array}} \\right.\\] With this in mind, let’s solve for possible \\(\\theta\\) values: \\[\\begin{aligned} \\rho \\left( 1 \\right) &amp;= \\frac{\\theta }{{{\\theta ^2} + 1}} \\hfill \\\\ \\Rightarrow \\theta &amp;= \\left( {{\\theta ^2} + 1} \\right)\\rho \\left( 1 \\right) \\hfill \\\\ \\theta &amp;= \\rho \\left( 1 \\right){\\theta ^2} + \\rho \\left( 1 \\right) \\hfill \\\\ 0 &amp;= \\rho \\left( 1 \\right){\\theta ^2} - \\theta + \\rho \\left( 1 \\right) \\hfill \\\\ \\end{aligned} \\] Yuck, that looks nasty. Let’s dig out an ol’ friend from middle school known as the quadratic formula: \\[\\theta = \\frac{{ - b \\pm \\sqrt {{b^2} - 4ac} }}{{2a}}\\] Applying the quadratic formula leads to: \\[\\begin{aligned} a &amp;= \\rho \\left( h \\right), b = -1, c = \\rho \\left( h \\right) \\\\ \\theta &amp;= \\frac{{1 \\pm \\sqrt {{1^2} - 4\\rho \\left( h \\right)\\rho \\left( h \\right)} }}{{2\\rho \\left( h \\right)}} \\hfill \\\\ \\theta &amp;= \\frac{{1 \\pm \\sqrt {1 - 4{{\\left[ {\\rho \\left( h \\right)} \\right]}^2}} }}{{2\\rho \\left( h \\right)}} \\hfill \\\\ \\end{aligned} \\] Thus, we have two possibilities: \\[\\begin{aligned} {\\theta _1} &amp;= \\frac{{1 + \\sqrt {1 - 4{{\\left[ {\\rho \\left( h \\right)} \\right]}^2}} }}{{2\\rho \\left( h \\right)}} \\hfill \\\\ {\\theta _2} &amp;= \\frac{{1 - \\sqrt {1 - 4{{\\left[ {\\rho \\left( h \\right)} \\right]}^2}} }}{{2\\rho \\left( h \\right)}} \\hfill \\\\ \\end{aligned}\\] To ensure invertibility, we mandate that \\(\\left| {\\rho \\left( 1 \\right)} \\right| &lt; \\frac{1}{2}\\). Thus, we opt for \\({\\theta _2}\\). So, our estimator is: \\[\\hat \\theta = \\frac{{1 - \\sqrt {1 - 4{{\\left[ {\\hat \\rho \\left( 1 \\right)} \\right]}^2}} }}{{2\\hat \\rho \\left( 1 \\right)}}\\] Furthermore, it can be shown that: \\[\\sqrt T \\left( {\\hat \\theta - \\theta } \\right)\\mathop \\to \\limits_{T \\to \\infty }^L N\\left( {0 ,\\frac{{1 + {\\theta ^2} + 4{\\theta ^4} + {\\theta ^6} + {\\theta ^8}}}{{{{\\left( {1 - {\\theta ^2}} \\right)}^2}}}} \\right)\\] So, this is not a really optimal estimator… 4.7 Prediction (Forecast) "]
]
