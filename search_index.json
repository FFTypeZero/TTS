[
["index.html", "Introduction to Time Series Chapter 1 Preface 1.1 A foreword 1.2 Rendering Mathematical Formulae 1.3 Mathematical Notation 1.4 R Code Conventions 1.5 License", " Introduction to Time Series James Balamuta and Stephane Guerrier 2016-08-16 Chapter 1 Preface Welcome to Introduction to Time Series with R! 1.1 A foreword This book was designed for use in STAT 429, Time Series Analysis, at the University of Illinois at Urbana-Champaign. When possible, it would be best to always access the text online to be sure you are using the latest version. The online version affords additional features over the traditional PDF copy such as a scaling text, variety of font faces, and themed backgrounds. However, if you are in need of a local copy, a pdf version is also available. Disclaimer: This book is under active development. As a result, errors may occur that range in severity from typos to broken code. If any of these issues arise, there are two options: If you are familiar with GitHub and know RMarkdown, make a pull request and fix the issue yourself! (fastest resolution) In the Online version, click the edit button in the top-left corner. Send an email to balamut2 AT illinois DOT edu and we will address issue. 1.2 Rendering Mathematical Formulae Throughout the book, there will be mathematical symbols used to express the material. Depending on the version of the book, there are two different render engines. For the online version, the text uses MathJax to render mathematical notation for the web. In the event the formulae does not load for a specific chapter, first try to refresh the page. 9 times out of 10 the issue is related to the software library not loading quickly. For the pdf version, the text is built using the recommended AMS LaTeX symbolic packages. As a result, there should be no issue displaying equations. An example of a mathematical rendering capabilities would be given as: \\[ a^2 + b^2 = c^2 \\] 1.3 Mathematical Notation The following notation will be adopted throughout the book. \\(X\\) denotes a (continuous) RV. \\(X_t\\) is \\(X\\) at time \\(t \\in N\\). \\(E\\left(X_t\\right)\\) is the Mean of \\(X\\) at time \\(t\\). \\(Var\\left(X_t\\right)\\) is the Variance of \\(X\\) at time \\(t\\). \\(X_{1}, X_{2}, \\ldots, X_{k}\\) are sequence of random variables. \\(f\\left(x\\right)\\) denotes the density function of \\(X\\) and \\(f\\left(x, y\\right)\\) denotes the joint density function of \\(x\\) and \\(Y\\). \\(\\left(X_t\\right)_{t=1,\\ldots,T} := \\left(X_t\\right) := (X_1, \\ldots, X_T)\\). 1.4 R Code Conventions The code used throughout the book will predominately be R code. To obtain a copy of R, go to the Comprehensive R Archive Network (CRAN) and download the appropriate installer for your operating system. When R code is displayed it will be typeset using a monospace font with syntax highlighting enabled to ensure the differentiation of functions, variables, and so on. For example, the following adds 1 to 1 a = 1L + 1L a Each code segment may contain actual output from R. Such output will appear in grey font prefixed by ##. For example, the output of the above code segment would look like so: ## [1] 2 Alongside the PDF download of the book, you should find the R code used within each chapter. 1.5 License This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],
["the-world-of-time-series.html", "Chapter 2 The World of Time Series 2.1 Objective of Time Series 2.2 What is a Time Series and can I eat it? 2.3 Exploratory Data Analysis (EDA) for Time Series", " Chapter 2 The World of Time Series 2.1 Objective of Time Series View a set of observations made sequentially in “time.” “One damned thing after another.” ~ R. A. Fisher Find a suitable model to describe an observed process “All models are wrong, but some are useful” ~ George Box Forecast future observations “Prediction is very difficult, especially if it’s about the future.” ~ Niels Bohr In essence, we seek to be able to predict, classify, and associate observed data with a theoretical backend. To do so, one must first create a model that provides an explanation of the data as a mixture of a pattern and noise (error). That is, we must be able to formulate the data in terms of: Model = Pattern + Noise In Time Series, the pattern represents the association between values observed over time (e.g. autocorrelation). Since these patterns are correlated in time, methods that assume independence are unable to be used. 2.2 What is a Time Series and can I eat it? The definition of Time Series (TS) is as follows: A Time Series is a stochastic process, a sequence of random variables (RV) defined on a common probability space denoted as \\(\\left(X_t\\right)_{t=1, \\ldots, T}\\) (i.e. \\(X_1, X_2, \\ldots, X_T\\)). Note: The time \\(t\\) belongs to discrete index sets (\\(\\in \\mathbb{Z}\\)) not continuous (\\(\\notin \\mathbb{R}\\)). After all, TS data is always collected at discrete time points. Furthermore, by time belonging to \\(\\mathbb{Z}\\) it can take upon itself negative and positive integer values (e.g. \\(-2, -1, 0, 1, 2\\)). In laypersons terms, a time series is a variable that gets measured sequentially at fixed intervals of time, which are oftenly spaced apart at equal distances (e.g. equispaced). Examples of Time Series: Stock Data from Johnson and Johson’s Quarterly earnings… Speech data from someone talking Earthquake and explosion data 2.3 Exploratory Data Analysis (EDA) for Time Series A large part of time series involves looking at graphs of time series. The graphs provide us information as to what kind of trends and outliers the data maybe hiding. 2.3.1 Identifying Trends A trend exists when there is a long term increase or decrease or combination of increases or decreases (polynomial) in the data. It could be linear or non-linear. Note: Long-term trend might change direction, indicating non-linear trends! Examples of non-linear trends: Seasonal trends (periodic): These are the cyclical patterns which repeat after a fixed/regular time period. Business cycles (bust/recession, recovery, boom) Seasons (summer, fall, winter, spring) Non-seasonal trends (periodic): These patterns cannot be associated to seasonal variation. Impact of economic indicators on stock returns. “Other” trends: These trends have no regular patterns. They could be just local, short-term. They change statistical properties of a TS over a segment of time (“window”). Earthquakes! El Nino 2.3.2 Noticing changes in time and outliers Change in time and outliers yields interesting results. These results can be seen as: Change in Means Change in means of a TS can be related to long-term, cyclical, and short-term trends. Change in Variance Change in variance can be related to change in the amplitude of the fluctuations of a TS. Change in State An event which causes change in statistical properties of TS for short term and long term! Some events cause abrupt changes in statistical properties of TS. They are often associated with “explosive” nature of TS. Outliers These are the “extreme” observations in the time series. May be related to data collection or change in state. "],
["autocorrelation-and-stationarity.html", "Chapter 3 Autocorrelation and Stationarity 3.1 Dependency 3.2 The Autocorrelation and Autocovariance Functions 3.3 Stationarity 3.4 Joint Stationarity", " Chapter 3 Autocorrelation and Stationarity \"I have seen the future and it is very much like the present, only longer.\" > > --- Kehlog Albran, The Profit --> After reading this chapter you will be able to: Describe independent and dependent data Interpret a processes ACF and CCF. Understand the notion of stationarity. Differentiate between Strong and Weak stationarity. Judge whether a process is stationary. 3.1 Dependency Generally speaking, there is a dependence that within the sequence of random variables. Recall the difference between independent and dependent data: Definition: Independence \\(X_1, X_2, \\ldots, X_T\\) are independent and identically distributed if and only if \\begin{equation} P\\left(X_1 \\le x_1, X_2 \\le x_2,\\ldots, X_{T} \\le x_T \\right) = P\\left(X_1 \\le x_1\\right) P\\left(X_2 \\le x_2\\right) \\cdots P\\left(X_{T} \\le x_T \\right) \\label{eq:independent} \\end{equation} for any \\(T \\ge 2\\) and \\(x_1, \\ldots, x_T \\in \\mathbb{R}\\). Definition: Dependence \\(X_1, X_2, \\ldots, X_T\\) are identically distributed but dependent, then \\begin{equation} \\left| {P\\left( {{X_1} &lt; {x_1},{X_2} &lt; {x_2}, \\ldots ,{X_T} &lt; {x_T}} \\right) - P\\left( {{X_1} &lt; {x_1}} \\right)P\\left( {{X_2} &lt; {x_2}} \\right) \\cdots P\\left( {{X_T} &lt; {x_T}} \\right)} \\right| \\ne 0 \\label{eq:dependent} \\end{equation} for some \\(x_1, \\ldots, x_T \\in \\mathbb{R}\\). 3.1.1 Measuring (Linear) Dependence There are many forms of dependency… dependency However, the methods, covariance and correlation, that we will be using are specific to measuring linear dependence. As a result, these tools are less helpful to measure monotonic dependence and they are much less helpful to measure nonlinearly dependence. 3.2 The Autocorrelation and Autocovariance Functions Dependence between \\(T\\) different RV is difficult to measure in one shot! So we consider just two random variables, \\(X_t\\) and \\(X_{t+h}\\). Then one (linear) measure of dependence is the covariance between \\(\\left(X_t , X_{t+h}\\right)\\). Since \\(X\\) is the same RV observed at two different time points, the covariance between \\(X_t\\) and \\(X_{t+h}\\) is defined as the Autocovariance. 3.2.1 Definitions The Autocovariance Function of a series \\(X_t\\) is defined as \\[{\\gamma _x}\\left( {t,t+h} \\right) = \\operatorname{cov} \\left( {{x_t},{x_{t+h}}} \\right).\\] Since we generally consider stochastic processes with constant zero mean we often have \\[{\\gamma _x}\\left( {t,t+h} \\right) = E\\left[X_t X_{t+h} \\right]. \\] We normally drop the subscript referring to the time series if it is clear to the time series the autocovariance function is referencing. For example, we generally use \\({\\gamma}\\left( {t,t+h} \\right)\\) instead of \\({\\gamma _x}\\left( {t,t+h} \\right)\\). Moreover, the notation is even further simplify when the covariance of \\(X_t\\) and \\(X_{t+h}\\) is the same as that of \\(X_{t+j}\\) and \\(X_{t+h+j}\\) (for \\(j \\in \\mathbb{Z}\\)), i.e. that the covariance depends only on the time between observations and not the absolute date \\(t\\). This is an important property call stationarity, which will be discuss in the next section. In this case, we simply use to following notation: \\[\\gamma \\left( {h} \\right) = \\operatorname{cov} \\left( X_t , X_{t+h} \\right). \\] A few other remarks: The covariance function is symmetric. That is, \\({\\gamma}\\left( {h} \\right) = {\\gamma}\\left( -h \\right)\\) since \\(\\operatorname{cov} \\left( {{X_t},{X_{t+h}}} \\right) = \\operatorname{cov} \\left( X_{t+h},X_{t} \\right)\\). Note that \\(\\operatorname{var} \\left( X_{t} \\right) = {\\gamma}\\left( 0 \\right)\\). We have that \\(|\\gamma(h)| \\leq \\gamma(0)\\) for all \\(h\\). The proof of this inequality follows from Cauchy-Schwarz inequality, i.e. \\[ \\begin{aligned} \\left(|\\gamma(h)| \\right)^2 &amp;= \\gamma(h)^2 = \\left(E\\left[\\left(X_t - E[X_t] \\right)\\left(X_{t+h} - E[X_{t+h}] \\right)\\right]\\right)^2\\\\ &amp;\\leq E\\left[\\left(X_t - E[X_t] \\right)^2 \\right] E\\left[\\left(X_{t+h} - E[X_{t+h}] \\right)^2 \\right] = \\gamma(0)^2. \\end{aligned} \\] Just as any covariance, the \\({\\gamma}\\left( {h} \\right)\\) is “scale dependent”, \\({\\gamma}\\left( {h} \\right) \\in \\mathbb{R}\\), or \\(-\\infty \\le {\\gamma}\\left( {h} \\right) \\le +\\infty\\) If \\(\\left| {\\gamma}\\left( {h} \\right) \\right|\\) is “close” to 0, then they are “less dependent”. If \\(\\left| {\\gamma}\\left( {h} \\right) \\right|\\) is “far” from 0, \\(X_t\\) and \\(X_{t+h}\\) are “more dependent”. \\({\\gamma}\\left( {h} \\right)=0\\) does not imply \\(X_t\\) and \\(X_{t+h}\\) are independent. This is only true in joint Gaussian case. An important related statistic is the correlation of \\(X_t\\) with \\(X_{t+h}\\) or autocorrelation which is defined (for stationary processes) as \\[\\rho \\left( h \\right) = \\operatorname{corr}\\left( {{X_t},{X_{t + h}}} \\right) = \\frac{\\gamma(h) }{\\gamma(0)}.\\] It is important to note that the above notation implies that the autocorrelation function is only a function of the lag \\(h\\) between observations. Thus, autocovariances and autocorrelations are one possible way to describe the joint distribution of a time series. Indeed, the correlation of \\(X_t\\) with \\(X_{t+1}\\) is an obvious measure of how persistent a time series is. Remeber that just as with any correlation: \\(\\rho \\left( h \\right)\\) is scale free. \\(\\rho \\left( {{X_t},{X_{t + h}}} \\right)\\) is closer to \\(\\pm 1 \\Rightarrow \\left({ X_t, X_{t+h} } \\right)\\) “more dependent.” \\(|\\rho \\left( h \\right)| \\leq 1\\) since \\(|\\gamma(h)| \\leq \\gamma(0)\\). Causation and correlation are two very different things! 3.2.2 A Fundamental Representation Autocovariances and autocorrelation also turn out to be a very useful tool because they are one of fundamental representations of time series. Indeed, if we consider a zero mean normally distrbuted process it is clear that its joint distribution is fully characterized by the autocariances \\(E[X_t X_{t+h}]\\) (since the joint probability density only depends of these covariances). Once we know the autocovariances we know everything there is to know about the process and therefore: If two processes have the same autocovariance function, then they are the same process. 3.2.3 Admissible autocorrelation functions Since the autocorrelation is related to a fundamental representation of time series it implies that one might be able to define a stochastic process by picking a set autocorrelation values. However, it turns out not every collection of numbers such as \\(\\{\\rho_1, \\rho_2, ...\\}\\) is the autocorrelation of a process. Two conditions are required to ensure the validity of an autocorrelation sequence: \\(\\operatorname{max}_j \\; | \\rho_j| \\leq 1\\). \\(\\operatorname{var} \\left[\\sum_{j = 0}^\\infty \\alpha_j X_{t-j} \\right] \\geq 0\\) for all \\(\\{\\alpha_0, \\alpha_1, ...\\}\\). The first condition is obvious and simply relects the fact that \\(|\\rho \\left( h \\right)| \\leq 1\\) but the second is more difficult to verify. Let \\(\\alpha_j = 0, \\; j &gt; 1\\), then conditon 2 implies that \\[\\operatorname{var} \\left[ \\alpha_0 X_{t} + \\alpha_1 X_{t-1} \\right] = \\gamma_0 \\begin{bmatrix} \\alpha_0 &amp; \\alpha_1 \\end{bmatrix} \\begin{bmatrix} 1 &amp; \\rho_1\\\\ \\rho_1 &amp; 1 \\end{bmatrix} \\begin{bmatrix} \\alpha_0 \\\\ \\alpha_1 \\end{bmatrix} \\geq 0. \\] Thus, the matrix \\[ \\boldsymbol{A}_1 = \\begin{bmatrix} 1 &amp; \\rho_1\\\\ \\rho_1 &amp; 1 \\end{bmatrix} \\] must be positive semi-definite. Therefore, \\[\\operatorname{det} \\left(\\boldsymbol{A}_1\\right) = 1 - \\rho_1^2 \\] implying that \\(|\\rho_1| &lt; 1\\). Next, let \\(\\alpha_j = 0, \\; j &gt; 2\\), then we must verify that: \\[\\operatorname{var} \\left[ \\alpha_0 X_{t} + \\alpha_1 X_{t-1} + \\alpha_2 X_{t-2} \\right] = \\gamma_0 \\begin{bmatrix} \\alpha_0 &amp; \\alpha_1 &amp;\\alpha_2 \\end{bmatrix} \\begin{bmatrix} 1 &amp; \\rho_1 &amp; \\rho_2\\\\ \\rho_1 &amp; 1 &amp; \\rho_1 \\\\ \\rho_2 &amp; \\rho_1 &amp; 1 \\end{bmatrix} \\begin{bmatrix} \\alpha_0 \\\\ \\alpha_1 \\\\ \\alpha_2 \\end{bmatrix} \\geq 0. \\] Similarly, this implies that the matrix \\[ \\boldsymbol{A}_2 = \\begin{bmatrix} 1 &amp; \\rho_1 &amp; \\rho_2\\\\ \\rho_1 &amp; 1 &amp; \\rho_1 \\\\ \\rho_2 &amp; \\rho_1 &amp; 1 \\end{bmatrix} \\] must be positive semi-definite. It is easy to verify that \\[\\operatorname{det} \\left(\\boldsymbol{A}_2\\right) = \\left(1 - \\rho_2 \\right)\\left(- 2 \\rho_1^2 + \\rho_2 + 1\\right). \\] It implies that \\(|\\rho_2| &lt; 1\\) as well as \\[\\begin{aligned} &amp;- 2 \\rho_1^2 + \\rho_2 + 1 \\geq 0 \\Rightarrow 1 &gt; \\rho_2 \\geq 2 \\rho_1^2 - 1 \\\\ &amp;\\Rightarrow 1 - \\rho_1^2 &gt; \\rho_2 - \\rho_1^2 \\geq -(1 - \\rho_1^2)\\\\ &amp;\\Rightarrow 1 &gt; \\frac{\\rho_2 - \\rho_1^2 }{1 - \\rho_1^2} \\geq 1, \\end{aligned}\\] imlying that \\(\\rho_1\\) and \\(\\rho_2\\) must lie in a parabolic shaped region defined by the above inequalities. Therefore, the restrictions on the autocorrelation are very complicated providing a motivation for other form of fundamental representation. 3.3 Stationarity 3.3.1 Definitions There are two kinds of stationarity which are commonly used. They are defined below: A process \\(\\{X_t\\}\\) is strongly stationary or strictly stationary if the joint probability distribution of \\(\\{X_{t-h}, ..., X_t, ..., X_{t+h}\\}\\) is independent of \\(t\\) for all \\(h\\). A process \\(\\{X_t\\}\\) is weakly stationary, covariance stationary or second order stationary if \\(E[X_t]\\), \\(E[X_t^2]\\) are finite and \\(E[X_t X_{t-h}]\\) depends only on \\(h\\) and not on \\(t\\). These types of stationarity are not equivalent and the presence of one kind of stationarity does not imply the other. That is, a time series can be strongly stationary but not weakly stationary and vice versa. In some cases, a time series can be both strong and weakly stationary, this is happends for example in the (joint) Gaussian case. Stationarity of \\(X_t\\) matters, because it provides the framework in which averaging dependent data makes sense. A few remarks: Strong stationarity \\(\\notimplies\\) weak stationarity. Example: an iid Cauchy process is strongly but not weakly stationary. Weak stationarity \\(\\notimplies\\) strong stationarity. Example: \\(X_{2t} = U_{2t}, X_{2t+1} = V_{2t+1} \\forall t\\) where \\({U_t}\\mathop \\sim \\limits^{iid} N\\left( {1,1} \\right)\\) and \\({V_t}\\mathop \\sim \\limits^{iid} Exponential\\left( 1 \\right)\\) is weakly stationary but NOT strongly stationary. Strong stationarity + \\(E[X_t]\\), \\(E[X_t^2] &lt; \\infty\\) \\(\\implies\\) weak stationarity Weak stationarity + normality \\(\\implies\\) strong stationarity. 3.3.2 Assessing Weak Stationarity of Time Series Models In order to verify if a process is weakly stationary, we must make sure the process satisfies: \\(E\\left[X_t \\right] = \\mu_t = \\mu &lt; \\infty\\), \\(\\operatorname{var}\\left[X_t \\right] = \\sigma^2_t = \\sigma^2 &lt; \\infty\\), \\(\\operatorname{cov}\\left(X_t, X_{t+h} \\right) = \\gamma \\left(h\\right)\\). 3.3.2.1 Example: Gaussian White Noise It is easy to verify that a Gaussian white noise is stationary. Indeed, we have: \\(E\\left[ {{X_t}} \\right] = 0\\), \\(\\gamma(0) = \\sigma^2 &lt; \\infty\\), \\(\\gamma(h) = 0\\) for \\(|h| &gt; 0\\). 3.3.2.2 Example: Random Walk To evaluate the stationarity of a random walk we first derive its properties: \\[\\begin{aligned} E\\left[ {{X_t}} \\right] &amp;= E\\left[ {{X_{t - 1}} + {W_t}} \\right] = E\\left[ {\\sum\\limits_{i = 1}^t {{W_t}} + {X_0}} \\right] \\\\ &amp;= E\\left[ {\\sum\\limits_{i = 1}^t {{W_t}} } \\right] + {X_0} = X_0 \\\\ \\end{aligned} \\] Note, the mean here is constant since it depends only on the value of the first term in the sequence. \\[\\begin{aligned} \\operatorname{var}\\left( {{X_t}} \\right) &amp;= \\operatorname{var}\\left( {\\sum\\limits_{i = 1}^t {{W_t}} + {X_0}} \\right) = \\operatorname{var}\\left( {\\sum\\limits_{i = 1}^t {{w_t}} } \\right) + \\underbrace {\\operatorname{var}\\left( {{X_0}} \\right)}_{= 0} \\\\ &amp;= \\sum\\limits_{i = 1}^t {Var\\left( {{w_t}} \\right)} = t \\sigma_w^2. \\end{aligned}\\] where \\(\\sigma_w^2 = \\operatorname{var}(W_t)\\). Therefore, the variance has a dependence on time and we have: \\[\\mathop {\\lim }\\limits_{t \\to \\infty } \\; \\operatorname{var}\\left(X_t\\right) = \\infty.\\] As a result, the process is not weakly stationary. Continuing on just to obtain the covariance, we have: \\[\\begin{aligned} \\gamma \\left( h \\right) &amp;= Cov\\left( {{y_t},{y_{t + h}}} \\right) = Cov\\left( {\\sum\\limits_{i = 1}^t {{w_i}} ,\\sum\\limits_{j = 1}^{t + h} {{w_j}} } \\right) \\\\ &amp;= Cov\\left( {\\sum\\limits_{i = 1}^t {{w_i}} ,\\sum\\limits_{j = 1}^t {{w_j}} } \\right) = \\min \\left( {t,t + h} \\right)\\sigma _w^2 \\\\ &amp;= \\left( {t + \\min \\left( {0,h} \\right)} \\right)\\sigma _w^2, \\end{aligned} \\] which also illustrates that non-stationarity of a random walk. In the following simulated example, we illustrate the non-stationary feature of such process: # In this example, we simulate a large number of random walks # Number of simulated processes B = 200 # Length of random walks n = 1000 # Output matrix out = matrix(NA,B,n) for (i in 1:B){ # Simulate random walk Xt = cumsum(rnorm(n)) # Store process out[i,] = Xt } # Plot random walks plot(NA, xlim = c(1,n), ylim = range(out), xlab = &quot;Time&quot;, ylab = &quot; &quot;) color = sample(topo.colors(B, alpha = 0.5)) for (i in 1:B){ lines(out[i,], col = color[i]) } # Add 95% confidence region lines(1:n, 1.96*sqrt(1:n), col = 2, lwd = 2, lty = 2) lines(1:n, -1.96*sqrt(1:n), col = 2, lwd = 2, lty = 2) The relationship between time and variance can clearly be observed in the above graph. 3.3.2.3 Example: MA(1) To evaluate the stationarity of an MA(1) process we first derive its properties: \\[\\begin{aligned} E\\left[ {{y_t}} \\right] &amp;= E\\left[ {{\\theta _1}{w_{t - 1}} + {w_t}} \\right] \\\\ &amp;= {\\theta _1}E\\left[ {{w_{t - 1}}} \\right] + E\\left[ {{w_t}} \\right] = 0 \\\\ \\end{aligned}\\] \\[\\begin{aligned} Cov\\left( {{y_t},{y_{t + h}}} \\right) &amp;= E\\left[ {\\left( {{y_t} - E\\left[ {{y_t}} \\right]} \\right)\\left( {{y_{t + h}} - E\\left[ {{y_{t + h}}} \\right]} \\right)} \\right] \\\\ &amp;= E\\left[ {{y_t}{y_{t + h}}} \\right] - \\underbrace {E\\left[ {{y_t}} \\right]}_{ = 0}\\underbrace {E\\left[ {{y_{t + h}}} \\right]}_{ = 0} \\\\ &amp;= E\\left[ {\\left( {{\\theta _1}{w_{t - 1}} + {w_t}} \\right)\\left( {{\\theta _1}{w_{t + h - 1}} + {w_{t + h}}} \\right)} \\right] \\\\ &amp;= E\\left[ {\\theta _1^2{w_{t - 1}}{w_{t + h - 1}} + \\theta {w_t}{w_{t + h}} + {\\theta _1}{w_{t - 1}}{w_{t + h}} + {w_t}{w_{t + h}}} \\right] \\\\ &amp;\\\\ E\\left[ {{w_t}{w_{t + h}}} \\right] &amp;= \\operatorname{cov} \\left( {{w_t},{w_{t + h}}} \\right) + E\\left[ {{w_t}} \\right]E\\left[ {{w_{t + h}}} \\right] = {1_{\\left\\{ {h = 0} \\right\\}}}\\sigma _w^2 \\\\ \\\\ &amp;\\Rightarrow Cov\\left( {{y_t},{y_{t + h}}} \\right) = \\left( {\\theta _1^2{1_{\\left\\{ {h = 0} \\right\\}}} + {\\theta _1}{1_{\\left\\{ {h = 1} \\right\\}}} + {\\theta _1}{1_{\\left\\{ {h = - 1} \\right\\}}} + {1_{\\left\\{ {h = 0} \\right\\}}}} \\right)\\sigma _w^2 \\\\ \\gamma \\left( h \\right) &amp;= \\left\\{ {\\begin{array}{*{20}{c}} {\\left( {\\theta _1^2 + 1} \\right)\\sigma _w^2}&amp;{h = 0} \\\\ {{\\theta _1}\\sigma _w^2}&amp;{\\left| h \\right| = 1} \\\\ 0&amp;{\\left| h \\right| &gt; 1} \\end{array}} \\right. \\end{aligned} \\] Therefore, an MA(1) process is weakly stationary since both the mean and variance are constant over time. In addition, we can easily obtain the autocorrelation function which is given by \\[\\Rightarrow \\rho \\left( h \\right) = \\left\\{ {\\begin{array}{*{20}{c}} 1&amp;{h = 0} \\\\ {\\frac{{{\\theta _1}\\sigma _w^2}}{{\\left( {\\theta _1^2 + 1} \\right)\\sigma _w^2}} = \\frac{{{\\theta _1}}}{{\\theta _1^2 + 1}}}&amp;{\\left| h \\right| = 1} \\\\ 0&amp;{\\left| h \\right| &gt; 1} \\end{array}} \\right.\\] Interestingly, we can note that \\(|\\rho(1)| \\leq 0.5\\). 3.3.2.4 Example: MA(1) Consider the AR(1) process given as: \\[{y_t} = {\\phi _1}{y_{t - 1}} + {w_t} \\text{, where } {w_t}\\mathop \\sim \\limits^{iid} WN\\left( {0,\\sigma _w^2} \\right)\\] This process was shown to simplify to: \\[y_t = {\\phi ^t}{y_0} + \\sum\\limits_{i = 0}^{t - 1} {\\phi _1^i{w_{t - i}}}\\] In addition, we add the requirement that \\(\\left| \\phi _1 \\right| &lt; 1\\). This requirement allows for the process to be stationary. If \\(\\phi _1 \\ge 1\\), the process would not converge. This way the process will be able to be written as a geometric series that converges: \\[\\sum\\limits_{k = 0}^\\infty {{r^k}} = \\frac{1}{{1 - r}},{\\text{ }}\\left| r \\right| &lt; 1\\] Next, we demonstrate how crucial this property is: \\[\\begin{aligned} \\mathop {\\lim }\\limits_{t \\to \\infty } E\\left[ {{y_t}} \\right] &amp;= \\mathop {\\lim }\\limits_{t \\to \\infty } E\\left[ {{\\phi ^t}{y_0} + \\sum\\limits_{i = 0}^{t - 1} {\\phi _1^i{w_{t - i}}} } \\right] \\\\ &amp;= \\mathop {\\lim }\\limits_{t \\to \\infty } \\underbrace {{\\phi ^t}{y_0}}_{\\left| \\phi \\right| &lt; 1 \\Rightarrow t \\to \\infty {\\text{ = 0}}} + \\sum\\limits_{i = 0}^{t - 1} {\\phi _1^i\\underbrace {E\\left[ {{w_{t - i}}} \\right]}_{ = 0}} \\\\ &amp;= 0 \\\\ \\mathop {\\lim }\\limits_{t \\to \\infty } Var\\left( {{y_t}} \\right) &amp;= \\mathop {\\lim }\\limits_{t \\to \\infty } Var\\left( {{\\phi ^t}{y_0} + \\sum\\limits_{i = 0}^{t - 1} {\\phi _1^i{w_{t - i}}} } \\right) \\\\ &amp;= \\mathop {\\lim }\\limits_{t \\to \\infty } \\underbrace {Var\\left( {{\\phi ^t}{y_0}} \\right)}_{ = 0{\\text{ since constant}}} + Var\\left( {\\sum\\limits_{i = 0}^{t - 1} {\\phi _1^i{w_{t - i}}} } \\right) \\\\ &amp;= \\mathop {\\lim }\\limits_{t \\to \\infty } \\sum\\limits_{i = 0}^{t - 1} {\\phi _1^{2i}Var\\left( {{w_{t - i}}} \\right)} \\\\ &amp;= \\mathop {\\lim }\\limits_{t \\to \\infty } \\sigma _w^2\\sum\\limits_{i = 0}^{t - 1} {\\phi _1^{2i}} \\\\ &amp;= \\sigma _w^2 \\cdot \\underbrace {\\frac{1}{{1 - {\\phi ^2}}}}_{\\begin{subarray}{l} {\\text{Geometric Series}} \\end{subarray}} \\end{aligned} \\] This leads us to being able to conclude the autocovariance function is: \\[\\begin{aligned} Cov\\left( {{y_t},{y_{t + h}}} \\right) &amp;= Cov\\left( {{y_t},\\phi {y_{t + h - 1}} + {w_{t + h}}} \\right) \\\\ &amp;= Cov\\left( {{y_t},\\phi {y_{t + h - 1}}} \\right) \\\\ &amp;= Cov\\left( {{y_t},{\\phi ^{\\left| h \\right|}}{y_t}} \\right) \\\\ &amp;= {\\phi ^{\\left| h \\right|}}Cov\\left( {{y_t},{y_t}} \\right) \\\\ &amp;= {\\phi ^{\\left| h \\right|}}Var\\left( {{y_t}} \\right) \\\\ &amp;= {\\phi ^{\\left| h \\right|}}\\frac{{\\sigma _w^2}}{{1 - \\phi _1^2}} \\\\ \\end{aligned} \\] Both the mean and autocovariance function do not depend on time and, thus, the AR(1) process is stationary if \\(\\left| \\phi _1 \\right| &lt; 1\\). If we assume that the AR(1) process is stationary, we can derive the mean and variance in another way. Without a loss of generality, we’ll assume \\(y_0 = 0\\). Therefore: \\[\\begin{aligned} {y_t} &amp;= {\\phi _t}{y_{t - 1}} + {w_t} \\\\ &amp;= {\\phi _1}\\left( {{\\phi _1}{y_{t - 2}} + {w_{t - 1}}} \\right) + {w_t} \\\\ &amp;= \\phi _1^2{y_{t - 2}} + {\\phi _1}{w_{t - 1}} + {w_t} \\\\ &amp;\\vdots \\\\ &amp;= \\sum\\limits_{i = 0}^{t - 1} {\\phi _1^i{w_{t - i}}} \\\\ &amp; \\\\ E\\left[ {{y_t}} \\right] &amp;= E\\left[ {\\sum\\limits_{i = 0}^{t - 1} {\\phi _1^i{w_{t - i}}} } \\right] \\\\ &amp;= \\sum\\limits_{i = 0}^{t - 1} {\\phi _1^i\\underbrace {E\\left[ {{w_{t - i}}} \\right]}_{ = 0}} \\\\ &amp;= 0 \\\\ &amp;\\\\ Var\\left( {{y_t}} \\right) &amp;= E\\left[ {{{\\left( {{y_t} - E\\left[ {{y_t}} \\right]} \\right)}^2}} \\right] \\\\ &amp;= E\\left[ {y_t^2} \\right] - {\\left( {E\\left[ {{y_t}} \\right]} \\right)^2} \\\\ &amp;= E\\left[ {y_t^2} \\right] \\\\ &amp;= E\\left[ {{{\\left( {{\\phi _1}{y_{t - 1}} + {w_t}} \\right)}^2}} \\right] \\\\ &amp;= E\\left[ {\\phi _1^2y_{t - 1}^2 + w_t^2 + 2{\\phi _1}{y_t}{w_t}} \\right] \\\\ &amp;= \\phi _1^2E\\left[ {y_{t - 1}^2} \\right] + \\underbrace {E\\left[ {w_t^2} \\right]}_{ = \\sigma _w^2} + 2{\\phi _1}\\underbrace {E\\left[ {{y_t}} \\right]}_{ = 0}\\underbrace {E\\left[ {{w_t}} \\right]}_{ = 0} \\\\ &amp;= \\underbrace {\\phi _1^2Var\\left( {{y_{t - 1}}} \\right) + \\sigma _w^2 = \\phi _1^2Var\\left( {{y_t}} \\right) + \\sigma _w^2}_{{\\text{Assume stationarity}}} \\\\ Var\\left( {{y_t}} \\right) &amp;= \\phi _1^2Var\\left( {{y_t}} \\right) + \\sigma _w^2 \\\\ Var\\left( {{y_t}} \\right) - \\phi _1^2Var\\left( {{y_t}} \\right) &amp;= \\sigma _w^2 \\\\ Var\\left( {{y_t}} \\right)\\left( {1 - \\phi _1^2} \\right) &amp;= \\sigma _w^2 \\\\ Var\\left( {{y_t}} \\right) &amp;= \\frac{{\\sigma _w^2}}{{1 - \\phi _1^2}} \\\\ \\end{aligned} \\] 3.3.3 Esimtation of the Mean Function If a time series is stationary, the mean function is constant and a possible estimator of this quantity is given by \\[\\bar{X} = {\\frac{1}{n}\\sum\\limits_{t = 1}^n {{X_t}} }.\\] This estimator is clearly unbiased and has the following variance: \\[\\begin{aligned} \\operatorname{var} \\left( {\\bar X} \\right) &amp;= \\operatorname{var} \\left( {\\frac{1}{n}\\sum\\limits_{t = 1}^n {{X_t}} } \\right) \\\\ &amp;= \\frac{1}{{{n^2}}}\\operatorname{var} \\left( {{{\\left[ {\\begin{array}{*{20}{c}} 1&amp; \\cdots &amp;1 \\end{array}} \\right]}_{1 \\times n}}{{\\left[ {\\begin{array}{*{20}{c}} {{X_1}} \\\\ \\vdots \\\\ {{X_n}} \\end{array}} \\right]}_{n \\times 1}}} \\right) \\\\ &amp;= \\frac{1}{{{n^2}}}{\\left[ {\\begin{array}{*{20}{c}} 1&amp; \\cdots &amp;1 \\end{array}} \\right]_{1 \\times n}}\\left[ {\\begin{array}{*{20}{c}} {\\gamma \\left( 0 \\right)}&amp;{\\gamma \\left( 1 \\right)}&amp; \\cdots &amp;{\\gamma \\left( {n - 1} \\right)} \\\\ {\\gamma \\left( 1 \\right)}&amp;{\\gamma \\left( 0 \\right)}&amp;{}&amp; \\vdots \\\\ \\vdots &amp;{}&amp; \\ddots &amp; \\vdots \\\\ {\\gamma \\left( {n - 1} \\right)}&amp; \\cdots &amp; \\cdots &amp;{\\gamma \\left( 0 \\right)} \\end{array}} \\right]{\\left[ {\\begin{array}{*{20}{c}} 1 \\\\ \\vdots \\\\ 1 \\end{array}} \\right]_{n \\times 1}} \\\\ &amp;= \\frac{1}{{{n^2}}}\\left( {n\\gamma \\left( 0 \\right) + 2\\left( {n - 1} \\right)\\gamma \\left( 1 \\right) + 2\\left( {n - 2} \\right)\\gamma \\left( 2 \\right) + \\cdots + 2\\gamma \\left( {n - 1} \\right)} \\right) \\\\ &amp;= \\frac{1}{n}\\sum\\limits_{h = - n}^n {\\left( {1 - \\frac{{\\left| h \\right|}}{n}} \\right)\\gamma \\left( h \\right)} \\\\ \\end{aligned}. \\] In the white noise case, the above formula reduces to the usual \\(\\operatorname{var} \\left( {\\bar X} \\right) = \\operatorname{var}(X_t)/n\\). 3.3.4 Sample Autocovariance and Autocorrelation Functions A natural estimator of the autocovariance function is given as: \\[\\hat \\gamma \\left( h \\right) = \\frac{1}{T}\\sum\\limits_{t = 1}^{T - h} {\\left( {{X_t} - \\bar X} \\right)\\left( {{X_{t + h}} - \\bar X} \\right)} \\] leading the following “plug-in” estimator of the autocorrelation function \\[\\hat \\rho \\left( h \\right) = \\frac{{\\hat \\gamma \\left( h \\right)}}{{\\hat \\gamma \\left( 0 \\right)}}.\\] A graphical representation of the autocorrelation function is often the first step of any time series analysis (assuming the process to be stationary). Consider the following simulated example: # Simulate iid gaussian RV (i.e. white noise) Xt = rnorm(100) # Compute autocorrelation acf_Xt = acf(Xt) # Plot autocorrelation plot(acf_Xt) In this example, the true autocorrelation at lag \\(h\\) (\\(|h|\\) &gt; 0 ) is equal 0 but obviously the estimated autocorrelations are random variables and are not equal to their true value. It would therefore be usefull to have have some knowledge about the variability of the sample autocorrelations (under some conditions) to assess whether the data comes from a completely random series or presents some significant correlation at some lags. The following result provide an asymptotic solution to this problem: If \\(X_t\\) is white noise with finite fourth moment, then \\(\\hat{\\rho}(h)\\) is approximately normally distributed with mean \\(0\\) and variance \\(T^{-1}\\) for all fixed \\(h\\). Using on this result, we now have an approximate method to assess whether peaks in sample autocorrelation are significant by determining whether the observed peak lies outside the interval \\(+/- 2/\\sqrt{T}\\) (i.e. an approximate 95% confidence interval). Returning to our previous example: # Plot autocorrelation with confidence bands plot(acf_Xt) It can now be observed that most peaks lies within the interval \\(+/- 2/\\sqrt{T}\\) suggesting that the true data generating process is completely random (in the linear sense). Unfortunately, this method is asymptotic (it relies on the central limit theorem) and there no “exact” tools that can be used in this case. In the simulation study below consider the “quality” of this result for \\(h = 3\\) considering different sample sizes: # Number of Monte Carlo replications B = 10000 # Define considered lag h = 3 # Sample size considered T = c(5,10,30,300) # Initialisation result = matrix(NA,B,length(T)) # Set seed set.seed(1) # Start Monte Carlo for (i in 1:B){ for (j in 1:length(T)){ # Simluate process Xt = rnorm(T[j]) # Save autocorrelation at lag h result[i,j] = acf(Xt, plot = FALSE)$acf[h+1] } } # Plot results par(mfrow = c(1,length(T))) for (i in 1:length(T)){ # Estimated empirical distribution hist(result[,i], col = &quot;lightgrey&quot;, main = paste(&quot;Sample size T =&quot;,T[i]), probability = TRUE, xlim = c(-1,1), xlab = &quot; &quot;) # Asymptotic distribution xx = seq(from = -10, to = 10, length.out = 10^3) yy = dnorm(xx,0,1/sqrt(T[i])) lines(xx,yy, col = &quot;red&quot;) } It can clearly be observed that asymptotic approximation is quite poor when \\(T = 5\\) but as the sample size increases the approximation becomes more appropriate and is nearly perfect with \\(T = 300\\). 3.4 Joint Stationarity Two time series, say \\(\\left(X_t \\right)\\) and \\(\\left(Y_t\\right)\\), are said to be jointly stationary if they are each stationary, and the cross-covariance function \\[{\\gamma _{XY}}\\left( {t,t + h} \\right) = Cov\\left( {{X_t},{Y_{t + h}}} \\right) = {\\gamma _{XY}}\\left( h \\right)\\] is a function only of lag \\(h\\). The cross-correlation function for jointly stationary times can be expressed as: \\[{\\rho _{XY}}\\left( {t,t + h} \\right) = \\frac{{{\\gamma _{XY}}\\left( {t,t + h} \\right)}}{{{\\sigma _{{X_t}}}{\\sigma _{{Y_{t + h}}}}}} = \\frac{{{\\gamma _{XY}}\\left( h \\right)}}{{{\\sigma _{{X_t}}}{\\sigma _{{Y_{t + h}}}}}} = {\\rho _{XY}}\\left( h \\right)\\] "],
["basic-models.html", "Chapter 4 Basic Models 4.1 The Backshift Operator 4.2 White Noise 4.3 Moving Average Process of Order q = 1 a.k.a MA(1) 4.4 Drift 4.5 Random Walk 4.6 Random Walk with Drift 4.7 Autoregressive Process of Order p = 1 a.k.a AR(1)", " Chapter 4 Basic Models 4.1 The Backshift Operator Definition: Backshift Operator The Backshift Operator is helpful when manipulating time series. When we backshift, we are changing the indices of the time series. e.g. \\(t \\rightarrow t-1\\). The operator is defined as: \\[B{x_t} = {x_{t - 1}}\\] If we were to repeatedly apply the backshift operator, we would receive: \\[\\begin{aligned} {B^2}{x_t} &amp;= B\\left( {B{x_t}} \\right) \\\\ &amp;= B\\left( {{x_{t - 1}}} \\right) \\\\ &amp;= {x_{t - 2}} \\\\ \\end{aligned}\\] We can generalize this behavior as: \\[{B^k}{x_t} = {x_{t - k}}\\] The backshift operator is helpful for later decompositions in addition to making differencing operations more straightforward. 4.2 White Noise The process name of white noise has meaning in the notion of colors of noise. Specifically, the white noise is a process that mirrors white light’s flat frequency spectrum. So, the process has equal frequencies in any interval of time. Definition: White Noise \\(w_t\\) or \\(\\varepsilon _t\\) is a white noise process if \\(w_t\\) are uncorrelated identically distributed random variables with \\(E\\left[w_t\\right] = 0\\) and \\(Var\\left[w_t\\right] = \\sigma ^2\\), for all \\(t\\). We can represent this algebraically as: \\[y_t = w_t,\\] where \\({w_t}\\mathop \\sim \\limits^{id} WN\\left( {0,\\sigma _w^2} \\right)\\) Now, if the \\(w_t\\) are Normally (Gaussian) distributed, then the process is known as a Gaussian White Noise e.g. \\({w_t}\\mathop \\sim \\limits^{iid} N\\left( {0,{\\sigma ^2}} \\right)\\) To generate gaussian white noise use: set.seed(1336) # Set seed to reproduce the results n = 200 # Number of observations to generate wn = ts(rnorm(n,0,1)) # Generate Guassian white noise. autoplot(wn) + ggtitle(&quot;White Noise Process&quot;) + ylab(&quot;Displacement&quot;) + xlab(&quot;Time (seconds)&quot;) 4.3 Moving Average Process of Order q = 1 a.k.a MA(1) Definition: Moving Average Process of Order (q = 1) The concept of a Moving Average Process of Order q is a way to remove “noise” and emphasize the signal. The moving average achieves this by taking the local averages of the data to produce a new smoother time series series. The newly created time series is more descriptive, but it does influence the dependence within the time series. This process is generally denoted as MA(1) and is defined as: \\[{y_t} = {\\theta _1}{w_{t - 1}} + {w_t},\\] where \\({w_t}\\mathop \\sim \\limits^{iid} WN\\left( {0,\\sigma _w^2} \\right)\\) set.seed(1345) # Set seed to reproduce the results n = 200 # Number of observations to generate sigma2 = 2 # Controls variance of Guassian white noise. theta = 0.3 # Handles the theta component of MA(1) # Generate a white noise wn = rnorm(n+1, sd = sqrt(sigma2)) # Simulate the MA(1) process ma = rep(0, n+1) for(i in 2:(n+1)) { ma[i] = theta*wn[i-1] + wn[i] } ma = ts(ma[2:(n+1)]) # Remove first item autoplot(ma) + ggtitle(&quot;Moving Average Order 1 Process&quot;) + ylab(&quot;Displacement&quot;) + xlab(&quot;Time (seconds)&quot;) 4.4 Drift Definition: Drift A drift process has two components: time and a slope. As more points are accumlated over time, the drift will match the common slope form. Specifically, the drift process has the following form: \\[y_t = y_{t-1} + \\delta \\] with the initial condition \\(y_0 = c\\). The process can be simplified using backsubstitution to being: \\[\\begin{aligned} {y_t} &amp;= {y_{t - 1}} + \\delta \\\\ &amp;= \\left( {{y_{t - 2}} + \\delta} \\right) + \\delta \\\\ &amp;\\vdots \\\\ &amp;= \\sum\\limits_{i = 1}^t {\\delta} + y_0 \\\\ {y_t} &amp;= t{\\delta} + c \\\\ \\end{aligned} \\] Again, note that a drift is similar to the slope-intercept form a linear line. e.g. \\(y = mx + b\\). To generate a drift use: n = 200 # Number of observations to generate drift = .3 # Drift Control dr = ts(drift*(1:n)) # Generate drift sequence (e.g. y = drift*x + 0) autoplot(dr) + ggtitle(&quot;Drift Process&quot;) + ylab(&quot;Displacement&quot;) + xlab(&quot;Time (seconds)&quot;) 4.5 Random Walk In 1906, Karl Pearson coined the term ‘random walk’ and demonstrated that “the most likely place to find a drunken walker is somewhere near his starting point.” Empirical evidence of this phenomenon is not too hard to find on a Friday night in Champaign. Definition: Random Walk A random walk is defined as a process where the current value of a variable is composed of the past value plus an error term that is a white noise. In algebraic form, \\[y_t = y_{t-1} + w_t\\] with the initial condition \\(y_0 = c\\). The process can be simplified using backsubstitution to being: \\[\\begin{aligned} {y_t} &amp;= {y_{t - 1}} + {w_t} \\\\ &amp;= \\left( {{y_{t - 2}} + {w_{t - 1}}} \\right) + {w_t} \\\\ &amp;\\vdots \\\\ {y_t} &amp;= \\sum\\limits_{i = 1}^t {{w_i}} + y_0 = \\sum\\limits_{i = 1}^t {{w_i}} + c \\\\ \\end{aligned} \\] To generate a random walk, we use: set.seed(1336) # Set seed to reproduce the results n = 200 # Number of observations to generate w = rnorm(n,0,1) # Generate Guassian white noise. rw = ts(cumsum(w)) # Cumulative sum # Create a data.frame to graph in ggplot2 autoplot(rw) + ggtitle(&quot;Random Walk&quot;) + ylab(&quot;Displacement&quot;) + xlab(&quot;Time (seconds)&quot;) 4.6 Random Walk with Drift In the previous case of a random walk, we assumed that drift, \\(\\delta\\), was equal to 0. What happens to the random walk if the drift is not equal to zero? That is, what happens with the initial condition \\(y_0 = c\\)? \\[\\begin{aligned} {y_t} &amp;= {y_{t - 1}} + {w_t} + \\delta \\\\ &amp;= \\left( {{y_{t - 2}} + {w_{t - 1}} + \\delta} \\right) + {w_t} + \\delta \\\\ &amp;\\vdots \\\\ {y_t} &amp;= \\sum\\limits_{i = 1}^t {\\left({w_{i} + \\delta}\\right)} + y_0 = \\sum\\limits_{i = 1}^t {{w_i}} + t\\delta + c \\\\ \\end{aligned} \\] To generate a random walk with drift we use: set.seed(1336) # Set seed to reproduce the results n = 200 # Number of observations to generate drift = .3 # Drift Control w = rnorm(n,0,1) # Generate Guassian white noise. wd = w + drift # Add a drift rwd = ts(cumsum(wd)) # Cumulative sum # Create a data.frame to graph in ggplot2 autoplot(rwd) + ggtitle(&quot;Random Walk with Drift&quot;) + ylab(&quot;Displacement&quot;) + xlab(&quot;Time (seconds)&quot;) Notice the difference the drift makes upon the random walk: # Add identifiers drift.df = data.frame(Index = 1:n, Data = drift*(1:n), Type = &quot;Drift&quot;) rw.df = data.frame(Index = 1:n, Data = rw, Type = &quot;Random Walk&quot;) rwd.df = data.frame(Index = 1:n, Data = rwd, Type = &quot;Random Walk with Drift&quot;) combined.df = rbind(drift.df, rw.df, rwd.df) ggplot(data = combined.df, aes(x = Index, y = Data, colour = Type)) + geom_line() + ggtitle(&quot;Comparisons of Random Walk&quot;) + ylab(&quot;Displacement&quot;) + xlab(&quot;Time (seconds)&quot;) 4.7 Autoregressive Process of Order p = 1 a.k.a AR(1) Definition: Autoregressive Process of Order p = 1 This process is generally denoted as AR(1) and is defined as: \\({y_t} = {\\phi _1}{y_{t - 1}} + {w_t},\\) where \\({w_t}\\mathop \\sim \\limits^{iid} WN\\left( {0,\\sigma _w^2} \\right)\\) If \\(\\phi _1 = 1\\), then the process is equivalent to a random walk. The process can be simplified using backsubstitution to being: \\[\\begin{aligned} {y_t} &amp;= {\\phi _t}{y_{t - 1}} + {w_t} \\\\ &amp;= {\\phi _1}\\left( {{\\phi _1}{y_{t - 2}} + {w_{t - 1}}} \\right) + {w_t} \\\\ &amp;= \\phi _1^2{y_{t - 2}} + {\\phi _1}{w_{t - 1}} + {w_t} \\\\ &amp;\\vdots \\\\ &amp;= {\\phi ^t}{y_0} + \\sum\\limits_{i = 0}^{t - 1} {\\phi _1^i{w_{t - i}}} \\end{aligned}\\] set.seed(1345) # Set seed to reproduce the results n = 200 # Number of observations to generate sigma2 = 2 # Controls variance of Guassian white noise. phi = 0.3 # Handles the phi component of AR(1) wn = rnorm(n+1, sd = sqrt(sigma2)) # Simulate the MA(1) process ar = rep(0,n+1) for(i in 2:n) { ar[i] = phi*ar[i-1] + wn[i] } ar = ts(ar[2:(n+1)]) autoplot(ar) + ggtitle(&quot;Autoregressive Order 1 Process&quot;) + ylab(&quot;Displacement&quot;) + xlab(&quot;Time (seconds)&quot;) "],
["arma.html", "Chapter 5 ARMA 5.1 Definition 5.2 MA / AR Operators 5.3 Redundancy 5.4 Causal + Invertible 5.5 Estimation of Parameters", " Chapter 5 ARMA 5.1 Definition 5.2 MA / AR Operators 5.3 Redundancy 5.4 Causal + Invertible 5.5 Estimation of Parameters There are two primary methods for estimating parameters: Maximum Likelihood Estimation and Method of Moments. Definition Consider \\(X_n = (X_1, X_2, \\ldots, X_n)\\) with the joint density \\(f(X_1, X_2, \\ldots, X_n ; \\theta)\\) where \\(\\theta \\in \\Theta\\). Given \\(X_1 = x_1, X_2 = x_2, \\ldots, X_n = x_n\\) is observed, we have the likelihood function of \\(\\theta\\) as \\[L(\\theta) = L(\\theta|x_1,x_2, \\ldots, x_n) = f(x_1,x_2, \\ldots, x_n | \\theta)\\] If the \\(X_i\\) are iid, then the likelihood simplifies to: \\[L(\\theta) = \\prod\\limits_{i = 1}^n {f\\left( {{x_i}|\\theta } \\right)} \\] However, that’s a bit painful to maximize with calculus. So, we opt to use the log of the function since derivatives are easier and the logarithmic function is always increasing. Thus, we traditionally use: \\[l\\left( \\theta \\right) = \\log \\left( {L\\left( \\theta \\right)} \\right) = \\sum\\limits_{i = 1}^n {\\log \\left( {f\\left( {{x_i}|\\theta } \\right)} \\right)} \\] From maximizes the likelihood function \\(L(\\theta)\\), we get the maximum likelihood estimate (MLE) of \\(\\theta\\). So, we end up with a value that makes the observed data the “most probable.” Note: The likelihood function is not a probability density function. "],
["ar1-with-mean-mu.html", "Chapter 6 \\(AR(1)\\) with mean \\(\\mu\\)", " Chapter 6 \\(AR(1)\\) with mean \\(\\mu\\) Consider an \\(AR(1)\\) process given as \\(y_t = \\phi y_{t-1} + w_t\\), \\({w_t}\\mathop \\sim \\limits^{iid} N\\left( {0,{\\sigma ^2}} \\right)\\), with \\(E\\left[ {{y_t}} \\right] = 0\\), \\(\\left| \\phi \\right| &lt; 1\\). Let \\(x_t = y_t + \\mu\\), so that \\(E\\left[ {{x_t}} \\right] = \\mu\\). Then, \\({x_t} - \\mu = {y_t}\\). Substituting in for \\(y_t\\), we get: \\[\\begin{aligned} y_t &amp;= \\phi y_{t-1} + w_t \\\\ \\underbrace {\\left( {{x_t} - \\mu } \\right)}_{ = {y_t}} &amp;= \\phi \\underbrace {\\left( {{x_{t - 1}} - \\mu } \\right)}_{ = {y_t}} + {w_t} \\\\ {x_t} &amp;= \\mu + \\phi \\left( {{x_{t - 1}} - \\mu } \\right) + {w_t} \\end{aligned}\\] In this case, \\(x_t\\) is an \\(AR(1)\\) process with mean \\(\\mu\\). This means that we have: \\(E\\left[ {{x_t}} \\right] = \\mu\\) \\[\\begin{aligned} Var\\left( {{x_t}} \\right) &amp;= Var\\left( {{x_t} - \\mu } \\right) \\hfill \\\\ &amp;= Var\\left( {{y_t}} \\right) \\hfill \\\\ &amp;= Var\\left( {\\sum\\limits_{j = 0}^\\infty {{\\phi ^j}{w_{t - j}}} } \\right) \\hfill \\\\ &amp;= \\sum\\limits_{j = 0}^\\infty {{\\phi ^{2j}}Var\\left( {{w_{t - j}}} \\right)} \\hfill \\\\ &amp;= {\\sigma ^2}\\sum\\limits_{j = 0}^\\infty {{\\phi ^{2j}}} \\hfill \\\\ &amp;= \\frac{{{\\sigma ^2}}}{{1 - {\\phi ^2}}},{\\text{ since }}\\left| \\phi \\right| &lt; 1{\\text{ and }}\\sum\\limits_{k = 0}^n {a{r^k}} = \\frac{a}{{1 - r}} \\hfill \\\\ \\end{aligned} \\] So, \\(x_t \\sim N\\left({ \\mu, \\frac{{{\\sigma ^2}}}{{1 - {\\phi ^2}}} }\\right)\\). Note that the distribution of \\(x_t\\) is normal and, thus, the density function of \\(x_t\\) is given by: \\[\\begin{aligned} f\\left( {{x_t}} \\right) &amp;= \\sqrt {\\frac{{1 - {\\phi ^2}}}{{2\\pi {\\sigma ^2}}}} \\exp \\left( { - \\frac{1}{2} \\cdot \\frac{{1 - {\\phi ^2}}}{{{\\sigma ^2}}} \\cdot {{\\left( {{x_t} - \\mu } \\right)}^2}} \\right) \\hfill \\\\ &amp;= {\\left( {2\\pi } \\right)^{ - \\frac{1}{2}}}{\\left( {{\\sigma ^2}} \\right)^{ - \\frac{1}{2}}}{\\left( {1 - {\\phi ^2}} \\right)^{\\frac{1}{2}}}\\exp \\left( { - \\frac{1}{2} \\cdot \\frac{{1 - {\\phi ^2}}}{{{\\sigma ^2}}} \\cdot {{\\left( {{x_t} - \\mu } \\right)}^2}} \\right) \\textrm{ [1]} \\\\ \\end{aligned} \\] We’ll call the last equation [1]. "],
["conditioning-time-x-t-x-t-1.html", "Chapter 7 Conditioning time \\(x_t | x_{t-1}\\)", " Chapter 7 Conditioning time \\(x_t | x_{t-1}\\) Now, consider \\(x_t | x_{t-1}\\) for \\(t &gt; 1\\). The mean is given by: \\[\\begin{aligned} E\\left[ {{x_t}|{x_{t - 1}}} \\right] &amp;= E\\left[ {\\mu + \\phi \\left( {{x_{t - 1}} - \\mu } \\right) + {w_t}|{x_{t - 1}}} \\right] \\nonumber \\\\ &amp;= \\mu + \\phi \\left( {{x_{t - 1}} - \\mu } \\right) \\end{aligned} \\] This is the case since \\(E\\left[ {{x_{t - 1}}|{x_{t - 1}}} \\right] = {x_{t - 1}}\\) and \\(E\\left[ {{w_t}|{x_{t - 1}}} \\right] = 0\\) Now, the variance is: \\[\\begin{aligned} Var\\left( {{x_t}|{x_{t - 1}}} \\right) &amp;= Var\\left( {\\mu + \\phi \\left( {{x_{t - 1}} - \\mu } \\right) + {w_t}|{x_{t - 1}}} \\right) \\hfill \\\\ &amp;= \\underbrace {Var\\left( {\\mu + \\phi \\left( {{x_{t - 1}} - \\mu } \\right)|{x_{t - 1}}} \\right)}_{ = 0} + Var\\left( {{w_t}|{x_{t - 1}}} \\right) \\hfill \\\\ &amp;= Var\\left( {{w_t}} \\right) \\hfill \\\\ &amp;= {\\sigma ^2} \\hfill \\\\ \\end{aligned} \\] Thus, we have: \\({x_t}\\sim N\\left( {\\mu + \\phi \\left( {{x_{t - 1}} - \\mu } \\right),{\\sigma ^2}} \\right)\\). Again, note that the distribution of \\(x_t\\) is normal and, thus, the density function of \\(x_t\\) is given by: \\[\\begin{aligned} f\\left( {{x_t}} \\right) &amp;= \\sqrt {\\frac{1}{{2\\pi {\\sigma ^2}}}} \\exp \\left( { - \\frac{1}{{2{\\sigma ^2}}} \\cdot {{\\left[ {\\left( {{x_t} - \\mu } \\right) - \\phi \\left( {{x_{t - 1}} - \\mu } \\right)} \\right]}^2}} \\right) \\hfill \\\\ &amp;= {\\left( {2\\pi } \\right)^{ - \\frac{1}{2}}}{\\left( {{\\sigma ^2}} \\right)^{ - \\frac{1}{2}}}\\exp \\left( { - \\frac{1}{{2{\\sigma ^2}}} \\cdot {{\\left[ {\\left( {{x_t} - \\mu } \\right) - \\phi \\left( {{x_{t - 1}} - \\mu } \\right)} \\right]}^2}} \\right) \\textrm{ [2]} \\\\ \\end{aligned} \\] And for this equation we’ll call it [2]. "],
["mle-for-sigma-2-on-ar1-with-mean-mu.html", "Chapter 8 MLE for \\(\\sigma ^2\\) on \\(AR(1)\\) with mean \\(\\mu\\)", " Chapter 8 MLE for \\(\\sigma ^2\\) on \\(AR(1)\\) with mean \\(\\mu\\) Whew, with all of the above said, we’re now ready to obtain an MLE estimate on an \\(AR(1)\\). Let \\(\\vec{\\theta} = \\left[ {\\begin{array}{*{20}{c}} \\mu \\\\ \\phi \\\\ {{\\sigma ^2}} \\end{array}} \\right]\\), then the likelihood of \\(\\vec{\\theta}\\) is given by \\(x_1, \\ldots , x_T\\) is: \\[\\begin{aligned} L\\left( {\\vec \\theta |{x_1}, \\ldots ,{x_T}} \\right) &amp;= f\\left( {{x_1}, \\ldots ,{x_T}|\\vec \\theta } \\right) \\hfill \\\\ &amp;= f\\left( {{x_1}} \\right) \\cdot \\prod\\limits_{t = 2}^T {f\\left( {{x_t}|{x_{t - 1}}} \\right)} \\end{aligned} \\] The last equality is the result of us using a lag 1 of “memory.” Also, note that \\(x_t | x_{t-1}\\) must have \\(t &gt; 1 \\in \\mathbb{N}\\). Furthermore, we have dropped the parameters in the densities, e.g. \\(\\vec{\\theta}\\) in \\(f(\\cdot)\\), to ease notation. Using equations [1] and [2], we have: \\[L\\left( {\\vec \\theta |{x_1}, \\ldots ,{x_T}} \\right) = {\\left( {2\\pi } \\right)^{ - \\frac{T}{2}}}{\\left( {{\\sigma ^2}} \\right)^{ - \\frac{T}{2}}}{\\left( {1 - {\\phi ^2}} \\right)^{\\frac{1}{2}}}\\exp \\left( { - \\frac{1}{{2{\\sigma ^2}}}\\left[ {\\left( {1 - {\\phi ^2}} \\right){{\\left( {{x_t} - \\mu } \\right)}^2} + \\sum\\limits_{t = 2}^T {{{\\left[ {\\left( {{x_t} - \\mu } \\right) - \\phi \\left( {{x_{t - 1}} - \\mu } \\right)} \\right]}^2}} } \\right]} \\right)\\] For convenience, we’ll define: \\[S\\left( {\\mu ,\\phi } \\right) = \\left( {1 - {\\phi ^2}} \\right){\\left( {{x_t} - \\mu } \\right)^2} + \\sum\\limits_{t = 2}^T {{{\\left[ {\\left( {{x_t} - \\mu } \\right) - \\phi \\left( {{x_{t - 1}} - \\mu } \\right)} \\right]}^2}} \\] Fun fact, this is called the “unconditional sum of squares.” Thus, we will operate on: \\[L\\left( {\\vec \\theta |{x_1}, \\ldots ,{x_T}} \\right) = {\\left( {2\\pi } \\right)^{ - \\frac{T}{2}}}{\\left( {{\\sigma ^2}} \\right)^{ - \\frac{T}{2}}}{\\left( {1 - {\\phi ^2}} \\right)^{\\frac{1}{2}}}\\exp \\left( { - \\frac{1}{{2{\\sigma ^2}}}S\\left( {\\mu ,\\phi } \\right)} \\right)\\] Taking the log of this yields: \\[\\begin{aligned} l\\left( {\\vec \\theta |{x_1}, \\ldots ,{x_T}} \\right) &amp;= \\log \\left( {L\\left( {\\vec \\theta |{x_1}, \\ldots ,{x_T}} \\right)} \\right) \\hfill \\\\ &amp;= - \\frac{T}{2}\\log \\left( {2\\pi } \\right) - \\frac{T}{2}\\log \\left( {{\\sigma ^2}} \\right) + \\frac{1}{2}\\left( {1 - {\\phi ^2}} \\right) - \\frac{1}{{2{\\sigma ^2}}}S\\left( {\\mu ,\\phi } \\right) \\hfill \\\\ \\end{aligned} \\] Now, taking the derivative and solving for the maximized point gives: \\[\\begin{aligned} \\frac{\\partial }{{\\partial {\\sigma ^2}}}l\\left( {\\vec \\theta |{x_1}, \\ldots ,{x_T}} \\right) &amp;= - \\frac{T}{{2{\\sigma ^2}}} + \\frac{1}{{2{\\sigma ^4}}}S\\left( {\\mu ,\\phi } \\right) \\hfill \\\\ 0 &amp;= - \\frac{T}{{2{\\sigma ^2}}} + \\frac{1}{{2{\\sigma ^4}}}S\\left( {\\mu ,\\phi } \\right) \\hfill \\\\ \\frac{T}{{2{\\sigma ^2}}} &amp;= \\frac{1}{{2{\\sigma ^4}}}S\\left( {\\mu ,\\phi } \\right) \\hfill \\\\ {{ \\sigma }^2} &amp;= \\frac{1}{T}S\\left( {\\mu ,\\phi } \\right) \\hfill \\\\ \\end{aligned} \\] Thus, the MLE for \\({\\hat \\sigma }^2 = \\frac{1}{T}S\\left( {\\hat \\mu ,\\hat \\phi } \\right)\\), where \\(\\hat \\mu\\) and \\(\\hat \\phi\\) are the MLEs for \\(\\mu , \\phi\\) that are obtained numerically via either Newton Raphson or a Scoring Algorithm. (More details in a numerical recipe book.) "],
["conditional-mle-on-ar1-with-mean-mu.html", "Chapter 9 Conditional MLE on \\(AR(1)\\) with mean \\(\\mu\\) 9.1 Method of Moments 9.2 Prediction (Forecast)", " Chapter 9 Conditional MLE on \\(AR(1)\\) with mean \\(\\mu\\) A common strategy to reduce the dependency on numerical recipes is to simplify \\(l\\left( {\\vec \\theta |{x_1}, \\ldots ,{x_T}} \\right)\\) by using \\({l^*}\\left( {\\vec \\theta |{x_1}, \\ldots ,{x_T}} \\right)\\): \\[\\begin{aligned} {l^*}\\left( {\\vec \\theta |{x_1}, \\ldots ,{x_T}} \\right) &amp;= \\prod\\limits_{t = 2}^T {\\log \\left( {f\\left( {{x_t}|{x_{t - 1}}} \\right)} \\right)} \\hfill \\\\ &amp;= \\prod\\limits_{t = 2}^T {\\log \\left( {{{\\left( {2\\pi } \\right)}^{ - \\frac{1}{2}}}{{\\left( {{\\sigma ^2}} \\right)}^{ - \\frac{1}{2}}}\\exp \\left( { - \\frac{1}{{2{\\sigma ^2}}} \\cdot {{\\left[ {\\left( {{x_t} - \\mu } \\right) - \\phi \\left( {{x_{t - 1}} - \\mu } \\right)} \\right]}^2}} \\right)} \\right)} \\hfill \\\\ &amp;= - \\frac{{\\left( {T - 1} \\right)}}{2}\\log \\left( {2\\pi } \\right) - \\frac{{\\left( {T - 1} \\right)}}{2}\\log \\left( {{\\sigma ^2}} \\right) - \\frac{1}{{2{\\sigma ^2}}}\\sum\\limits_{t = 2}^T {{{\\left[ {\\left( {{x_t} - \\mu } \\right) - \\phi \\left( {{x_{t - 1}} - \\mu } \\right)} \\right]}^2}} \\hfill \\\\ \\end{aligned} \\] Again, for convenience, we’ll define: \\[{S_c}\\left( {\\mu ,\\phi } \\right) = \\sum\\limits_{t = 2}^T {{{\\left[ {\\left( {{x_t} - \\mu } \\right) - \\phi \\left( {{x_{t - 1}} - \\mu } \\right)} \\right]}^2}} \\] Fun fact, this is called the “conditional sum of squares.” So, we will use: \\[{l^*}\\left( {\\vec \\theta |{x_1}, \\ldots ,{x_T}} \\right) = - \\frac{{\\left( {T - 1} \\right)}}{2}\\log \\left( {2\\pi } \\right) - \\frac{{\\left( {T - 1} \\right)}}{2}\\log \\left( {{\\sigma ^2}} \\right) - \\frac{1}{{2{\\sigma ^2}}}{S_c}\\left( {\\mu ,\\phi } \\right)\\] Taking the derivative with respect to \\(\\mu\\) gives: \\[\\begin{aligned} \\frac{\\partial }{{\\partial \\mu }}{l^*}\\left( {\\vec \\theta |{x_1}, \\ldots ,{x_T}} \\right) &amp;= - \\frac{1}{{2{\\sigma ^2}}}\\sum\\limits_{t = 2}^T {2\\left[ {\\left( {{x_t} - \\mu } \\right) - \\phi \\left( {{x_{t - 1}} - \\mu } \\right)} \\right]\\left( {\\phi - 1} \\right)} \\hfill \\\\ &amp;= \\frac{{1 - \\phi }}{{{\\sigma ^2}}}\\sum\\limits_{t = 2}^T {\\left[ {\\left( {{x_t} - \\mu } \\right) - \\phi \\left( {{x_{t - 1}} - \\mu } \\right)} \\right]} \\hfill \\\\ &amp;= \\frac{{1 - \\phi }}{{{\\sigma ^2}}}\\sum\\limits_{t = 2}^T {\\left( {{x_t} - \\phi {x_{t - 1}} - \\mu \\left( {1 - \\phi } \\right)} \\right)} \\hfill \\\\ &amp;= -\\frac{{{{\\left( {1 - \\phi } \\right)}^2}}}{{{\\sigma ^2}}}\\mu \\left( {T - 1} \\right) + \\frac{{\\left( {1 - \\phi } \\right)}}{{{\\sigma ^2}}}\\sum\\limits_{t = 2}^T {\\left( {{x_t} - \\phi {x_{t - 1}}} \\right)} \\hfill \\\\ \\end{aligned} \\] Solving for \\(\\mu^{*}\\) gives: \\[\\begin{aligned} 0 &amp;= \\frac{\\partial }{{\\partial \\mu }}{l^*}\\left( {\\vec \\theta |{x_1}, \\ldots ,{x_t}} \\right) \\hfill \\\\ 0 &amp;= - \\frac{{{{\\left( {1 - \\phi } \\right)}^2}}}{{{\\sigma ^2}}}{\\mu ^*}\\left( {T - 1} \\right) + \\frac{{\\left( {1 - {\\phi ^*}} \\right)}}{{\\sigma _*^2}}\\sum\\limits_{t = 2}^T {\\left( {{x_t} - {\\phi ^*}{x_{t - 1}}} \\right)} \\hfill \\\\ \\frac{{{{\\left( {1 - {\\phi ^*}} \\right)}^2}}}{{\\sigma _*^2}}{\\mu ^*}\\left( {T - 1} \\right) &amp;= \\frac{{\\left( {1 - {\\phi ^*}} \\right)}}{{\\sigma _*^2}}\\sum\\limits_{t = 2}^T {\\left( {{x_t} - {\\phi ^*}{x_{t - 1}}} \\right)} \\hfill \\\\ {\\mu ^*}\\left( {1 - {\\phi ^*}} \\right)\\left( {T - 1} \\right) &amp;= \\sum\\limits_{t = 2}^T {\\left( {{x_t} - {\\phi ^*}{x_{t - 1}}} \\right)} \\hfill \\\\ {\\mu ^*} &amp;= \\frac{1}{{\\left( {1 - {\\phi ^*}} \\right)\\left( {T - 1} \\right)}}\\sum\\limits_{t = 2}^T {\\left( {{x_t} - {\\phi ^*}{x_{t - 1}}} \\right)} \\hfill \\\\ {\\mu ^*} &amp;= \\frac{1}{{1 - {\\phi ^*}}}\\left[ {\\underbrace {\\frac{1}{{T - 1}}\\sum\\limits_{t = 2}^T {{x_t}} }_{ = {{\\bar x}_{\\left( 2 \\right)}}} - \\underbrace {\\frac{{{\\phi ^*}}}{{T - 1}}\\sum\\limits_{t = 2}^T {{x_{t - 1}}} }_{ = {{\\bar x}_{\\left( 1 \\right)}}}} \\right] \\hfill \\\\ {{\\hat \\mu }^*} &amp;= \\frac{1}{{1 - {\\phi ^*}}}\\left( {{{\\bar x}_{\\left( 2 \\right)}} - \\phi {{\\bar x}_{\\left( 1 \\right)}}} \\right) \\hfill \\\\ \\end{aligned} \\] When \\(T\\) is large, we have the following: \\[\\begin{aligned} {{\\bar x}_{\\left( 1 \\right)}} \\approx \\bar x &amp;,{{\\bar x}_{\\left( 2 \\right)}} \\approx \\bar x \\hfill \\\\ \\hfill \\\\ {{\\hat \\mu }^*} &amp;= \\frac{1}{{1 - {\\phi ^*}}}\\left( {\\bar x - {\\phi ^*}\\bar x} \\right) \\hfill \\\\ &amp;= \\frac{{\\bar x}}{{1 - {\\phi ^*}}}\\left( {1 - {\\phi ^*}} \\right) \\hfill \\\\ &amp;= \\bar x \\hfill \\\\ \\end{aligned} \\] Taking the derivative with respect to \\(\\sigma^2\\) and solving for \\(\\sigma^2\\) gives: \\[\\begin{aligned} \\frac{\\partial }{{\\partial {\\sigma ^2}}}{l^*}\\left( {\\vec \\theta |{x_1}, \\ldots ,{x_T}} \\right) &amp;= - \\frac{{\\left( {T - 1} \\right)}}{{2\\sigma _*^2}} + \\frac{1}{{2\\sigma _*^4}}{S_c}\\left( {\\mu ,\\phi } \\right) \\hfill \\\\ 0 &amp;= - \\frac{{\\left( {T - 1} \\right)}}{{2\\sigma _*^2}} + \\frac{1}{{2\\sigma _*^4}}{S_c}\\left( {\\mu ,\\phi } \\right) \\hfill \\\\ \\frac{{\\left( {T - 1} \\right)}}{{2\\sigma _*^2}} &amp;= \\frac{1}{{2\\sigma _*^4}}{S_c}\\left( {\\mu ,\\phi } \\right) \\hfill \\\\ \\hat \\sigma _*^2 &amp;= \\frac{1}{{T - 1}}{S_c}\\left( {{{\\hat \\mu }^*},{{\\hat \\phi }^*}} \\right) \\hfill \\\\ \\end{aligned} \\] Taking the derivative with respect to \\(\\phi\\) gives: \\[\\begin{aligned} \\frac{\\partial }{{\\partial \\phi }}{l^*}\\left( {\\vec \\theta |{x_1}, \\ldots ,{x_T}} \\right) &amp;= - \\frac{1}{{2{\\sigma ^2}}}\\sum\\limits_{t = 2}^T { - 2\\left[ {\\left( {{x_t} - \\mu } \\right) - \\phi \\left( {{x_{t - 1}} - \\mu } \\right)} \\right]\\left( {{x_{t - 1}} - \\mu } \\right)} \\hfill \\\\ &amp;= \\frac{1}{{{\\sigma ^2}}}\\sum\\limits_{t = 2}^T {\\left[ {{x_t} - \\phi {x_{t - 1}} - \\mu \\left( {1 - \\phi } \\right)} \\right]\\left( {{x_{t - 1}} - \\mu } \\right)} \\hfill \\\\ &amp;= \\frac{1}{{{\\sigma ^2}}}\\sum\\limits_{t = 2}^T {\\left[ {{x_t}{x_{t - 1}} - \\phi x_{t - 1}^2 - \\mu \\left( {1 - \\phi } \\right){x_{t - 1}} - \\mu {x_t} + \\mu \\phi {x_{t - 1}} + {\\mu ^2}\\left( {1 - \\phi } \\right)} \\right]} \\hfill \\\\ &amp;= \\frac{1}{{{\\sigma ^2}}}\\left[ \\begin{gathered} \\sum\\limits_{t = 2}^T {{x_t}{x_{t - 1}}} - \\phi \\sum\\limits_{t = 2}^T {x_{t - 1}^2} - \\mu \\left( {1 - \\phi } \\right)\\left( {T - 1} \\right){{\\bar x}_{\\left( 1 \\right)}} \\hfill \\\\ - \\mu \\left( {T - 1} \\right){{\\bar x}_{\\left( 2 \\right)}} + \\phi \\mu \\left( {T - 1} \\right){{\\bar x}_{\\left( 1 \\right)}} + {\\mu ^2}\\left( {1 - \\phi } \\right)\\left( {T - 1} \\right) \\hfill \\\\ \\end{gathered} \\right] \\end{aligned}\\] Solving for \\(\\phi\\) gives: \\[\\begin{aligned} 0 &amp;= \\frac{\\partial }{{\\partial \\phi }}{l^*}\\left( {\\vec \\theta |{x_1}, \\ldots ,{x_T}} \\right) \\hfill \\\\ 0 &amp;= \\sum\\limits_{t = 2}^T {{x_t}{x_{t - 1}}} - {{\\hat \\phi }^*}\\sum\\limits_{t = 2}^T {x_{t - 1}^2} - \\left( {{{\\bar x}_{\\left( 2 \\right)}} - {{\\hat \\phi }^*}{{\\bar x}_{\\left( 1 \\right)}}} \\right)\\left( {T - 1} \\right){{\\bar x}_{\\left( 1 \\right)}} - \\frac{{{{\\bar x}_{\\left( 2 \\right)}} - {{\\hat \\phi }^*}{{\\bar x}_{\\left( 1 \\right)}}}}{{1 - {{\\hat \\phi }^*}}}\\left( {T - 1} \\right){{\\bar x}_{\\left( 2 \\right)}} \\\\ &amp;+ {{\\hat \\phi }^*}\\frac{{{{\\bar x}_{\\left( 2 \\right)}} - {{\\hat \\phi }^*}{{\\bar x}_{\\left( 1 \\right)}}}}{{1 - {{\\hat \\phi }^*}}}\\left( {T - 1} \\right){{\\bar x}_{\\left( 1 \\right)}} + {\\left( {\\frac{{{{\\bar x}_{\\left( 2 \\right)}} - {{\\hat \\phi }^*}{{\\bar x}_{\\left( 1 \\right)}}}}{{1 - {{\\hat \\phi }^*}}}} \\right)^2}\\left( {1 - {{\\hat \\phi }^*}} \\right)\\left( {T - 1} \\right) \\hfill \\\\ &amp;\\vdots \\hfill \\\\ &amp;{\\text{Magic}} \\hfill \\\\ &amp;\\vdots \\hfill \\\\ {{\\hat \\phi }^*} &amp;= \\frac{{\\sum\\limits_{t = 2}^T {\\left( {{x_t} - {{\\bar x}_{\\left( 2 \\right)}}} \\right)\\left( {{x_{t-1}} - {{\\bar x}_{\\left( 1 \\right)}}} \\right)} }}{{\\sum\\limits_{t = 2}^T {{{\\left( {{x_{t - 1}} - {{\\bar x}_{\\left( 1 \\right)}}} \\right)}^2}} }} \\hfill \\\\ \\end{aligned} \\] When \\(T\\) is large, we have: \\[\\begin{aligned} \\sum\\limits_{t = 2}^T {\\left( {{x_t} - {{\\bar x}_{\\left( 2 \\right)}}} \\right)\\left( {{x_t} - {{\\bar x}_{\\left( 1 \\right)}}} \\right)} &amp;\\approx \\sum\\limits_{t = 2}^T {\\left( {{x_t} - \\bar x} \\right)\\left( {{x_{t - 1}} - \\bar x} \\right)} \\hfill \\\\ \\sum\\limits_{t = 2}^T {{{\\left( {{x_{t - 1}} - {{\\bar x}_{\\left( 1 \\right)}}} \\right)}^2}} &amp;\\approx \\sum\\limits_{t = 1}^T {{{\\left( {{x_t} - \\bar x} \\right)}^2}} \\hfill \\\\ \\hfill \\\\ {{\\hat \\phi }^*} &amp;= \\frac{{\\sum\\limits_{t = 2}^T {\\left( {{x_t} - {{\\bar x}_{\\left( 2 \\right)}}} \\right)\\left( {{x_t} - {{\\bar x}_{\\left( 1 \\right)}}} \\right)} }}{{\\sum\\limits_{t = 2}^T {{{\\left( {{x_{t - 1}} - {{\\bar x}_{\\left( 1 \\right)}}} \\right)}^2}} }} \\approx \\frac{{\\sum\\limits_{t = 2}^T {\\left( {{x_t} - \\bar x} \\right)\\left( {{x_{t - 1}} - \\bar x} \\right)} }}{{\\sum\\limits_{t = 1}^T {{{\\left( {{x_t} - \\bar x} \\right)}^2}} }} = \\hat \\rho \\left( 1 \\right) \\hfill \\\\ \\end{aligned} \\] Consider a time series given by \\(x_t \\sim ARMA(p,q)\\). This gives us with a paramter space \\(\\Omega\\) that looks like so: \\[\\vec \\varphi = \\left[ {\\begin{array}{*{20}{c}} {{\\phi _1}} \\\\ \\vdots \\\\ {{\\phi _p}} \\\\ {{\\theta _1}} \\\\ \\vdots \\\\ {{\\theta _q}} \\\\ {{\\sigma ^2}} \\end{array}} \\right]\\] In order to estimate this parameter space, we must assume the following three conditions: The process is casual The process is invertible The process has Gaussian innovations. Innovations are a time series equivalent to residuals. That is, an innovation is given by \\({x_t} - \\hat x_t^{t - 1}\\), where \\(\\hat x_t^{t - 1}\\) is the prediction at time \\(t\\) given \\(t-1\\) observations and \\({x_t}\\) is the true value observed at time \\(t\\). There are two main ways of performing such an estimation of the parameter space. Method of Moments (MoM) Maximum Likelihood / Least Squares Estimation [MLE / LSE] 9.1 Method of Moments The goal behind the estimation with Method of Moments is to match the theoretical moment (e.g. \\(E\\left[ {x_t^k} \\right]\\)) with the sample moment (e.g \\(\\frac{1}{n}\\sum\\limits_{i = 1}^n {x_i^k}\\)), where \\(k\\) denotes the moment. This method often leads to suboptimal estimates for general ARMA models. However, it is quite optimal for \\(AR(p)\\). 9.1.1 Method of Moments - AR(p) Consider an \\(AR(p)\\) process represented by: \\[{x_t} = {\\phi _1}{x_{t - 1}} + \\cdots + {\\phi _p}{x_{t - p}} + {w_t}\\] where \\(w_t \\sim N(0,\\sigma^2)\\) To begin, we find the Covariance of the process when \\(h &gt; 0\\): \\[\\begin{aligned} Cov\\left( {{x_{t + h}},{x_t}} \\right) &amp;\\mathop = \\limits^{\\left( {h &gt; 0} \\right)} Cov\\left( {{\\phi _1}{x_{t + h - 1}} + \\cdots + {\\phi _p}{x_{t + h - p}} + {w_{t + h}},{x_t}} \\right) \\hfill \\\\ &amp;= {\\phi _1}Cov\\left( {{x_{t + h - 1}},{x_t}} \\right) + \\cdots + {\\phi _p}Cov\\left( {{x_{t + h - p}},{x_t}} \\right) + Cov\\left( {{w_{t + h}},{x_t}} \\right) \\hfill \\\\ &amp;= {\\phi _1}\\gamma \\left( {h - 1} \\right) + \\cdots + {\\phi _p}\\gamma \\left( {h - p} \\right) \\hfill \\\\ \\end{aligned} \\] Now, we turn our attention to the variance of the process: \\[\\begin{aligned} Var\\left( {{w_t}} \\right) &amp;= Cov\\left( {{w_t},{w_t}} \\right) \\hfill \\\\ &amp;= Cov\\left( {{w_t},{w_t}} \\right) + \\underbrace {Cov\\left( {{\\phi _1}{x_{t - 1}},{w_t}} \\right)}_{ = 0} + \\cdots + \\underbrace {Cov\\left( {{\\phi _p}{x_{t - p}},{w_t}} \\right)}_{ = 0} \\hfill \\\\ &amp;= Cov\\left( {\\underbrace {{\\phi _1}{x_{t - 1}} + \\cdots + {\\phi _p}{x_p} + {w_t}}_{ = {x_t}},{w_t}} \\right) \\hfill \\\\ &amp;= Cov\\left( {{x_t},{w_t}} \\right) \\hfill \\\\ &amp;= Cov\\left( {{x_t},{x_t} - {\\phi _1}{x_{t - 1}} - \\cdots - {\\phi _p}{x_p}} \\right) \\hfill \\\\ &amp;= Cov\\left( {{x_t},{x_t}} \\right) - {\\phi _1}Cov\\left( {{x_t},{x_{t - 1}}} \\right) - \\cdots - {\\phi _p}Cov\\left( {{x_t},{x_{t - p}}} \\right) \\hfill \\\\ &amp;= \\gamma \\left( 0 \\right) - {\\phi _1}\\gamma \\left( 1 \\right) - \\cdots - {\\phi _p}\\gamma \\left( p \\right) \\hfill \\\\ \\end{aligned} \\] Together, these equations are known as the Yule-Walker equations. 9.1.2 Yule-Walker Definition Equation form: \\[\\begin{aligned} \\gamma \\left( h \\right) &amp;= {\\phi _1}\\gamma \\left( {h - 1} \\right) - \\cdots - {\\phi _p}\\gamma \\left( {h - p} \\right) \\hfill \\\\ {\\sigma ^2} &amp;= \\gamma \\left( 0 \\right) - {\\phi _1}\\gamma \\left( 1 \\right) - \\cdots - {\\phi _p}\\gamma \\left( p \\right) \\hfill \\\\ h &amp; = 1, \\ldots, p \\end{aligned} \\] Matrix form: \\[\\begin{aligned} \\Gamma \\vec \\phi &amp;= \\vec \\gamma \\hfill \\\\ {\\sigma ^2} &amp;= \\gamma \\left( 0 \\right) - {{\\vec \\phi }^T}\\vec \\gamma \\hfill \\\\ \\hfill \\\\ \\vec \\phi &amp;= \\left[ {\\begin{array}{*{20}{c}} {{\\phi _1}} \\\\ \\vdots \\\\ {{\\phi _p}} \\end{array}} \\right]_{p \\times 1},\\vec \\gamma = \\left[ {\\begin{array}{*{20}{c}} {\\gamma \\left( 1 \\right)} \\\\ \\vdots \\\\ {\\gamma \\left( p \\right)} \\end{array}} \\right]_{p \\times 1},\\Gamma = \\left\\{ {\\gamma \\left( {k - j} \\right)} \\right\\}_{j,k = 1}^p \\\\ \\end{aligned} \\] More aptly, the structure of \\(\\Gamma\\) looks like the following: \\[\\Gamma = {\\left[ {\\begin{array}{*{20}{c}} {\\gamma \\left( 0 \\right)}&amp;{\\gamma \\left( { - 1} \\right)}&amp;{\\gamma \\left( { - 2} \\right)}&amp; \\cdots &amp;{\\gamma \\left( {1 - p} \\right)} \\\\ {\\gamma \\left( 1 \\right)}&amp;{\\gamma \\left( 0 \\right)}&amp;{\\gamma \\left( { - 1} \\right)}&amp; \\cdots &amp;{\\gamma \\left( {2 - p} \\right)} \\\\ {\\gamma \\left( 2 \\right)}&amp;{\\gamma \\left( 1 \\right)}&amp;{\\gamma \\left( 0 \\right)}&amp; \\cdots &amp;{\\gamma \\left( {3 - p} \\right)} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ {\\gamma \\left( {p - 1} \\right)}&amp;{\\gamma \\left( {p - 2} \\right)}&amp;{\\gamma \\left( {p - 3} \\right)}&amp; \\cdots &amp;{\\gamma \\left( 0 \\right)} \\end{array}} \\right]_{p \\times p}}\\] Note, that we are able to use the above equations to effectively estimate \\(\\vec \\phi\\) and \\(\\sigma ^2\\). \\[\\left[ \\begin{aligned} \\hat{\\vec{\\phi}} &amp;= {{\\hat \\Gamma }^{ - 1}}\\hat{\\vec{\\gamma}} \\hfill \\\\ {{\\hat \\sigma }^2} &amp;= \\hat \\gamma \\left( 0 \\right) - {{\\hat{\\vec{\\gamma}}}^T}{{\\hat \\Gamma }^{ - 1}}\\hat{\\vec{\\gamma}} \\hfill \\\\ \\end{aligned} \\right. \\to {\\text{Yule - Walker Estimates}}\\] For the second equation, we are effectively substituting in the first equation for \\(\\hat{\\vec{\\phi}}\\), hence the quadratic form \\({{\\hat{\\vec{\\gamma}}}^T}{{\\hat \\Gamma }^{ - 1}}\\hat{\\vec{\\gamma}}\\). With this being said, there are a few nice asymptotic properties that we obtain for an \\(AR(p)\\). \\(\\sqrt T \\left( {\\hat{\\vec{\\phi}} - \\vec \\phi } \\right)\\mathop \\to \\limits_{t \\to \\infty }^L N\\left( {\\vec 0,{\\sigma ^2}{\\Gamma ^{ - 1}}} \\right)\\) \\({\\hat \\sigma ^2}\\mathop \\to \\limits^p {\\sigma ^2}\\) Yule-Walker estimates are optimal in the sense that they have the smallest asymptotic variance i.e. \\[Var\\left( {\\sqrt{T} \\hat{\\vec{\\phi}} } \\right) = {\\sigma ^2}{\\Gamma ^{ - 1}}\\] However, they are not necessarily optimal with small sample sizes. Conceptually, the reason for this optimality result is a consequence from the linear dependence between moments and variables. This is not true for MA or ARMA, which are both nonlinear and suboptimal. 9.1.3 Estimates Consider \\(x_t\\) as an \\(MA(1)\\) process: \\({x_t} = \\theta {w_{t - 1}} + {w_t},{w_t}\\mathop \\sim \\limits^{i.i.d} N\\left( {0,{\\sigma ^2}} \\right)\\) Finding the covariance when \\(h = 1\\) gives: \\[\\begin{aligned} Cov\\left( {{x_t},{x_{t - 1}}} \\right) &amp;= Cov\\left( {\\theta {w_{t - 1}} + {w_t},\\theta {w_{t - 2}} + {w_{t - 1}}} \\right) \\hfill \\\\ &amp;= Cov\\left( {\\theta {w_{t - 1}},{w_{t - 1}}} \\right) \\hfill \\\\ &amp;= \\theta {\\sigma ^2} \\hfill \\\\ \\end{aligned} \\] Finding the variance (e.g. \\(h=0\\)) gives: \\[\\begin{aligned} Cov\\left( {{x_t},{x_t}} \\right) &amp;= Cov\\left( {\\theta {w_{t - 1}} + {w_t},\\theta {w_{t - 1}} + {w_t}} \\right) \\hfill \\\\ &amp;= {\\theta ^2}Cov\\left( {{w_{t - 1}},{w_{t - 1}}} \\right) + \\underbrace {2\\theta Cov\\left( {{w_{t - 1}},{w_t}} \\right)}_{ = 0} + Cov\\left( {{w_t},{w_t}} \\right) \\hfill \\\\ &amp;= {\\theta ^2}{\\sigma ^2} + {\\sigma ^2} \\hfill \\\\ &amp;= {\\sigma ^2}\\left( {1 + {\\theta ^2}} \\right) \\hfill \\\\ \\end{aligned} \\] This gives us the MA(1) ACF of: \\[\\rho \\left( h \\right) = \\left\\{ {\\begin{array}{*{20}{c}} 1&amp;{h = 0} \\\\ {\\frac{\\theta }{{{\\theta ^2} + 1}}}&amp;{h = \\pm 1} \\end{array}} \\right.\\] With this in mind, let’s solve for possible \\(\\theta\\) values: \\[\\begin{aligned} \\rho \\left( 1 \\right) &amp;= \\frac{\\theta }{{{\\theta ^2} + 1}} \\hfill \\\\ \\Rightarrow \\theta &amp;= \\left( {{\\theta ^2} + 1} \\right)\\rho \\left( 1 \\right) \\hfill \\\\ \\theta &amp;= \\rho \\left( 1 \\right){\\theta ^2} + \\rho \\left( 1 \\right) \\hfill \\\\ 0 &amp;= \\rho \\left( 1 \\right){\\theta ^2} - \\theta + \\rho \\left( 1 \\right) \\hfill \\\\ \\end{aligned} \\] Yuck, that looks nasty. Let’s dig out an ol’ friend from middle school known as the quadratic formula: \\[\\theta = \\frac{{ - b \\pm \\sqrt {{b^2} - 4ac} }}{{2a}}\\] Applying the quadratic formula leads to: \\[\\begin{aligned} a &amp;= \\rho \\left( h \\right), b = -1, c = \\rho \\left( h \\right) \\\\ \\theta &amp;= \\frac{{1 \\pm \\sqrt {{1^2} - 4\\rho \\left( h \\right)\\rho \\left( h \\right)} }}{{2\\rho \\left( h \\right)}} \\hfill \\\\ \\theta &amp;= \\frac{{1 \\pm \\sqrt {1 - 4{{\\left[ {\\rho \\left( h \\right)} \\right]}^2}} }}{{2\\rho \\left( h \\right)}} \\hfill \\\\ \\end{aligned} \\] Thus, we have two possibilities: \\[\\begin{aligned} {\\theta _1} &amp;= \\frac{{1 + \\sqrt {1 - 4{{\\left[ {\\rho \\left( h \\right)} \\right]}^2}} }}{{2\\rho \\left( h \\right)}} \\hfill \\\\ {\\theta _2} &amp;= \\frac{{1 - \\sqrt {1 - 4{{\\left[ {\\rho \\left( h \\right)} \\right]}^2}} }}{{2\\rho \\left( h \\right)}} \\hfill \\\\ \\end{aligned}\\] To ensure invertibility, we mandate that \\(\\left| {\\rho \\left( 1 \\right)} \\right| &lt; \\frac{1}{2}\\). Thus, we opt for \\({\\theta _2}\\). So, our estimator is: \\[\\hat \\theta = \\frac{{1 - \\sqrt {1 - 4{{\\left[ {\\hat \\rho \\left( 1 \\right)} \\right]}^2}} }}{{2\\hat \\rho \\left( 1 \\right)}}\\] Furthermore, it can be shown that: \\[\\sqrt T \\left( {\\hat \\theta - \\theta } \\right)\\mathop \\to \\limits_{T \\to \\infty }^L N\\left( {0 ,\\frac{{1 + {\\theta ^2} + 4{\\theta ^4} + {\\theta ^6} + {\\theta ^8}}}{{{{\\left( {1 - {\\theta ^2}} \\right)}^2}}}} \\right)\\] So, this is not a really optimal estimator… 9.2 Prediction (Forecast) "]
]
