[
["index.html", "A Tour of Time Series Analysis with R Preface Contributing Bibliographic Note Rendering Mathematical Formulae R Code Conventions License", " A Tour of Time Series Analysis with R James Balamuta, Stéphane Guerrier, Roberto Molinari and Haotian Xu 2016-08-22 Preface This text is designed as an introduction to time series analysis and is used as a support document for the class STAT 429 (Time Series Analysis) given at the University of Illinois at Urbana-Champaign. It preferable to always access the text online rather than a printed to be sure you are using the latest version. The online version so affords additional features over the traditional PDF copy such as a scaling text, variety of font faces, and themed backgrounds. However, if you are in need of a local copy, a pdf version is also available. This document is under active development and as a result is likely to contains many errors. As Montesquieu puts it: “La nature semblait avoir sagement pourvu à ce que les sottises des hommes fussent passagères, et les livres les immortalisent.” Contributing If you notice any errors, we would be grateful if you would let us know. To let us know about the errors, there are two options available to you. The first and subsequently the fastest being if you are familiar with GitHub and know RMarkdown, then make a pull request and fix the issue yourself!. Note, in the online version, there is even an option to automatically start the pull request by clicking the edit button in the top-left corner of the text. The second option, that will have a slightly slower resolution time is to send an email to balamut2 AT illinois DOT edu that includes: the error and a possible revision. Please put in the subject header: [TTS]. Bibliographic Note This text is heavily inspired by the following three execellent references: “Time Series Analysis and Its Applications”, Third Edition, Robert H. Shumway &amp; David S. Stoffer. “Time Series for Macroeconomics and Finance”, John H. Cochrane. “Cours de Séries Temporelles: Théorie et Applications”, Volume 1, Arthur Charpentier. Rendering Mathematical Formulae Throughout the book, there will be mathematical symbols used to express the material. Depending on the version of the book, there are two different render engines. For the online version, the text uses MathJax to render mathematical notation for the web. In the event the formulae does not load for a specific chapter, first try to refresh the page. 9 times out of 10 the issue is related to the software library not loading quickly. For the pdf version, the text is built using the recommended AMS LaTeX symbolic packages. As a result, there should be no issue displaying equations. An example of a mathematical rendering capabilities would be given as: \\[ a^2 + b^2 = c^2 \\] R Code Conventions The code used throughout the book will predominately be R code. To obtain a copy of R, go to the Comprehensive R Archive Network (CRAN) and download the appropriate installer for your operating system. When R code is displayed it will be typeset using a monospace font with syntax highlighting enabled to ensure the differentiation of functions, variables, and so on. For example, the following adds 1 to 1 a = 1L + 1L a Each code segment may contain actual output from R. Such output will appear in grey font prefixed by ##. For example, the output of the above code segment would look like so: ## [1] 2 Alongside the PDF download of the book, you should find the R code used within each chapter. License This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],
["introduction.html", "Chapter 1 Introduction 1.1 Time Series 1.2 Exploratory Data Analysis for Time Series 1.3 Basic Time Series Models", " Chapter 1 Introduction \"One damned thing after another.\" ~ R. A. Fisher > --> Prévoir consiste à projeter dans l’avenir ce qu’on a perçu dans le passé. Henri Bergson > *Hâtons-nous; le temps fuit, et nous entraîne avec soi : le moment où je parle est déjà loin de moi*. Nicolas Boileau > --> After reading this chapter you will be able to: Describe what a time series is. Perform exploratory data analysis on time series data. Evaluate different characteristics of a time series. Classify basic time series models through equations and plots. Manipulate a time series equation using backsubstitution. 1.1 Time Series Generally speaking a time series (or stochastic process) corresponds to set of “repeated” observations of the same variable such as price of a financial asset or temperature in a given location. In terms of notation a time series is often written as \\[\\left(X_1, X_2, ..., X_n \\right) \\;\\;\\; \\text{ or } \\;\\;\\; \\left(X_t\\right)_{t = 1,...,n}.\\] The time index \\(t\\) is contained within either the set of reals, \\(\\real\\), or integers, \\(\\integers\\). When \\(t \\in \\real\\), the time series becomes a continuous-time stochastic process such a Brownian motion, a model used to represent the random movement of particles within a suspended liquid or gas, or an ElectroCardioGram (ECG) signal, which corresponds to the palpitations of the heart. However, within this text, we will limit ourselves to the cases where \\(t \\in \\integers\\), better known as discrete-time processes. Discrete-time processes are where a variable is measured sequentially at fixed and equally spaced intervals in time akin to 1.1. This implies that we will have two assumptions: \\(t\\) is not random e.g. the time at which each observation is measured is known, and the time between two consecutive observations is constant. Figure 1.1: Discrete-time can be thought of as viewing a number line with equally spaced points. Moreover, the term “time series” can also represent a probability model for a set of observations. For example, one of the fundamental probability models used in time series analysis is called a white noise process and is defined as \\[W_t \\mathop \\sim \\limits^{iid} N(0, \\sigma^2).\\] This statement simply means that \\((W_t)\\) is normally distributed and independent over time. This model may appear to be dull but as we will see it is a crucial component to constructing more complex models. Unlike the white noise process, time series are typically not independent over time. Suppose that the temperature in Champaign is unusually low, then it is reasonable to assume that tomorrow’s temperature will also be low. Indeed, such behavior would suggest the existence of a dependency over time. The time series methods we will discuss in this text consists of parametric models used to characterize (or at least approximate) the joint distribution of \\((X_t)\\). Often, time series models can be decomposed into two components, the first of which is what we call a signal, say \\((Y_t)\\), and the second component is a noise, say \\((W_t)\\), leading to the model \\[X_t = Y_t + W_t.\\] Typically, we have \\(E[Y_t] \\neq 0\\) while \\(E[W_t] = 0\\) (although we may have \\(E[W_t | W_{t-1}, ..., W_1] \\neq 0\\)). Such models impose some parametric structure which represents a convenient and flexible way of studying time series as well as a means to evalute future values of the series through forecasting. As we will see, predicting future values is one of the main aspects of time series analysis. However, making predictions is often a daunting task or as famously stated by Nils Bohr: “Prediction is very difficult, especially about the future.” There are plenty of examples of predictions that turned out to be completely erroneous. For example, three days before the 1929 crash, Irving Fisher, Professor of Economics at Yale University, famously predicted: “Stock prices have reached what looks like a permanently high plateau”. Another example is given by Thomas Watson, president of IBM, who said in 1943: “I think there is a world market for maybe five computers.” 1.2 Exploratory Data Analysis for Time Series When dealing with relatively small time series (e.g. a few thousands), it is often useful to look at a graph of the original data. These graphs can be informative to “detect” some features of a time series such as trends and the presence of outliers. Indeed, a trend is typically assumed to be present in a time series when the data exhibit some form of long term increase or decrease or combination of increases or decreases. Such trends could be linear or non-linear and represent an important part of the “signal” of a model. Here are a few examples of non-linear trends: Seasonal trends (periodic): These are the cyclical patterns which repeat after a fixed/regular time period. This could be due to business cycles (e.g. bust/recession, recovery). Non-seasonal trends (periodic): These patterns cannot be associated to seasonal variation and can for example be due to an external variable such as, for example, the impact of economic indicators on stock returns. Note that such trends are often hard to detect based on a graphical analysis of the data. “Other” trends: These trends have typically no regular patterns and are over a segment of time, known as a “window”, that change the statistical properties of a time series. A common example of such trends is given by the vibrations observed before, during and after an earthquake. Example: A traditional example of a time series is the quarterly earnings of the company Johnson and Johson. In the figure below, we present these earnings between 1960 and 1980: One trait that the graph makes evident is that the data contains a non-linear increasing trend as well as a yearly seasonal component. In addition, one can note that the variability of the data seems to increase with time. Being able to make such observations provides important information to select suitable models for the data. Moreover, when observing “raw” time series data it is also interesting to evaluate if some of the following phenomena occur: Change in Mean: Does the mean of the process shift over time? Change in Variance: Does the variance of the process evolve with time? Change in State: Does the time series appear to change between “states” having distinct statistical properties? Outliers Does the time series contain some “extreme” observations? Note that this is typically difficult to assess visually. Example: In the figure below, we present an example of displacement recorded during an earthquake as well as an explosion. From the graph, it can be observed that the statistical properties of the time series appear to change over time. For instance, the variance of the time series shifts at around \\(t = 1150\\) for both series. The shift in variance also opens “windows” where there appear to be distinct states. In the case of the explosion data, this is particularly relevant around \\(t = 50, \\cdots, 250\\) and then again from \\(t = 1200, \\cdots, 1500\\). Even within these windows, there are “spikes” that could be considered as outliers most notably around \\(t = 1200\\) for explosion series. Next, we consider an example coming from high-frequency finance to illustrate the limitations our current framework. Example: The figure below presents the returns or price innovation (i.e. informally speaking the changes in price from one observation to the other) for the Starbuck’s stock on July 1, 2011 for about 150 seconds (left panel) and about 400 minutes (right panel). # Load packages library(timeDate) # Load &quot;high-frequency&quot; Starbucks returns for Jul 01 2011 data(sbux.xts, package = &quot;highfrequency&quot;) # Plot returns par(mfrow = c(1,2)) plot(sbux.xts[1:89], main = &quot; &quot;, ylab = &quot;Returns&quot;) plot(sbux.xts, main = &quot; &quot;, ylab = &quot;Returns&quot;) It can be observed on the left panel that observations are not equally spaced. Indeed, in high-frequency data the intervals between two points is typically not constant and, even worse, is a random variable. This implies that the time when a new observation will be available is in general unknown. On the right panel, one can observe that the variability of the data seems to change during the course of the trading day. Such a phenomenon is well known in the finance community since a lot of variation occurs at the start (and the end) of the day while the middle of the day is associated with small changes. Moreover, clear extreme observations can also be noted in this graph at around 09:30:34. Finally, let us consider the limitations of a direct graphical representation of a time series when the sample size is large. Indeed, due to visual limitations, a direct plotting of the data will probably result in an uninformative aggregation of points between which it is unable to distinguish anything. As an example, let us take the data coming from the calibration procedure of an Inertial Measurement Unit (IMU) which, in general terms, is used to enhance navigation precision. The signals coming from these instruments are measured at high frequencies over a long time and are often characterized by linear trends and numerous underlying stochastic processes. The code below retrieves some data from an IMU and plots it directly: # Load packages library(gmwm) library(imudata) # Load IMU data data(imu6, package = &quot;imudata&quot;) Xt = gts(imu6[,1], name = &quot;Gyroscope data&quot;, unit = &quot;sec&quot;, freq = 100) # Plot gryoscope data autoplot(Xt) + ylab(&quot;Error (rad/s^2)&quot;) Although a linear trend and other processes are present in this signal (time series), it is practically impossible to understand or guess anything from the plot. 1.3 Basic Time Series Models In this section, we introduce some simple time series models. Before doing so it is useful to define \\(\\Omega_t\\) as all the information avaiable up to time \\(t-1\\), i.e. \\[\\Omega_t = \\left(X_{t-1}, X_{t-2}, ..., X_0 \\right).\\] As we will see this compact notation is quite useful. 1.3.1 White noise processes The building block for most time series models is the Gaussian white noise process, which can be defined as \\[{W_t}\\mathop \\sim \\limits^{iid} N\\left( {0,\\sigma _w^2} \\right).\\] This definition implies that: \\(E[W_t | \\Omega_t] = 0\\) for all \\(t\\), \\(\\cov\\left(W_t, W_{t-h} \\right) = \\boldsymbol{1}_{h = 0} \\; \\sigma^2\\) for all \\(t, h\\). Therefore, in this process there is an absence of temporal (or serial) dependence and is homoskedastic (i.e it has a constant variance). This definition can be generalized into two sorts of processes, the weak and strong white noise. The process \\((W_t)\\) is a weak white noise if \\(E[W_t] = 0\\) for all \\(t\\), \\(\\var\\left(W_t\\right) = \\sigma^2\\) for all \\(t\\), \\(\\cov \\left(W_t, W_{t-h}\\right) = 0\\), for all \\(t\\), and for all \\(h \\neq 0\\). Note that this definition does not imply that \\(W_t\\) and \\(W_{t-h}\\) are independent (for \\(h \\neq 0\\)) but simply uncorrelated. However, the notion of independence is used to define a strong white noise as \\(E[W_t] = 0\\) and \\(\\var(W_t) = \\sigma^2 &lt; \\infty\\), for all \\(t\\), \\(F(W_t) = F(W_{t-h})\\), for all \\(t,h\\) (where \\(F(W_t)\\) denotes the distribution of \\(W_t\\)), \\(W_t\\) and \\(W_{t-h}\\) are independent for all \\(t\\) and for all \\(h \\neq 0\\). It is clear from these definitions that if a process is a strong white noise it is also a weak white noise. However, the converse is not true as shown in the following example: Example: Let \\(X_t \\mathop \\sim \\limits^{iid} F_t\\), where \\(F_t\\) denotes a Student distribution with \\(t\\) degrees of freedom. This process is a weak white noise but not a strong one. The code below presents an example of how to simulate a Gaussian white noise process # This code simulates a gaussian white noise process n = 100 # process length sigma2 = 1 # process variance Xt = gen.gts(WN(sigma2 = sigma2), N = n) plot(Xt) 1.3.2 Random Walk Processes The term random walk was first introduce by Karl Pearson in the early 19 hundreds. As for the white noise, there exist a large range of random walk processes. For example, one of the simplest forms of random walk can be explained as follows: suppose that you are walking on campus and your next step can either be to your left, your right, forward or backward (each with equal probability). Two realizations of such processes are represented below: # Function computes direction random walk moves RW2dimension = function(steps = 100){ # Initial matrix step_direction = matrix(0, steps+1, 2) # Start random walk for (i in seq(2, steps+1)){ # Draw a random number from U(0,1) rn = runif(1) # Go right if rn \\in [0,0.25) if (rn &lt; 0.25) {step_direction[i,1] = 1} # Go left if rn \\in [0.25,0.5) if (rn &gt;= 0.25 &amp;&amp; rn &lt; 0.5) {step_direction[i,1] = -1} # Go forward if rn \\in [0.5,0.75) if (rn &gt;= 0.5 &amp;&amp; rn &lt; 0.75) {step_direction[i,2] = 1} # Go backward if rn \\in [0.75,1] if (rn &gt;= 0.75) {step_direction[i,2] = -1} } # Cumulative steps position = data.frame(x = cumsum(step_direction[, 1]), y = cumsum(step_direction[, 2])) # Mark start and stop locations start_stop = data.frame(x = c(0, position[steps+1, 1]), y = c(0, position[steps+1, 2]), type = factor(c(&quot;Start&quot;,&quot;End&quot;), levels = c(&quot;Start&quot;,&quot;End&quot;))) # Plot results ggplot(mapping = aes(x = x, y = y)) + geom_path(data = position) + # Mimics type = &#39;l&#39; geom_point(data = start_stop, aes(color = type), size = 4) + theme_bw() + labs( x = &quot;X-position&quot;, y = &quot;Y-position&quot;, title = paste(&quot;2D random walk with&quot;, steps, &quot;steps&quot;), color = &quot;&quot; ) + theme(legend.position = c(0.15, 0.55)) } # Plot 2D random walk with 10^2 and 10^5 steps set.seed(2) a = RW2dimension(steps = 10^2) b = RW2dimension(steps = 10^5) library(&quot;gridExtra&quot;) grid.arrange(a, b, nrow = 1) Such processes inspired Karl Pearson’s famous quote that “the most likely place to find a drunken walker is somewhere near his starting point.” Empirical evidence of this phenomenon is not too hard to find on a Friday night in Champaign. In this class, we only consider one very specific form of random walk, namely the Gaussian random walk which can be defined as: \\[X_t = X_{t-1} + W_t,\\] where \\(W_t\\) is a Gaussian white noise and with initial condition \\(X_0 = c\\) (typically \\(c = 0\\)). This process can be expressed differently by backsubstitution as follows: \\[\\begin{aligned} {X_t} &amp;= {X_{t - 1}} + {W_t} \\\\ &amp;= \\left( {{X_{t - 2}} + {W_{t - 1}}} \\right) + {W_t} \\\\ &amp;= \\vdots \\\\ {X_t} &amp;= \\sum\\limits_{i = 1}^t {{W_i}} + X_0 = \\sum\\limits_{i = 1}^t {{W_i}} + c \\\\ \\end{aligned} \\] The code below presents an example of how to simulate a such process # This code simulates a gaussian random walk process n = 1000 # process length gamma2 = 1 # innovation variance Xt = gen.gts(RW(gamma2 = gamma2), N = n) plot(Xt) 1.3.3 Autoregressive Process of Order 1 An autoregressive process of order 1 or AR(1) is a generalization of both the white noise and random walk processes which are both themselves special cases of an AR(1). A (Gaussian) AR(1) process can be defined as \\[{X_t} = {\\phi}{X_{t - 1}} + {W_t},\\] where \\(W_t\\) is a Gaussian white noise. Clearly, an AR(1) with \\(\\phi = 0\\) is a Gaussian white noise and when \\(\\phi = 1\\) the process becomes a random walk. Remark: We generally assume that an AR(1), as well as other time series models, have zero mean. The reason for this assumption is only to simplfy the notation but it is easy to consider an AR(1) process around an arbitrary mean \\(\\mu\\), i.e. \\[\\left(X_t - \\mu\\right) = \\phi \\left(X_{t-1} - \\mu \\right) + W_t,\\] which is of course equivalent to \\[X_t = \\left(1 - \\phi \\right) \\mu + \\phi X_{t-1} + W_t.\\] Thus, we will generally only work with zero mean processes since adding means is simple. Remark: An AR(1) is in fact a linear combination of the past realisations of the white noise \\(W_t\\). Indeed, we have \\[\\begin{aligned} {X_t} &amp;= {\\phi_t}{X_{t - 1}} + {W_t} = {\\phi}\\left( {{\\phi}{X_{t - 2}} + {W_{t - 1}}} \\right) + {W_t} \\\\ &amp;= \\phi^2{X_{t - 2}} + {\\phi}{W_{t - 1}} + {W_t} = {\\phi^t}{X_0} + \\sum\\limits_{i = 0}^{t - 1} {\\phi^i{W_{t - i}}}. \\end{aligned}\\] Under the assumption of infinite past (i.e. \\(t \\in \\mathbb{Z}\\)) and \\(|\\phi| &lt; 1\\), we obtain \\[X_t = \\sum\\limits_{i = 0}^{\\infty} {\\phi^i {W_{t - i}}},\\] since \\(\\operatorname{lim}_{i \\to \\infty} \\; {\\phi^i}{X_{t-i}} = 0\\). The code below presents an example of how an AR(1) can be simulated # This code simulate a gaussian random walk process n = 100 # process length phi = 0.5 # phi parameter sigma2 = 1 # innovation variance Xt = gen.gts(AR1(phi = phi, sigma2 = sigma2), N = n) plot(Xt) 1.3.4 Moving Average Process of Order 1 As we have seen in the previous example, an AR(1) can be expressed as a linear combination of all past observations of \\((W_t)\\) while the next process, called a moving average process of order 1 or MA(1), is (in some sense) a “truncated” version of an AR(1). It is defined as \\begin{equation} X_t = \\theta W_{t-1} + W_t, \\label{eq:defMA1} \\end{equation} where (again) \\(W_t\\) denotes a Gaussian white noise process. An example on how to generate an MA(1) is given below: # This code simulates a gaussian white noise process n = 100 # process length sigma2 = 1 # innovation variance theta = 0.5 # theta parameter Xt = gen.gts(MA1(theta = theta, sigma2 = sigma2), N = n) plot(Xt) 1.3.5 Linear Drift A linear drift is a very simple deterministic time series model which can be expressed as \\[X_t = X_{t+1} + \\omega, \\] where \\(\\omega\\) is a constant and with the initial condition \\(X_0 = c\\), an arbitrary constant (typically zero). This process can be expressed in a more familiar form as follows: \\[ {X_t} = {X_{t - 1}} + \\omega = \\left( {{X_{t - 2}} + \\omega} \\right) + \\omega = t{\\omega} + c \\] Therefore, a (linear) drift corresponds to a simple linear model with slope \\(\\omega\\) and intercept \\(c\\). A drift can simply be generated using the code below: # This code simulate a linear drift with 0 intercept n = 100 # process length omega = 0.5 # slope parameter Xt = gen.gts(DR(omega = omega), N = n) plot(Xt) 1.3.6 Composite Stochastic Processes A composite stochastic process can be defined as the sum of underlying (or latent) stochastic processes. In this text, we will use the term latent time series as a synomym for composite stochastic processes. A simple example of such a process is given by \\[\\begin{aligned} Y_t &amp;= Y_{t-1} + W_t + \\delta\\\\ X_t &amp;= Y_t + Z_t, \\end{aligned}\\] where \\(W_t\\) and \\(Z_t\\) are two independent Gaussian white noise processes. This model is often used as a first tool to approximate the number of individuals in the context ecological population dynamics. For example, suppose we want to study the population of Chamois in the Swiss Alps. Let \\(Y_t\\) denote the “true” number of individuals in this population at time \\(t\\). It is reasonable that \\(Y_t\\) is (approximately) the population at the previous time \\(t-1\\) (e.g the previous year) plus a random variation and a drift. This random variation is due to the natural randomness in ecological population dynamics and reflects the changes in the number of predators, in the aboundance of food or in the weather conditions. On the other hand, the drift is often of particular interest for ecologists as it can be used to determine the “long” term trends of the population (e.g. is the population increasing, stable or decreasing). Of course, \\(Y_t\\) (the number of individauls) is typically unknown and we observe a noisy version of it, denoted as \\(X_t\\). This process corresponds to the true population plus a measurement error since some individuals may not be observed while others may have been counted several times. Interestingly, this process can clearly be expressed as a latent time series model (or composite stochastic process) as follows: \\[\\begin{aligned} R_t &amp;= R_{t-1} + W_t \\\\ S_t &amp;= \\delta t \\\\ X_t &amp;= R_t + S_t + Z_t, \\end{aligned}\\] where \\(R_t\\), \\(S_t\\) and \\(Z_t\\) denote, respectively, a random walk, a drift and a white noise. The code below can be used to simulate such data: n = 1000 # process length delta = 0.005 # delta parameter (drift) sigma2 = 10 # variance parameter (white noise) gamma2 = 0.1 # innovation variance (random walk) model = WN(sigma2 = sigma2) + RW(gamma2 = gamma2) + DR(omega = delta) Xt = gen.lts(model, N = n) plot(Xt) In the above graph, the first three plots represent the latent (unobserved) processes (i.e. white noise, random walk and drift) and the last one represents the sum of the three (i.e. \\((X_t)\\)). "],
["autocorrelation-and-stationarity.html", "Chapter 2 Autocorrelation and Stationarity 2.1 The Autocorrelation and Autocovariance Functions 2.2 Stationarity 2.3 Estimation of the Mean Function 2.4 Sample Autocovariance and Autocorrelation Functions 2.5 Robustness Issues", " Chapter 2 Autocorrelation and Stationarity \"I have seen the future and it is very much like the present, only longer.\" > > --- Kehlog Albran, The Profit --> “One of the first things taught in introductory statistics textbooks is that correlation is not causation. It is also one of the first things forgotten.”, Thomas Sowell In this chapter we will discuss and formalize a little how knowledge about \\(X_{t-1}\\) (or \\(\\Omega_t\\)) can provide us with some information about \\(X_t\\). In particular, we will consisder the correlation (or covariance) of \\((X_t)\\) at different times such as \\(\\corr \\left(X_t, X_{t+h}\\right)\\). This “form” of correlation (covariance) is called the autocorrelation (autocovariance) and is a very usefull tool in time series analysis. Without assuming that a time series is characterized by a certain form of “stability”, it would be rather difficult to estimate \\(\\corr \\left(X_t, X_{t+h}\\right)\\) as this quantity would depend on both \\(t\\) and \\(h\\) leading to far more parameters to estimate than observations available. Therefore, the concept of stationarity is conveniant in this context as it allows (among other things) to assume that \\[\\corr \\left(X_t, X_{t+h}\\right) = \\corr \\left(X_t+j, X_{t+h+j}\\right),\\] implying that the autocorrelation (or autocovariance) is a function only of the lag between observations. These two concepts (autocorrelation and stationarity) will be discussed in this chapter. Before moving on, it is helpful to remind that correlation (or autocorrelation) is only approriate to measure a very spefic kind of dependence, i.e. linear dependence. There are many forms of dependence as illustrated in the bottom panels of the graph below, which all have a (true) zero correlation: dependency Note that several other metrics have been introduced in the litterature to assess the degree of “dependence” of two random variables but this goes beyond the material discussed in this text. 2.1 The Autocorrelation and Autocovariance Functions 2.1.1 Definitions The autocovariance function of a series \\((X_t)\\) is defined as \\[{\\gamma_x}\\left( {t,t+h} \\right) = \\operatorname{cov} \\left( {{x_t},{x_{t+h}}} \\right).\\] Since we generally consider stochastic processes with constant zero mean we often have \\[{\\gamma_x}\\left( {t,t+h} \\right) = E\\left[X_t X_{t+h} \\right]. \\] We normally drop the subscript referring to the time series if it is clear from the context which time series the autocovariance refers to. For example, we generally use \\({\\gamma}\\left( {t,t+h} \\right)\\) instead of \\({\\gamma_x}\\left( {t,t+h} \\right)\\). Moreover, the notation is even further simplified when the covariance of \\(X_t\\) and \\(X_{t+h}\\) is the same as that of \\(X_{t+j}\\) and \\(X_{t+h+j}\\) (for \\(j \\in \\mathbb{Z}\\)), i.e. the covariance depends only on the time between observations and not on the specific time \\(t\\). This is an important property called stationarity, which will be discuss in the next section. In this case, we simply use to following notation: \\[\\gamma \\left( {h} \\right) = \\operatorname{cov} \\left( X_t , X_{t+h} \\right). \\] A few other remarks: The covariance function is symmetric. That is, \\({\\gamma}\\left( {h} \\right) = {\\gamma}\\left( -h \\right)\\) since \\(\\operatorname{cov} \\left( {{X_t},{X_{t+h}}} \\right) = \\operatorname{cov} \\left( X_{t+h},X_{t} \\right)\\). Note that \\(\\var \\left( X_{t} \\right) = {\\gamma}\\left( 0 \\right)\\). We have that \\(|\\gamma(h)| \\leq \\gamma(0)\\) for all \\(h\\). The proof of this inequality follows from the Cauchy-Schwarz inequality, i.e. \\[ \\begin{aligned} \\left(|\\gamma(h)| \\right)^2 &amp;= \\gamma(h)^2 = \\left(E\\left[\\left(X_t - E[X_t] \\right)\\left(X_{t+h} - E[X_{t+h}] \\right)\\right]\\right)^2\\\\ &amp;\\leq E\\left[\\left(X_t - E[X_t] \\right)^2 \\right] E\\left[\\left(X_{t+h} - E[X_{t+h}] \\right)^2 \\right] = \\gamma(0)^2. \\end{aligned} \\] Just as any covariance, \\({\\gamma}\\left( {h} \\right)\\) is “scale dependent” since \\({\\gamma}\\left( {h} \\right) \\in \\real\\), or \\(-\\infty \\le {\\gamma}\\left( {h} \\right) \\le +\\infty\\). We therefore have: if \\(\\left| {\\gamma}\\left( {h} \\right) \\right|\\) is “close” to zero, then \\(X_t\\) and \\(X_{t+h}\\) are “weakly” (linearly) dependent; if \\(\\left| {\\gamma}\\left( {h} \\right) \\right|\\) is “far” from zero, then the two random variable present a “strong” (linear) dependence. However it is generally difficult to asses what “close” and “far” from zero means in this case. \\({\\gamma}\\left( {h} \\right)=0\\) does not imply that \\(X_t\\) and \\(X_{t+h}\\) are independent. This is only true in the jointly Gaussian case. An important related statistic is the correlation of \\(X_t\\) with \\(X_{t+h}\\) or autocorrelation which is defined as \\[\\rho \\left( h \\right) = \\operatorname{corr}\\left( {{X_t},{X_{t + h}}} \\right) = \\frac{\\gamma(h) }{\\gamma(0)}.\\] Similarly to \\(\\gamma(h)\\), it is important to note that the above notation implies that the autocorrelation function is only a function of the lag \\(h\\) between observations. Thus, autocovariances and autocorrelations are one possible way to describe the joint distribution of a time series. Indeed, the correlation of \\(X_t\\) with \\(X_{t+1}\\) is an obvious measure of how persistent a time series is. Remember that just as with any correlation: \\(\\rho \\left( h \\right)\\) is “scale free” so it is much easier to interpret than \\(\\gamma(h)\\). \\(|\\rho \\left( h \\right)| \\leq 1\\) since \\(|\\gamma(h)| \\leq \\gamma(0)\\). Causation and correlation are two very different things! 2.1.2 A Fundamental Representation Autocovariances and autocorrelations also turn out to be a very useful tool because they are one of the fundamental representations of time series. Indeed, if we consider a zero mean normally distrbuted process, it is clear that its joint distribution is fully characterized by the autocariances \\(E[X_t X_{t+h}]\\) (since the joint probability density only depends of these covariances). Once we know the autocovariances we know everything there is to know about the process and therefore: if two processes have the same autocovariance function, then they are the same process. 2.1.3 Admissible autocorrelation functions Since the autocorrelation is related to a fundamental representation of time series, it implies that one might be able to define a stochastic process by picking a set of autocorrelation values. However, it turns out that not every collection of numbers, say \\(\\{\\rho_1, \\rho_2, ...\\}\\), can represent the autocorrelation of a process. Indeed, two conditions are required to ensure the validity of an autocorrelation sequence: \\(\\operatorname{max}_j \\; | \\rho_j| \\leq 1\\). \\(\\var \\left[\\sum_{j = 0}^\\infty \\alpha_j X_{t-j} \\right] \\geq 0\\) for all \\(\\{\\alpha_0, \\alpha_1, ...\\}\\). The first condition is obvious and simply reflects the fact that \\(|\\rho \\left( h \\right)| \\leq 1\\) but the second is more difficult to verify. Let \\(\\alpha_j = 0\\) for \\(j &gt; 1\\), then conditon 2 implies that \\[\\var \\left[ \\alpha_0 X_{t} + \\alpha_1 X_{t-1} \\right] = \\gamma_0 \\begin{bmatrix} \\alpha_0 &amp; \\alpha_1 \\end{bmatrix} \\begin{bmatrix} 1 &amp; \\rho_1\\\\ \\rho_1 &amp; 1 \\end{bmatrix} \\begin{bmatrix} \\alpha_0 \\\\ \\alpha_1 \\end{bmatrix} \\geq 0. \\] Thus, the matrix \\[ \\boldsymbol{A}_1 = \\begin{bmatrix} 1 &amp; \\rho_1\\\\ \\rho_1 &amp; 1 \\end{bmatrix} \\] must be positive semi-definite. Taking the determinant we have \\[\\operatorname{det} \\left(\\boldsymbol{A}_1\\right) = 1 - \\rho_1^2 \\] implying that the condition \\(|\\rho_1| &lt; 1\\) must be respected. Now, let \\(\\alpha_j = 0\\) for \\(j &gt; 2\\), then we must verify that: \\[\\var \\left[ \\alpha_0 X_{t} + \\alpha_1 X_{t-1} + \\alpha_2 X_{t-2} \\right] = \\gamma_0 \\begin{bmatrix} \\alpha_0 &amp; \\alpha_1 &amp;\\alpha_2 \\end{bmatrix} \\begin{bmatrix} 1 &amp; \\rho_1 &amp; \\rho_2\\\\ \\rho_1 &amp; 1 &amp; \\rho_1 \\\\ \\rho_2 &amp; \\rho_1 &amp; 1 \\end{bmatrix} \\begin{bmatrix} \\alpha_0 \\\\ \\alpha_1 \\\\ \\alpha_2 \\end{bmatrix} \\geq 0. \\] Again, this implies that the matrix \\[ \\boldsymbol{A}_2 = \\begin{bmatrix} 1 &amp; \\rho_1 &amp; \\rho_2\\\\ \\rho_1 &amp; 1 &amp; \\rho_1 \\\\ \\rho_2 &amp; \\rho_1 &amp; 1 \\end{bmatrix} \\] must be positive semi-definite. It is easy to verify that \\[\\operatorname{det} \\left(\\boldsymbol{A}_2\\right) = \\left(1 - \\rho_2 \\right)\\left(- 2 \\rho_1^2 + \\rho_2 + 1\\right). \\] It implies that \\(|\\rho_2| &lt; 1\\) as well as \\[\\begin{aligned} &amp;- 2 \\rho_1^2 + \\rho_2 + 1 \\geq 0 \\Rightarrow 1 &gt; \\rho_2 \\geq 2 \\rho_1^2 - 1 \\\\ &amp;\\Rightarrow 1 - \\rho_1^2 &gt; \\rho_2 - \\rho_1^2 \\geq -(1 - \\rho_1^2)\\\\ &amp;\\Rightarrow 1 &gt; \\frac{\\rho_2 - \\rho_1^2 }{1 - \\rho_1^2} \\geq -1, \\end{aligned}\\] imlying that \\(\\rho_1\\) and \\(\\rho_2\\) must lie in a parabolic shaped region defined by the above inequalities as illustrated in the figure below: plot(NA, xlim = c(-1.1,1.1), ylim = c(-1.1,1.1), xlab = expression(rho[1]), ylab = expression(rho[2]), cex.lab = 1.5) grid() # Adding boundary of constraint |rho_1| &lt; 1 abline(v = c(-1,1), lty = 2, col = &quot;darkgrey&quot;) # Adding boundary of constraint |rho_2| &lt; 1 abline(h = c(-1,1), lty = 2, col = &quot;darkgrey&quot;) # Adding boundary of non-linear constraint rho1 = seq(from = -1, to = 1, length.out = 10^3) rho2 = (rho1^2 - 1) + rho1^2 lines(rho1, rho2, lty = 2, col = &quot;darkgrey&quot;) # Adding admissible region polygon(c(rho1,rev(rho1)),c(rho2,rep(1,10^3)), border = NA, col= rgb(0,0,1, alpha = 0.1)) # Adding text text(0,0, c(&quot;Admissible Region&quot;)) Therefore, the restrictions on the autocorrelation are very complicated thereby justifying the need for other forms of fundamental representation which will explore later in this text. Before moving on to the estimation of the autocorrelation and autocovariance functions, we first discuss the stationarity of \\((X_t)\\), which will provide a convenient framework in which \\(\\gamma(h)\\) and \\(\\rho(h)\\) can be used (rather that \\(\\gamma(t,t+h)\\) for example). 2.2 Stationarity 2.2.1 Definitions There are two kinds of stationarity that are commonly used. They are defined below: A process \\((X_t)\\) is strongly stationary or strictly stationary if the joint probability distribution of \\(\\{X_{t-h}, ..., X_t, ..., X_{t+h}\\}\\) is independent of \\(t\\) for all \\(h\\). A process \\((X_t)\\) is weakly stationary, covariance stationary or second order stationary if \\(E[X_t]\\), \\(E[X_t^2]\\) are finite and \\(E[X_t X_{t-h}]\\) depends only on \\(h\\) and not on \\(t\\). These types of stationarity are not equivalent and the presence of one kind of stationarity does not imply the other. That is, a time series can be strongly stationary but not weakly stationary and vice versa. In some cases, a time series can be both strongly and weakly stationary and this is occurs, for example, in the (jointly) Gaussian case. Stationarity of \\((X_t)\\) matters because it provides the framework in which averaging dependent data makes sense thereby allowing to easily estimate quantities such as the autocorrelation function. A few remarks: As mentioned earlier, strong stationarity does not imply weak stationarity. Example: an iid Cauchy process is strongly but not weakly stationary. Weak stationarity does not imply strong stationarity. Example: Consider the following weak white noise process: \\(X_{2t} = U_{2t}, X_{2t+1} = V_{2t+1}\\), for \\(t = 1,..., n\\) where \\({U_t}\\mathop \\sim \\limits^{iid} N\\left( {1,1} \\right)\\) and \\({V_t}\\mathop \\sim \\limits^{iid} Exponential\\left( 1 \\right)\\) is weakly stationary but not strongly stationary. Strong stationarity combined with bounded values of \\(E[X_t]\\) and \\(E[X_t^2]\\) implies weak stationarity Weak stationarity combined with normalityof the process implies strong stationarity. 2.2.2 Assessing Weak Stationarity of Time Series Models It is important to understand how to verify if a postulated model is (weakly) stationary. In order to do so, we must ensure that the model satisfies three properties, i.e. \\(E\\left[X_t \\right] = \\mu_t = \\mu &lt; \\infty\\), \\(\\var\\left[X_t \\right] = \\sigma^2_t = \\sigma^2 &lt; \\infty\\), \\(\\operatorname{cov}\\left(X_t, X_{t+h} \\right) = \\gamma \\left(h\\right)\\). In the following examples we evaluate the stationarity of the processes introduced in Section 1.3. Example: Gaussian White Noise It is easy to verify that this process is stationary. Indeed, we have: \\(E\\left[ {{X_t}} \\right] = 0\\), \\(\\gamma(0) = \\sigma^2 &lt; \\infty\\), \\(\\gamma(h) = 0\\) for \\(|h| &gt; 0\\). Example: Random Walk To evaluate the stationarity of this process we first derive its properties: \\[\\begin{aligned} E\\left[ {{X_t}} \\right] &amp;= E\\left[ {{X_{t - 1}} + {W_t}} \\right] = E\\left[ {\\sum\\limits_{i = 1}^t {{W_t}} + {X_0}} \\right] \\\\ &amp;= E\\left[ {\\sum\\limits_{i = 1}^t {{W_t}} } \\right] + {c} = c \\\\ \\end{aligned} \\] Note that the mean here is constant since it depends only on the value of the first term in the sequence. \\[\\begin{aligned} \\var\\left( {{X_t}} \\right) &amp;= \\var\\left( {\\sum\\limits_{i = 1}^t {{W_t}} + {X_0}} \\right) = \\var\\left( {\\sum\\limits_{i = 1}^t {{w_t}} } \\right) + \\underbrace {\\var\\left( {{X_0}} \\right)}_{= 0} \\\\ &amp;= \\sum\\limits_{i = 1}^t {Var\\left( {{w_t}} \\right)} = t \\sigma_w^2. \\end{aligned}\\] where \\(\\sigma_w^2 = \\var(W_t)\\). Therefore, the variance depends on time (\\(t\\)) contradicting our second property. Moreover, we have: \\[\\mathop {\\lim }\\limits_{t \\to \\infty } \\; \\var\\left(X_t\\right) = \\infty.\\] This process is therefore not weakly stationary. Rgearding the autovariance of a random walk we have: \\[\\begin{aligned} \\gamma \\left( h \\right) &amp;= Cov\\left( {{X_t},{X_{t + h}}} \\right) = Cov\\left( {\\sum\\limits_{i = 1}^t {{W_i}} ,\\sum\\limits_{j = 1}^{t + h} {{W_j}} } \\right) \\\\ &amp;= Cov\\left( {\\sum\\limits_{i = 1}^t {{W_i}} ,\\sum\\limits_{j = 1}^t {{W_j}} } \\right) = \\min \\left( {t,t + h} \\right)\\sigma _w^2 \\\\ &amp;= \\left( {t + \\min \\left( {0,h} \\right)} \\right)\\sigma _w^2, \\end{aligned} \\] which further illustrates that this process is not weakly stationary. Moreover, the autocorrelation of this process is given by \\[\\rho (h) = \\frac{t + \\min \\left( {0,h} \\right)}{\\sqrt{t}\\sqrt{t+h}},\\] implying (for a fixed \\(h\\)) that \\[\\mathop {\\lim }\\limits_{t \\to \\infty } \\; \\rho(h) = 1.\\] In the following simulated example, we illustrate the non-stationary feature of such a process: # In this example, we simulate a large number of random walks # Number of simulated processes B = 200 # Length of random walks n = 1000 # Output matrix out = matrix(NA,B,n) for (i in 1:B){ # Simulate random walk Xt = cumsum(rnorm(n)) # Store process out[i,] = Xt } # Plot random walks plot(NA, xlim = c(1,n), ylim = range(out), xlab = &quot;Time&quot;, ylab = &quot; &quot;) color = sample(topo.colors(B, alpha = 0.5)) for (i in 1:B){ lines(out[i,], col = color[i]) } # Add 95% confidence region lines(1:n, 1.96*sqrt(1:n), col = 2, lwd = 2, lty = 2) lines(1:n, -1.96*sqrt(1:n), col = 2, lwd = 2, lty = 2) In the plot, two hundred simulated random walks are plotted along with the theoretical 95% confidence intervals (red-dashed lines). The relationship between time and variance can clearly be observed (i.e. the variance of the process increases with the time). Example: MA(1) Similarly to our previous examples, we attempt to verify the stationary properties for the MA(1) model defined in \\ref{eq:defMA1} REF NOT WORKING: \\[ E\\left[ {{X_t}} \\right] = E\\left[ {{\\theta_1}{W_{t - 1}} + {W_t}} \\right] = {\\theta_1}E\\left[ {{W_{t - 1}}} \\right] + E\\left[ {{W_t}} \\right] = 0. \\] \\[\\var \\left( {{X_t}} \\right) = \\theta_1^2 \\var \\left( W_{t - 1}\\right) + \\var \\left( W_{t}\\right) = \\left(1 + \\theta^2 \\right) \\sigma^2_w.\\] \\[\\begin{aligned} Cov\\left( {{X_t},{X_{t + h}}} \\right) &amp;= E\\left[ {\\left( {{X_t} - E\\left[ {{X_t}} \\right]} \\right)\\left( {{X_{t + h}} - E\\left[ {{X_{t + h}}} \\right]} \\right)} \\right] = E\\left[ {{X_t}{X_{t + h}}} \\right] \\\\ &amp;= E\\left[ {\\left( {{\\theta}{W_{t - 1}} + {W_t}} \\right)\\left( {{\\theta }{W_{t + h - 1}} + {W_{t + h}}} \\right)} \\right] \\\\ &amp;= E\\left[ {\\theta^2{W_{t - 1}}{W_{t + h - 1}} + \\theta {W_t}{W_{t + h}} + {\\theta}{W_{t - 1}}{W_{t + h}} + {W_t}{W_{t + h}}} \\right]. \\\\ \\end{aligned} \\] It is easy to see that \\(E\\left[ {{W_t}{W_{t + h}}} \\right] = {\\boldsymbol{1}_{\\left\\{ {h = 0} \\right\\}}}\\sigma _w^2\\) and therefore, we obtain \\[\\cov \\left( {{X_t},{X_{t + h}}} \\right) = \\left( {\\theta^2{ \\boldsymbol{1}_{\\left\\{ {h = 0} \\right\\}}} + {\\theta}{\\boldsymbol{1}_{\\left\\{ {h = 1} \\right\\}}} + {\\theta}{\\boldsymbol{1}_{\\left\\{ {h = - 1} \\right\\}}} + {\\boldsymbol{1}_{\\left\\{ {h = 0} \\right\\}}}} \\right)\\sigma _w^2\\] implying the following autocovariance function: \\[\\gamma \\left( h \\right) = \\left\\{ {\\begin{array}{*{20}{c}} {\\left( {\\theta^2 + 1} \\right)\\sigma _w^2}&amp;{h = 0} \\\\ {{\\theta}\\sigma _w^2}&amp;{\\left| h \\right| = 1} \\\\ 0&amp;{\\left| h \\right| &gt; 1} \\end{array}} \\right. .\\] Therefore, an MA(1) process is weakly stationary since both the mean and variance are constant over time and its covariance function is only a function of the lag \\(h\\). Finally, we can easily obtain the autocorrelation for this process, which is given by \\[\\Rightarrow \\rho \\left( h \\right) = \\left\\{ {\\begin{array}{*{20}{c}} 1&amp;{h = 0} \\\\ {\\frac{{{\\theta}\\sigma _w^2}}{{\\left( {\\theta^2 + 1} \\right)\\sigma _w^2}} = \\frac{{{\\theta}}}{{\\theta^2 + 1}}}&amp;{\\left| h \\right| = 1} \\\\ 0&amp;{\\left| h \\right| &gt; 1} \\end{array}} \\right.\\] Interestingly, we can note that \\(|\\rho(1)| \\leq 0.5\\). Example AR(1) JAMES TO DO - USE MA(1) AS EXAMPLE, ADD DETAILS FROM HOMEWORK, CHANGE \\(\\phi_1\\) in \\(\\phi\\) and add ref to chap 1. Thanks Consider the AR(1) process given by: \\[{y_t} = {\\phi _1}{y_{t - 1}} + {w_t} \\text{, where } {w_t}\\mathop \\sim \\limits^{iid} WN\\left( {0,\\sigma _w^2} \\right)\\] It can be shown that this process simplifies to: \\[y_t = {\\phi ^t}{y_0} + \\sum\\limits_{i = 0}^{t - 1} {\\phi _1^i{w_{t - i}}}.\\] In addition, we add the requirement that \\(\\left| \\phi _1 \\right| &lt; 1\\). This requirement allows the process to be stationary. If \\(\\phi _1 \\ge 1\\), the process would not converge. In this way the process can represented as a geometric series that converges: \\[\\sum\\limits_{k = 0}^\\infty {{r^k}} = \\frac{1}{{1 - r}},{\\text{ }}\\left| r \\right| &lt; 1.\\] Next, we demonstrate how crucial this property is: \\[\\begin{aligned} \\mathop {\\lim }\\limits_{t \\to \\infty } E\\left[ {{y_t}} \\right] &amp;= \\mathop {\\lim }\\limits_{t \\to \\infty } E\\left[ {{\\phi ^t}{y_0} + \\sum\\limits_{i = 0}^{t - 1} {\\phi _1^i{w_{t - i}}} } \\right] \\\\ &amp;= \\mathop {\\lim }\\limits_{t \\to \\infty } \\underbrace {{\\phi ^t}{y_0}}_{= 0 \\, \\text{since} \\, \\left| \\phi \\right| &lt; 1 \\, \\text{and} \\, t \\to \\infty} + \\sum\\limits_{i = 0}^{t - 1} {\\phi _1^i\\underbrace {E\\left[ {{w_{t - i}}} \\right]}_{ = 0}} \\\\ &amp;= 0 \\\\ \\mathop {\\lim }\\limits_{t \\to \\infty } Var\\left( {{y_t}} \\right) &amp;= \\mathop {\\lim }\\limits_{t \\to \\infty } Var\\left( {{\\phi ^t}{y_0} + \\sum\\limits_{i = 0}^{t - 1} {\\phi _1^i{w_{t - i}}} } \\right) \\\\ &amp;= \\mathop {\\lim }\\limits_{t \\to \\infty } \\underbrace {Var\\left( {{\\phi ^t}{y_0}} \\right)}_{ = 0{\\text{ since constant}}} + Var\\left( {\\sum\\limits_{i = 0}^{t - 1} {\\phi _1^i{w_{t - i}}} } \\right) \\\\ &amp;= \\mathop {\\lim }\\limits_{t \\to \\infty } \\sum\\limits_{i = 0}^{t - 1} {\\phi _1^{2i}Var\\left( {{w_{t - i}}} \\right)} \\\\ &amp;= \\mathop {\\lim }\\limits_{t \\to \\infty } \\sigma _w^2\\sum\\limits_{i = 0}^{t - 1} {\\phi _1^{2i}} \\\\ &amp;= \\sigma _w^2 \\cdot \\underbrace {\\frac{1}{{1 - {\\phi ^2}}}.}_{\\begin{subarray}{l} {\\text{Geometric Series}} \\end{subarray}} \\end{aligned} \\] This leads us to conclude that the autocovariance function is: \\[\\begin{aligned} Cov\\left( {{y_t},{y_{t + h}}} \\right) &amp;= Cov\\left( {{y_t},\\phi {y_{t + h - 1}} + {w_{t + h}}} \\right) \\\\ &amp;= Cov\\left( {{y_t},\\phi {y_{t + h - 1}}} \\right) \\\\ &amp;= Cov\\left( {{y_t},{\\phi ^{\\left| h \\right|}}{y_t}} \\right) \\\\ &amp;= {\\phi ^{\\left| h \\right|}}Cov\\left( {{y_t},{y_t}} \\right) \\\\ &amp;= {\\phi ^{\\left| h \\right|}}Var\\left( {{y_t}} \\right) \\\\ &amp;= {\\phi ^{\\left| h \\right|}}\\frac{{\\sigma _w^2}}{{1 - \\phi _1^2}}. \\\\ \\end{aligned} \\] Both the mean and autocovariance function do not depend on time and, thus, the AR(1) process is stationary if \\(\\left| \\phi _1 \\right| &lt; 1\\). If we assume that the AR(1) process is stationary, we can derive the mean and variance in another way. Without loss of generality, we’ll assume that \\(y_0 = 0\\). Therefore: \\[\\begin{aligned} {y_t} &amp;= {\\phi _t}{y_{t - 1}} + {w_t} \\\\ &amp;= {\\phi _1}\\left( {{\\phi _1}{y_{t - 2}} + {w_{t - 1}}} \\right) + {w_t} \\\\ &amp;= \\phi _1^2{y_{t - 2}} + {\\phi _1}{w_{t - 1}} + {w_t} \\\\ &amp;\\vdots \\\\ &amp;= \\sum\\limits_{i = 0}^{t - 1} {\\phi _1^i{w_{t - i}}} \\\\ &amp; \\\\ E\\left[ {{y_t}} \\right] &amp;= E\\left[ {\\sum\\limits_{i = 0}^{t - 1} {\\phi _1^i{w_{t - i}}} } \\right] \\\\ &amp;= \\sum\\limits_{i = 0}^{t - 1} {\\phi _1^i\\underbrace {E\\left[ {{w_{t - i}}} \\right]}_{ = 0}} \\\\ &amp;= 0 \\\\ &amp;\\\\ Var\\left( {{y_t}} \\right) &amp;= E\\left[ {{{\\left( {{y_t} - E\\left[ {{y_t}} \\right]} \\right)}^2}} \\right] \\\\ &amp;= E\\left[ {y_t^2} \\right] - {\\left( {E\\left[ {{y_t}} \\right]} \\right)^2} \\\\ &amp;= E\\left[ {y_t^2} \\right] \\\\ &amp;= E\\left[ {{{\\left( {{\\phi _1}{y_{t - 1}} + {w_t}} \\right)}^2}} \\right] \\\\ &amp;= E\\left[ {\\phi _1^2y_{t - 1}^2 + w_t^2 + 2{\\phi _1}{y_t}{w_t}} \\right] \\\\ &amp;= \\phi _1^2E\\left[ {y_{t - 1}^2} \\right] + \\underbrace {E\\left[ {w_t^2} \\right]}_{ = \\sigma _w^2} + 2{\\phi _1}\\underbrace {E\\left[ {{y_t}} \\right]}_{ = 0}\\underbrace {E\\left[ {{w_t}} \\right]}_{ = 0} \\\\ &amp;= \\underbrace {\\phi _1^2Var\\left( {{y_{t - 1}}} \\right) + \\sigma _w^2 = \\phi _1^2Var\\left( {{y_t}} \\right) + \\sigma _w^2}_{{\\text{Assume stationarity}}} \\\\ Var\\left( {{y_t}} \\right) &amp;= \\phi _1^2Var\\left( {{y_t}} \\right) + \\sigma _w^2 \\\\ Var\\left( {{y_t}} \\right) - \\phi _1^2Var\\left( {{y_t}} \\right) &amp;= \\sigma _w^2 \\\\ Var\\left( {{y_t}} \\right)\\left( {1 - \\phi _1^2} \\right) &amp;= \\sigma _w^2 \\\\ Var\\left( {{y_t}} \\right) &amp;= \\frac{{\\sigma _w^2}}{{1 - \\phi _1^2}}. \\\\ \\end{aligned} \\] 2.3 Estimation of the Mean Function If a time series is stationary, the mean function is constant and a possible estimator of this quantity is given by \\[\\bar{X} = {\\frac{1}{n}\\sum\\limits_{t = 1}^n {{X_t}} }.\\] This estimator is clearly unbiased and has the following variance: \\[\\begin{aligned} \\var \\left( {\\bar X} \\right) &amp;= \\var \\left( {\\frac{1}{n}\\sum\\limits_{t = 1}^n {{X_t}} } \\right) \\\\ &amp;= \\frac{1}{{{n^2}}}\\var \\left( {{{\\left[ {\\begin{array}{*{20}{c}} 1&amp; \\cdots &amp;1 \\end{array}} \\right]}_{1 \\times n}}{{\\left[ {\\begin{array}{*{20}{c}} {{X_1}} \\\\ \\vdots \\\\ {{X_n}} \\end{array}} \\right]}_{n \\times 1}}} \\right) \\\\ &amp;= \\frac{1}{{{n^2}}}{\\left[ {\\begin{array}{*{20}{c}} 1&amp; \\cdots &amp;1 \\end{array}} \\right]_{1 \\times n}}\\left[ {\\begin{array}{*{20}{c}} {\\gamma \\left( 0 \\right)}&amp;{\\gamma \\left( 1 \\right)}&amp; \\cdots &amp;{\\gamma \\left( {n - 1} \\right)} \\\\ {\\gamma \\left( 1 \\right)}&amp;{\\gamma \\left( 0 \\right)}&amp;{}&amp; \\vdots \\\\ \\vdots &amp;{}&amp; \\ddots &amp; \\vdots \\\\ {\\gamma \\left( {n - 1} \\right)}&amp; \\cdots &amp; \\cdots &amp;{\\gamma \\left( 0 \\right)} \\end{array}} \\right]{\\left[ {\\begin{array}{*{20}{c}} 1 \\\\ \\vdots \\\\ 1 \\end{array}} \\right]_{n \\times 1}} \\\\ &amp;= \\frac{1}{{{n^2}}}\\left( {n\\gamma \\left( 0 \\right) + 2\\left( {n - 1} \\right)\\gamma \\left( 1 \\right) + 2\\left( {n - 2} \\right)\\gamma \\left( 2 \\right) + \\cdots + 2\\gamma \\left( {n - 1} \\right)} \\right) \\\\ &amp;= \\frac{1}{n}\\sum\\limits_{h = - n}^n {\\left( {1 - \\frac{{\\left| h \\right|}}{n}} \\right)\\gamma \\left( h \\right)} \\\\ \\end{aligned}. \\] In the white noise case, the above formula reduces to the usual \\(\\var \\left( {\\bar X} \\right) = \\var(X_t)/n\\). Example: For an AR(1) we have \\(\\gamma(h) = \\phi^h \\sigma_w^2 \\left(1 - \\phi^2\\right)^2\\), therefore, we obtain (after some computations): \\[ \\var \\left( {\\bar X} \\right) = \\frac{\\sigma_w^2 \\left( n - 2\\phi - n \\phi^2 + 2 \\phi^{n + 1}\\right)}{n^2\\left(1-\\phi^2\\right)\\left(1-\\phi\\right)^2}.\\] Unfortunately, deriving such an exact formula is often difficult when considering more complex models. However, asymptotic approximations are often employed to simplify the calculation. For example, in our case we have \\[\\mathop {\\lim }\\limits_{n \\to \\infty } \\; n \\var \\left( {\\bar X} \\right) = \\frac{\\sigma_w^2}{\\left(1-\\phi\\right)^2},\\] providing the following approximate formula: \\[\\var \\left( {\\bar X} \\right) \\approx \\frac{\\sigma_w^2}{n \\left(1-\\phi\\right)^2}.\\] Alternatively, simulation methods can also be employed. The figure generated by the following code compares these three methods: # Define sample size n = 10 # Number of Monte-Carlo replications B = 5000 # Define grid of values for phi phi = seq(from = 0.95, to = -0.95, length.out = 30) # Define result matrix result = matrix(NA,B,length(phi)) # Start simulation for (i in 1:length(phi)){ # Define model model = AR1(phi = phi[i], sigma2 = 1) # Monte-Carlo for (j in 1:B){ # Simulate AR(1) Xt = gen.gts(model, N = n) # Estimate Xbar result[j,i] = mean(Xt) } } # Estimate variance of Xbar var.Xbar = apply(result,2,var) # Compute theoretical variance var.theo = (n - 2*phi - n*phi^2 + 2*phi^(n+1))/(n^2*(1-phi^2)*(1-phi)^2) # Compute (approximate) vairance var.approx = 1/(n*(1-phi)^2) # Compare variance estimations plot(NA, xlim = c(-1,1), ylim = range(var.approx), log = &quot;y&quot;, ylab = expression(paste(&quot;var(&quot;, bar(X), &quot;)&quot;)), xlab= expression(phi), cex.lab = 1.5) grid() lines(phi,var.theo, col = &quot;deepskyblue4&quot;) lines(phi, var.Xbar, col = &quot;firebrick3&quot;) lines(phi,var.approx, col = &quot;springgreen4&quot;) legend(&quot;topleft&quot;,c(&quot;Theoretical variance&quot;,&quot;Estimated variance&quot;,&quot;Approximate variance&quot;), col = c(&quot;deepskyblue4&quot;,&quot;firebrick3&quot;,&quot;springgreen4&quot;), lty = 1, bty = &quot;n&quot;,bg = &quot;white&quot;, box.col = &quot;white&quot;, cex = 1.2) 2.4 Sample Autocovariance and Autocorrelation Functions A natural estimator of the autocovariance function is given by: \\[\\hat \\gamma \\left( h \\right) = \\frac{1}{T}\\sum\\limits_{t = 1}^{T - h} {\\left( {{X_t} - \\bar X} \\right)\\left( {{X_{t + h}} - \\bar X} \\right)} \\] leading the following “plug-in” estimator of the autocorrelation function \\[\\hat \\rho \\left( h \\right) = \\frac{{\\hat \\gamma \\left( h \\right)}}{{\\hat \\gamma \\left( 0 \\right)}}.\\] A graphical representation of the autocorrelation function is often the first step for any time series analysis (assuming the process to be stationary). Consider the following simulated example: # Load package library(&quot;gmwm&quot;) # Simulate 100 observation from a Gaussian white noise Xt = gen.gts(WN(sigma2 = 1), N = 100) # Compute autocorrelation acf_Xt = ACF(Xt) # Plot autocorrelation plot(acf_Xt, show.ci = FALSE) In this example, the true autocorrelation is equal to zero at any lag \\(h \\neq 0\\) but obviously the estimated autocorrelations are random variables and are not equal to their true values. It would therefore be usefull to have some knowledge about the variability of the sample autocorrelations (under some conditions) to assess whether the data comes from a completely random series or presents some significant correlation at some lags. The following result provides an asymptotic solution to this problem: Theorem: If \\(X_t\\) is white noise with finite fourth moment, then \\(\\hat{\\rho}(h)\\) is approximately normally distributed with mean \\(0\\) and variance \\(n^{-1}\\) for all fixed \\(h\\). The proof of this Theorem is given in Appendix ??. Using this result, we now have an approximate method to assess whether peaks in the sample autocorrelation are significant by determining whether the observed peak lies outside the interval \\(\\pm 2/\\sqrt{T}\\) (i.e. an approximate 95% confidence interval). Returning to our previous example: # Plot autocorrelation with confidence bands plot(acf_Xt) It can now be observed that most peaks lie within the interval \\(\\pm 2/\\sqrt{T}\\) suggesting that the true data generating process is completely random (in the linear sense). Unfortunately, this method is asymptotic (it relies on the central limit theorem) and there are no “exact” tools that can be used in this case. In the simulation study below we consider the “quality” of this result for \\(h = 3\\) considering different sample sizes: # Number of Monte Carlo replications B = 10000 # Define considered lag h = 3 # Sample size considered T = c(5,10,30,300) # Initialisation result = matrix(NA,B,length(T)) # Set seed set.seed(1) # Start Monte Carlo for (i in 1:B){ for (j in 1:length(T)){ # Simluate process Xt = rnorm(T[j]) # Save autocorrelation at lag h result[i,j] = acf(Xt, plot = FALSE)$acf[h+1] } } # Plot results par(mfrow = c(1,length(T))) for (i in 1:length(T)){ # Estimated empirical distribution hist(result[,i], col = &quot;lightgrey&quot;, main = paste(&quot;Sample size T =&quot;,T[i]), probability = TRUE, xlim = c(-1,1), xlab = &quot; &quot;) # Asymptotic distribution xx = seq(from = -10, to = 10, length.out = 10^3) yy = dnorm(xx,0,1/sqrt(T[i])) lines(xx,yy, col = &quot;red&quot;) } It can clearly be observed that the asymptotic approximation is quite poor when \\(T = 5\\) but as the sample size increases the approximation improves and is very close when, for example, \\(T = 300\\). 2.5 Robustness Issues The data generating process delivers a theoretical autocorrelation (autocovariance) function that, as explained in the previous section, can then be estimated through the sample autocorrelation (autocovariance) functions. However, in practice, the sample is often issued from a data generating process that is “close” to the true one, meaning that the sample suffers from some form of small contamination. This contamination is typically represented by a small amount of extreme observations that are called “outliers” that come from a process that is different from the true data generating process. The fact that the sample can suffer from outliers implies that the standard estimation of the autocorrelation (autocovariance) functions through the sample functions can be highly biased. The standard estimators presented in the previous section are therefore not “robust” and can behave badly when the sample suffers from contamination. The following simulated example highlights how the performance of these estimators can deteriorate if the sample is contaminated: # Define sample size n = 100 # Define proportion of &quot;extreme&quot; observation alpha = 0.05 # Extreme observation are generated from N(0,sigma2.cont) sigma2.cont = 10 # Number of Monte-Carlo replications B = 1000 # Define model AR(1) phi = 0.5 sigma2 = 1 model = AR1(phi = phi, sigma2 = sigma2) # Initialization of result array result = array(NA, c(B,2,20)) # Start Monte-Carlo for (i in 1:B){ # Simulate AR(1) Xt = gen.gts(model, N = n) # Create a copy of Xt Yt = Xt # Add a proportion alpha of extreme observations to Yt Yt[sample(1:n,round(alpha*n))] = rnorm(round(alpha*n), 0, sigma2.cont) # Compute ACF of Xt and Yt acf_Xt = ACF(Xt) acf_Yt = ACF(Yt) # Store ACFs result[i,1,] = acf_Xt[1:20] result[i,2,] = acf_Yt[1:20] } # Compare empirical distribution of ACF based on Xt and Yt # Vector of lags considered (h &lt;= 20) lags = c(1,2,5,10) + 1 # Make graph par(mfrow = c(2,2)) for (i in 1:4){ boxplot(result[,1,lags[i]], result[,2,lags[i]], col = &quot;lightgrey&quot;, names = c(&quot;Uncont.&quot;,&quot;Cont.&quot;), main = paste(&quot;lag: h = &quot;, lags[i]-1), ylab = &quot;Sample autocorrelation&quot;) abline(h = phi^(lags[i]-1), col = 2, lwd = 2) } The boxplots in each figure show how the standard autocorrelation estimator is centred around the true value (red line) when the sample is not contaminated (left boxplot) while it is considerably biased when the sample is contaminated (right boxplot), especially at the smallest lags. Indeed, it can be seen how the boxplots under contamination are often close to zero indicating that it does not detect much dependence in the data although it should. This is a known result in robustness, more specifically that outliers in the data can break the dependence structure and make it more difficult for the latter to be detected. In order to limit this problematic, different robust estimators exist for time series problems allowing to reduce the impact of contamination on the estimation procedure. Among these estimators there are a few that estimate the autocorrelation (autocovariance) functions in a robust manner. One of these estimators is provided in the robacf() function in the “robcor” package and the following simulated example shows how it limits the bias due to contamination: # Load packages library(&quot;robcor&quot;) # Define sample size n = 100 # Define proportion of &quot;extreme&quot; observation alpha = 0.05 # Extreme observation are generated from N(0,sigma2.cont) sigma2.cont = 10 # Number of Monte-Carlo replications B = 1000 # Define model AR(1) phi = 0.5 sigma2 = 1 model = AR1(phi = phi, sigma2 = sigma2) # Initialization of result array result = array(NA, c(B,2,20)) # Start Monte-Carlo for (i in 1:B){ # Simulate AR(1) Xt = gen.gts(model, N = n) # Add a proportion alpha of extreme observations to Yt Xt[sample(1:n,round(alpha*n))] = rnorm(round(alpha*n), 0, sigma2.cont) # Compute standard and robust ACF of Xt and Yt acf = ACF(Xt) rob_acf = robacf(Xt, plot=FALSE)$acf # Store ACFs result[i,1,] = acf[1:20] result[i,2,] = rob_acf[1:20] } # Compare empirical distribution of standard and robust ACF based on Xt # Vector of lags considered (h &lt;= 20) lags = c(1,2,5,10) + 1 # Make graph par(mfrow = c(2,2)) for (i in 1:4){ boxplot(result[,1,lags[i]], result[,2,lags[i]], col = &quot;lightgrey&quot;, names = c(&quot;Standard&quot;,&quot;Robust&quot;), main = paste(&quot;lag: h = &quot;, lags[i]-1), ylab = &quot;Sample autocorrelation&quot;) abline(h = phi^(lags[i]-1), col = 2, lwd = 2) } The robust estimator remains close to the true value represented by the red line in the boxplots as opposed to the standard estimator. It can also be observed that to reduce the bias induced by contamination in the sample, robust estimators pay a certain price in terms of efficiency as highlighted by the boxplots that show more variability compared to those of the standard estimator. Robustness issues therefore need to be considered for any time series analysis, not only when estimating the autocorrelation (autocovariance) functions. "],
["basic-models.html", "Chapter 3 Basic Models 3.1 The Backshift Operator 3.2 White Noise 3.3 Moving Average Process of Order q = 1 a.k.a MA(1) 3.4 Drift 3.5 Random Walk 3.6 Random Walk with Drift 3.7 Autoregressive Process of Order p = 1 a.k.a AR(1)", " Chapter 3 Basic Models 3.1 The Backshift Operator Definition: Backshift Operator The Backshift Operator is helpful when manipulating time series. When we backshift, we are changing the indices of the time series. e.g. \\(t \\rightarrow t-1\\). The operator is defined as: \\[B{x_t} = {x_{t - 1}}\\] If we were to repeatedly apply the backshift operator, we would receive: \\[\\begin{aligned} {B^2}{x_t} &amp;= B\\left( {B{x_t}} \\right) \\\\ &amp;= B\\left( {{x_{t - 1}}} \\right) \\\\ &amp;= {x_{t - 2}} \\\\ \\end{aligned}\\] We can generalize this behavior as: \\[{B^k}{x_t} = {x_{t - k}}\\] The backshift operator is helpful for later decompositions in addition to making differencing operations more straightforward. 3.2 White Noise The process name of white noise has meaning in the notion of colors of noise. Specifically, the white noise is a process that mirrors white light’s flat frequency spectrum. So, the process has equal frequencies in any interval of time. Definition: White Noise \\(w_t\\) or \\(\\varepsilon _t\\) is a white noise process if \\(w_t\\) are uncorrelated identically distributed random variables with \\(E\\left[w_t\\right] = 0\\) and \\(Var\\left[w_t\\right] = \\sigma ^2\\), for all \\(t\\). We can represent this algebraically as: \\[y_t = w_t,\\] where \\({w_t}\\mathop \\sim \\limits^{id} WN\\left( {0,\\sigma _w^2} \\right)\\) Now, if the \\(w_t\\) are Normally (Gaussian) distributed, then the process is known as a Gaussian White Noise e.g. \\({w_t}\\mathop \\sim \\limits^{iid} N\\left( {0,{\\sigma ^2}} \\right)\\) To generate gaussian white noise use: set.seed(1336) # Set seed to reproduce the results n = 200 # Number of observations to generate wn = ts(rnorm(n,0,1)) # Generate Guassian white noise. autoplot(wn) + ggtitle(&quot;White Noise Process&quot;) + ylab(&quot;Displacement&quot;) + xlab(&quot;Time (seconds)&quot;) 3.3 Moving Average Process of Order q = 1 a.k.a MA(1) Definition: Moving Average Process of Order (q = 1) The concept of a Moving Average Process of Order q is a way to remove “noise” and emphasize the signal. The moving average achieves this by taking the local averages of the data to produce a new smoother time series series. The newly created time series is more descriptive, but it does influence the dependence within the time series. This process is generally denoted as MA(1) and is defined as: \\[{y_t} = {\\theta _1}{w_{t - 1}} + {w_t},\\] where \\({w_t}\\mathop \\sim \\limits^{iid} WN\\left( {0,\\sigma _w^2} \\right)\\) set.seed(1345) # Set seed to reproduce the results n = 200 # Number of observations to generate sigma2 = 2 # Controls variance of Guassian white noise. theta = 0.3 # Handles the theta component of MA(1) # Generate a white noise wn = rnorm(n+1, sd = sqrt(sigma2)) # Simulate the MA(1) process ma = rep(0, n+1) for(i in 2:(n+1)) { ma[i] = theta*wn[i-1] + wn[i] } ma = ts(ma[2:(n+1)]) # Remove first item autoplot(ma) + ggtitle(&quot;Moving Average Order 1 Process&quot;) + ylab(&quot;Displacement&quot;) + xlab(&quot;Time (seconds)&quot;) 3.4 Drift Definition: Drift A drift process has two components: time and a slope. As more points are accumlated over time, the drift will match the common slope form. Specifically, the drift process has the following form: \\[y_t = y_{t-1} + \\delta \\] with the initial condition \\(y_0 = c\\). The process can be simplified using backsubstitution to being: \\[\\begin{aligned} {y_t} &amp;= {y_{t - 1}} + \\delta \\\\ &amp;= \\left( {{y_{t - 2}} + \\delta} \\right) + \\delta \\\\ &amp;\\vdots \\\\ &amp;= \\sum\\limits_{i = 1}^t {\\delta} + y_0 \\\\ {y_t} &amp;= t{\\delta} + c \\\\ \\end{aligned} \\] Again, note that a drift is similar to the slope-intercept form a linear line. e.g. \\(y = mx + b\\). To generate a drift use: n = 200 # Number of observations to generate drift = .3 # Drift Control dr = ts(drift*(1:n)) # Generate drift sequence (e.g. y = drift*x + 0) autoplot(dr) + ggtitle(&quot;Drift Process&quot;) + ylab(&quot;Displacement&quot;) + xlab(&quot;Time (seconds)&quot;) 3.5 Random Walk In 1906, Karl Pearson coined the term ‘random walk’ and demonstrated that “the most likely place to find a drunken walker is somewhere near his starting point.” Empirical evidence of this phenomenon is not too hard to find on a Friday night in Champaign. Definition: Random Walk A random walk is defined as a process where the current value of a variable is composed of the past value plus an error term that is a white noise. In algebraic form, \\[y_t = y_{t-1} + w_t\\] with the initial condition \\(y_0 = c\\). The process can be simplified using backsubstitution to being: \\[\\begin{aligned} {y_t} &amp;= {y_{t - 1}} + {w_t} \\\\ &amp;= \\left( {{y_{t - 2}} + {w_{t - 1}}} \\right) + {w_t} \\\\ &amp;\\vdots \\\\ {y_t} &amp;= \\sum\\limits_{i = 1}^t {{w_i}} + y_0 = \\sum\\limits_{i = 1}^t {{w_i}} + c \\\\ \\end{aligned} \\] To generate a random walk, we use: set.seed(1336) # Set seed to reproduce the results n = 200 # Number of observations to generate w = rnorm(n,0,1) # Generate Guassian white noise. rw = ts(cumsum(w)) # Cumulative sum # Create a data.frame to graph in ggplot2 autoplot(rw) + ggtitle(&quot;Random Walk&quot;) + ylab(&quot;Displacement&quot;) + xlab(&quot;Time (seconds)&quot;) 3.6 Random Walk with Drift In the previous case of a random walk, we assumed that drift, \\(\\delta\\), was equal to 0. What happens to the random walk if the drift is not equal to zero? That is, what happens with the initial condition \\(y_0 = c\\)? \\[\\begin{aligned} {y_t} &amp;= {y_{t - 1}} + {w_t} + \\delta \\\\ &amp;= \\left( {{y_{t - 2}} + {w_{t - 1}} + \\delta} \\right) + {w_t} + \\delta \\\\ &amp;\\vdots \\\\ {y_t} &amp;= \\sum\\limits_{i = 1}^t {\\left({w_{i} + \\delta}\\right)} + y_0 = \\sum\\limits_{i = 1}^t {{w_i}} + t\\delta + c \\\\ \\end{aligned} \\] To generate a random walk with drift we use: set.seed(1336) # Set seed to reproduce the results n = 200 # Number of observations to generate drift = .3 # Drift Control w = rnorm(n,0,1) # Generate Guassian white noise. wd = w + drift # Add a drift rwd = ts(cumsum(wd)) # Cumulative sum # Create a data.frame to graph in ggplot2 autoplot(rwd) + ggtitle(&quot;Random Walk with Drift&quot;) + ylab(&quot;Displacement&quot;) + xlab(&quot;Time (seconds)&quot;) Notice the difference the drift makes upon the random walk: # Add identifiers drift.df = data.frame(Index = 1:n, Data = drift*(1:n), Type = &quot;Drift&quot;) rw.df = data.frame(Index = 1:n, Data = rw, Type = &quot;Random Walk&quot;) rwd.df = data.frame(Index = 1:n, Data = rwd, Type = &quot;Random Walk with Drift&quot;) combined.df = rbind(drift.df, rw.df, rwd.df) ggplot(data = combined.df, aes(x = Index, y = Data, colour = Type)) + geom_line() + ggtitle(&quot;Comparisons of Random Walk&quot;) + ylab(&quot;Displacement&quot;) + xlab(&quot;Time (seconds)&quot;) 3.7 Autoregressive Process of Order p = 1 a.k.a AR(1) Definition: Autoregressive Process of Order p = 1 This process is generally denoted as AR(1) and is defined as: \\({y_t} = {\\phi _1}{y_{t - 1}} + {w_t},\\) where \\({w_t}\\mathop \\sim \\limits^{iid} WN\\left( {0,\\sigma _w^2} \\right)\\) If \\(\\phi _1 = 1\\), then the process is equivalent to a random walk. The process can be simplified using backsubstitution to being: \\[\\begin{aligned} {y_t} &amp;= {\\phi _t}{y_{t - 1}} + {w_t} \\\\ &amp;= {\\phi _1}\\left( {{\\phi _1}{y_{t - 2}} + {w_{t - 1}}} \\right) + {w_t} \\\\ &amp;= \\phi _1^2{y_{t - 2}} + {\\phi _1}{w_{t - 1}} + {w_t} \\\\ &amp;\\vdots \\\\ &amp;= {\\phi ^t}{y_0} + \\sum\\limits_{i = 0}^{t - 1} {\\phi _1^i{w_{t - i}}} \\end{aligned}\\] set.seed(1345) # Set seed to reproduce the results n = 200 # Number of observations to generate sigma2 = 2 # Controls variance of Guassian white noise. phi = 0.3 # Handles the phi component of AR(1) wn = rnorm(n+1, sd = sqrt(sigma2)) # Simulate the MA(1) process ar = rep(0,n+1) for(i in 2:n) { ar[i] = phi*ar[i-1] + wn[i] } ar = ts(ar[2:(n+1)]) autoplot(ar) + ggtitle(&quot;Autoregressive Order 1 Process&quot;) + ylab(&quot;Displacement&quot;) + xlab(&quot;Time (seconds)&quot;) "],
["arma.html", "Chapter 4 ARMA 4.1 Definition 4.2 MA / AR Operators 4.3 Redundancy 4.4 Causal + Invertible 4.5 Estimation of Parameters 4.6 Method of Moments 4.7 Prediction (Forecast)", " Chapter 4 ARMA 4.1 Definition 4.2 MA / AR Operators 4.3 Redundancy 4.4 Causal + Invertible Example: Causal Conditons for an AR(2) Process We already know that an AR(1) is causal with the simple condition \\(|\\phi_1| &lt; 1\\). It could seem natural to believe that an AR(2) should be stationary with the conditon: \\(|\\phi_i| &lt; 1, \\, i = 1,2\\), however, this is not the case. Indeed, an AR(1) can be expressed as \\[X_t = \\phi_1 X_{t-1} + \\phi_2 X_{t-2} + W_t = \\phi_1 B X_t + \\phi_2 B^2 X_t + W_t,\\] corresponding to the following autoregressive operator: \\[\\phi(z) = 1 - \\phi_1 z + \\phi_2 z^2.\\] Therefore, the process is causal when the roots of \\(\\phi(z)\\) lies outside of the unit circle. Letting \\(z_1\\) and \\(z_2\\) denote those roots, we impose the following constraints to ensure the causality of the model: \\[\\begin{aligned} |z_1| &amp;&gt; 1, \\;\\;\\;\\; \\text{where} \\;\\; &amp;z_1 = \\frac{\\phi_1 + \\sqrt{\\phi_1^2 + 4\\phi_2}}{-2 \\phi_2},\\\\ |z_2| &amp;&gt; 1, \\;\\;\\;\\; \\text{where} \\;\\; &amp;z_2 = \\frac{\\phi_1 - \\sqrt{\\phi_1^2 + 4\\phi_2}}{-2 \\phi_2}. \\end{aligned}\\] TO FINISH plot(NA, xlim = c(-2.1,2.1), ylim = c(-1.1,1.1), xlab = expression(phi[1]), ylab = expression(phi[2]), cex.lab = 1.5) grid() # Adding boundary of constraint |phi_1| &lt; 2 abline(v = c(-2,2), lty = 2, col = &quot;darkgrey&quot;) # Adding boundary of constraint |phi_2| &lt; 1 abline(h = c(-1,1), lty = 2, col = &quot;darkgrey&quot;) # Adding boundary of constraint phi_2 = 1 - phi_1 phi1 = seq(from = -2, to = 2, length.out = 10^3) phi2.c1 = 1 - phi1 lines(phi1, phi2.c1, lty = 2, col = &quot;darkgrey&quot;) # Adding boundary of constraint phi_2 = 1 + phi_1 phi1 = seq(from = -2, to = 2, length.out = 10^3) phi2.c2 = 1 + phi1 lines(phi1, phi2.c2, lty = 2, col = &quot;darkgrey&quot;) # Adding admissible region polygon(c(phi1,rev(phi1)),c(rep(-1,10^3), rev(phi2.c1[501:1000]),rev(phi2.c2[1:500])), border = NA, col= rgb(0,0,1, alpha = 0.1)) # Adding text text(0,-0.5, c(&quot;Causal Region&quot;)) 4.5 Estimation of Parameters Consider a time series given by \\(x_t \\sim ARMA(p,q)\\). This gives us with a paramter space \\(\\Omega\\) that looks like so: \\[\\vec \\varphi = \\left[ {\\begin{array}{*{20}{c}} {{\\phi _1}} \\\\ \\vdots \\\\ {{\\phi _p}} \\\\ {{\\theta _1}} \\\\ \\vdots \\\\ {{\\theta _q}} \\\\ {{\\sigma ^2}} \\end{array}} \\right]\\] In order to estimate this parameter space, we must assume the following three conditions: The process is casual The process is invertible The process has Gaussian innovations. Innovations are a time series equivalent to residuals. That is, an innovation is given by \\({x_t} - \\hat x_t^{t - 1}\\), where \\(\\hat x_t^{t - 1}\\) is the prediction at time \\(t\\) given \\(t-1\\) observations and \\({x_t}\\) is the true value observed at time \\(t\\). There are two main ways of performing such an estimation of the parameter space. Maximum Likelihood / Least Squares Estimation [MLE / LSE] Method of Moments (MoM) To begin, we’ll explore using the MLE to perform the estimation. 4.5.1 Maximum Likelihood Estimation Definition Consider \\(X_n = (X_1, X_2, \\ldots, X_n)\\) with the joint density \\(f(X_1, X_2, \\ldots, X_n ; \\theta)\\) where \\(\\theta \\in \\Theta\\). Given \\(X_1 = x_1, X_2 = x_2, \\ldots, X_n = x_n\\) is observed, we have the likelihood function of \\(\\theta\\) as \\[L(\\theta) = L(\\theta|x_1,x_2, \\ldots, x_n) = f(x_1,x_2, \\ldots, x_n | \\theta)\\] If the \\(X_i\\) are iid, then the likelihood simplifies to: \\[L(\\theta) = \\prod\\limits_{i = 1}^n {f\\left( {{x_i}|\\theta } \\right)} \\] However, that’s a bit painful to maximize with calculus. So, we opt to use the log of the function since derivatives are easier and the logarithmic function is always increasing. Thus, we traditionally use: \\[l\\left( \\theta \\right) = \\log \\left( {L\\left( \\theta \\right)} \\right) = \\sum\\limits_{i = 1}^n {\\log \\left( {f\\left( {{x_i}|\\theta } \\right)} \\right)} \\] From maximizes the likelihood function \\(L(\\theta)\\), we get the maximum likelihood estimate (MLE) of \\(\\theta\\). So, we end up with a value that makes the observed data the “most probable.” Note: The likelihood function is not a probability density function. 4.5.1.1 \\(AR(1)\\) with mean \\(\\mu\\) Consider an \\(AR(1)\\) process given as \\(y_t = \\phi y_{t-1} + w_t\\), \\({w_t}\\mathop \\sim \\limits^{iid} N\\left( {0,{\\sigma ^2}} \\right)\\), with \\(E\\left[ {{y_t}} \\right] = 0\\), \\(\\left| \\phi \\right| &lt; 1\\). Let \\(x_t = y_t + \\mu\\), so that \\(E\\left[ {{x_t}} \\right] = \\mu\\). Then, \\({x_t} - \\mu = {y_t}\\). Substituting in for \\(y_t\\), we get: \\[\\begin{aligned} y_t &amp;= \\phi y_{t-1} + w_t \\\\ \\underbrace {\\left( {{x_t} - \\mu } \\right)}_{ = {y_t}} &amp;= \\phi \\underbrace {\\left( {{x_{t - 1}} - \\mu } \\right)}_{ = {y_t}} + {w_t} \\\\ {x_t} &amp;= \\mu + \\phi \\left( {{x_{t - 1}} - \\mu } \\right) + {w_t} \\end{aligned}\\] In this case, \\(x_t\\) is an \\(AR(1)\\) process with mean \\(\\mu\\). This means that we have: \\(E\\left[ {{x_t}} \\right] = \\mu\\) \\[\\begin{aligned} Var\\left( {{x_t}} \\right) &amp;= Var\\left( {{x_t} - \\mu } \\right) \\hfill \\\\ &amp;= Var\\left( {{y_t}} \\right) \\hfill \\\\ &amp;= Var\\left( {\\sum\\limits_{j = 0}^\\infty {{\\phi ^j}{w_{t - j}}} } \\right) \\hfill \\\\ &amp;= \\sum\\limits_{j = 0}^\\infty {{\\phi ^{2j}}Var\\left( {{w_{t - j}}} \\right)} \\hfill \\\\ &amp;= {\\sigma ^2}\\sum\\limits_{j = 0}^\\infty {{\\phi ^{2j}}} \\hfill \\\\ &amp;= \\frac{{{\\sigma ^2}}}{{1 - {\\phi ^2}}},{\\text{ since }}\\left| \\phi \\right| &lt; 1{\\text{ and }}\\sum\\limits_{k = 0}^n {a{r^k}} = \\frac{a}{{1 - r}} \\hfill \\\\ \\end{aligned} \\] So, \\(x_t \\sim N\\left({ \\mu, \\frac{{{\\sigma ^2}}}{{1 - {\\phi ^2}}} }\\right)\\). Note that the distribution of \\(x_t\\) is normal and, thus, the density function of \\(x_t\\) is given by: \\[\\begin{aligned} f\\left( {{x_t}} \\right) &amp;= \\sqrt {\\frac{{1 - {\\phi ^2}}}{{2\\pi {\\sigma ^2}}}} \\exp \\left( { - \\frac{1}{2} \\cdot \\frac{{1 - {\\phi ^2}}}{{{\\sigma ^2}}} \\cdot {{\\left( {{x_t} - \\mu } \\right)}^2}} \\right) \\hfill \\\\ &amp;= {\\left( {2\\pi } \\right)^{ - \\frac{1}{2}}}{\\left( {{\\sigma ^2}} \\right)^{ - \\frac{1}{2}}}{\\left( {1 - {\\phi ^2}} \\right)^{\\frac{1}{2}}}\\exp \\left( { - \\frac{1}{2} \\cdot \\frac{{1 - {\\phi ^2}}}{{{\\sigma ^2}}} \\cdot {{\\left( {{x_t} - \\mu } \\right)}^2}} \\right) \\textrm{ [1]} \\\\ \\end{aligned} \\] We’ll call the last equation [1]. 4.5.1.2 Conditioning time \\(x_t | x_{t-1}\\) Now, consider \\(x_t | x_{t-1}\\) for \\(t &gt; 1\\). The mean is given by: \\[\\begin{aligned} E\\left[ {{x_t}|{x_{t - 1}}} \\right] &amp;= E\\left[ {\\mu + \\phi \\left( {{x_{t - 1}} - \\mu } \\right) + {w_t}|{x_{t - 1}}} \\right] \\nonumber \\\\ &amp;= \\mu + \\phi \\left( {{x_{t - 1}} - \\mu } \\right) \\end{aligned} \\] This is the case since \\(E\\left[ {{x_{t - 1}}|{x_{t - 1}}} \\right] = {x_{t - 1}}\\) and \\(E\\left[ {{w_t}|{x_{t - 1}}} \\right] = 0\\) Now, the variance is: \\[\\begin{aligned} Var\\left( {{x_t}|{x_{t - 1}}} \\right) &amp;= Var\\left( {\\mu + \\phi \\left( {{x_{t - 1}} - \\mu } \\right) + {w_t}|{x_{t - 1}}} \\right) \\hfill \\\\ &amp;= \\underbrace {Var\\left( {\\mu + \\phi \\left( {{x_{t - 1}} - \\mu } \\right)|{x_{t - 1}}} \\right)}_{ = 0} + Var\\left( {{w_t}|{x_{t - 1}}} \\right) \\hfill \\\\ &amp;= Var\\left( {{w_t}} \\right) \\hfill \\\\ &amp;= {\\sigma ^2} \\hfill \\\\ \\end{aligned} \\] Thus, we have: \\({x_t}\\sim N\\left( {\\mu + \\phi \\left( {{x_{t - 1}} - \\mu } \\right),{\\sigma ^2}} \\right)\\). Again, note that the distribution of \\(x_t\\) is normal and, thus, the density function of \\(x_t\\) is given by: \\[\\begin{aligned} f\\left( {{x_t}} \\right) &amp;= \\sqrt {\\frac{1}{{2\\pi {\\sigma ^2}}}} \\exp \\left( { - \\frac{1}{{2{\\sigma ^2}}} \\cdot {{\\left[ {\\left( {{x_t} - \\mu } \\right) - \\phi \\left( {{x_{t - 1}} - \\mu } \\right)} \\right]}^2}} \\right) \\hfill \\\\ &amp;= {\\left( {2\\pi } \\right)^{ - \\frac{1}{2}}}{\\left( {{\\sigma ^2}} \\right)^{ - \\frac{1}{2}}}\\exp \\left( { - \\frac{1}{{2{\\sigma ^2}}} \\cdot {{\\left[ {\\left( {{x_t} - \\mu } \\right) - \\phi \\left( {{x_{t - 1}} - \\mu } \\right)} \\right]}^2}} \\right) \\textrm{ [2]} \\\\ \\end{aligned} \\] And for this equation we’ll call it [2]. 4.5.2 MLE for \\(\\sigma ^2\\) on \\(AR(1)\\) with mean \\(\\mu\\) Whew, with all of the above said, we’re now ready to obtain an MLE estimate on an \\(AR(1)\\). Let \\(\\vec{\\theta} = \\left[ {\\begin{array}{*{20}{c}} \\mu \\\\ \\phi \\\\ {{\\sigma ^2}} \\end{array}} \\right]\\), then the likelihood of \\(\\vec{\\theta}\\) is given by \\(x_1, \\ldots , x_T\\) is: \\[\\begin{aligned} L\\left( {\\vec \\theta |{x_1}, \\ldots ,{x_T}} \\right) &amp;= f\\left( {{x_1}, \\ldots ,{x_T}|\\vec \\theta } \\right) \\hfill \\\\ &amp;= f\\left( {{x_1}} \\right) \\cdot \\prod\\limits_{t = 2}^T {f\\left( {{x_t}|{x_{t - 1}}} \\right)} \\end{aligned} \\] The last equality is the result of us using a lag 1 of “memory.” Also, note that \\(x_t | x_{t-1}\\) must have \\(t &gt; 1 \\in \\mathbb{N}\\). Furthermore, we have dropped the parameters in the densities, e.g. \\(\\vec{\\theta}\\) in \\(f(\\cdot)\\), to ease notation. Using equations [1] and [2], we have: \\[L\\left( {\\vec \\theta |{x_1}, \\ldots ,{x_T}} \\right) = {\\left( {2\\pi } \\right)^{ - \\frac{T}{2}}}{\\left( {{\\sigma ^2}} \\right)^{ - \\frac{T}{2}}}{\\left( {1 - {\\phi ^2}} \\right)^{\\frac{1}{2}}}\\exp \\left( { - \\frac{1}{{2{\\sigma ^2}}}\\left[ {\\left( {1 - {\\phi ^2}} \\right){{\\left( {{x_t} - \\mu } \\right)}^2} + \\sum\\limits_{t = 2}^T {{{\\left[ {\\left( {{x_t} - \\mu } \\right) - \\phi \\left( {{x_{t - 1}} - \\mu } \\right)} \\right]}^2}} } \\right]} \\right)\\] For convenience, we’ll define: \\[S\\left( {\\mu ,\\phi } \\right) = \\left( {1 - {\\phi ^2}} \\right){\\left( {{x_t} - \\mu } \\right)^2} + \\sum\\limits_{t = 2}^T {{{\\left[ {\\left( {{x_t} - \\mu } \\right) - \\phi \\left( {{x_{t - 1}} - \\mu } \\right)} \\right]}^2}} \\] Fun fact, this is called the “unconditional sum of squares.” Thus, we will operate on: \\[L\\left( {\\vec \\theta |{x_1}, \\ldots ,{x_T}} \\right) = {\\left( {2\\pi } \\right)^{ - \\frac{T}{2}}}{\\left( {{\\sigma ^2}} \\right)^{ - \\frac{T}{2}}}{\\left( {1 - {\\phi ^2}} \\right)^{\\frac{1}{2}}}\\exp \\left( { - \\frac{1}{{2{\\sigma ^2}}}S\\left( {\\mu ,\\phi } \\right)} \\right)\\] Taking the log of this yields: \\[\\begin{aligned} l\\left( {\\vec \\theta |{x_1}, \\ldots ,{x_T}} \\right) &amp;= \\log \\left( {L\\left( {\\vec \\theta |{x_1}, \\ldots ,{x_T}} \\right)} \\right) \\hfill \\\\ &amp;= - \\frac{T}{2}\\log \\left( {2\\pi } \\right) - \\frac{T}{2}\\log \\left( {{\\sigma ^2}} \\right) + \\frac{1}{2}\\left( {1 - {\\phi ^2}} \\right) - \\frac{1}{{2{\\sigma ^2}}}S\\left( {\\mu ,\\phi } \\right) \\hfill \\\\ \\end{aligned} \\] Now, taking the derivative and solving for the maximized point gives: \\[\\begin{aligned} \\frac{\\partial }{{\\partial {\\sigma ^2}}}l\\left( {\\vec \\theta |{x_1}, \\ldots ,{x_T}} \\right) &amp;= - \\frac{T}{{2{\\sigma ^2}}} + \\frac{1}{{2{\\sigma ^4}}}S\\left( {\\mu ,\\phi } \\right) \\hfill \\\\ 0 &amp;= - \\frac{T}{{2{\\sigma ^2}}} + \\frac{1}{{2{\\sigma ^4}}}S\\left( {\\mu ,\\phi } \\right) \\hfill \\\\ \\frac{T}{{2{\\sigma ^2}}} &amp;= \\frac{1}{{2{\\sigma ^4}}}S\\left( {\\mu ,\\phi } \\right) \\hfill \\\\ {{ \\sigma }^2} &amp;= \\frac{1}{T}S\\left( {\\mu ,\\phi } \\right) \\hfill \\\\ \\end{aligned} \\] Thus, the MLE for \\({\\hat \\sigma }^2 = \\frac{1}{T}S\\left( {\\hat \\mu ,\\hat \\phi } \\right)\\), where \\(\\hat \\mu\\) and \\(\\hat \\phi\\) are the MLEs for \\(\\mu , \\phi\\) that are obtained numerically via either Newton Raphson or a Scoring Algorithm. (More details in a numerical recipe book.) 4.5.2.1 Conditional MLE on \\(AR(1)\\) with mean \\(\\mu\\) A common strategy to reduce the dependency on numerical recipes is to simplify \\(l\\left( {\\vec \\theta |{x_1}, \\ldots ,{x_T}} \\right)\\) by using \\({l^*}\\left( {\\vec \\theta |{x_1}, \\ldots ,{x_T}} \\right)\\): \\[\\begin{aligned} {l^*}\\left( {\\vec \\theta |{x_1}, \\ldots ,{x_T}} \\right) &amp;= \\prod\\limits_{t = 2}^T {\\log \\left( {f\\left( {{x_t}|{x_{t - 1}}} \\right)} \\right)} \\hfill \\\\ &amp;= \\prod\\limits_{t = 2}^T {\\log \\left( {{{\\left( {2\\pi } \\right)}^{ - \\frac{1}{2}}}{{\\left( {{\\sigma ^2}} \\right)}^{ - \\frac{1}{2}}}\\exp \\left( { - \\frac{1}{{2{\\sigma ^2}}} \\cdot {{\\left[ {\\left( {{x_t} - \\mu } \\right) - \\phi \\left( {{x_{t - 1}} - \\mu } \\right)} \\right]}^2}} \\right)} \\right)} \\hfill \\\\ &amp;= - \\frac{{\\left( {T - 1} \\right)}}{2}\\log \\left( {2\\pi } \\right) - \\frac{{\\left( {T - 1} \\right)}}{2}\\log \\left( {{\\sigma ^2}} \\right) - \\frac{1}{{2{\\sigma ^2}}}\\sum\\limits_{t = 2}^T {{{\\left[ {\\left( {{x_t} - \\mu } \\right) - \\phi \\left( {{x_{t - 1}} - \\mu } \\right)} \\right]}^2}} \\hfill \\\\ \\end{aligned} \\] Again, for convenience, we’ll define: \\[{S_c}\\left( {\\mu ,\\phi } \\right) = \\sum\\limits_{t = 2}^T {{{\\left[ {\\left( {{x_t} - \\mu } \\right) - \\phi \\left( {{x_{t - 1}} - \\mu } \\right)} \\right]}^2}} \\] Fun fact, this is called the “conditional sum of squares.” So, we will use: \\[{l^*}\\left( {\\vec \\theta |{x_1}, \\ldots ,{x_T}} \\right) = - \\frac{{\\left( {T - 1} \\right)}}{2}\\log \\left( {2\\pi } \\right) - \\frac{{\\left( {T - 1} \\right)}}{2}\\log \\left( {{\\sigma ^2}} \\right) - \\frac{1}{{2{\\sigma ^2}}}{S_c}\\left( {\\mu ,\\phi } \\right)\\] Taking the derivative with respect to \\(\\mu\\) gives: \\[\\begin{aligned} \\frac{\\partial }{{\\partial \\mu }}{l^*}\\left( {\\vec \\theta |{x_1}, \\ldots ,{x_T}} \\right) &amp;= - \\frac{1}{{2{\\sigma ^2}}}\\sum\\limits_{t = 2}^T {2\\left[ {\\left( {{x_t} - \\mu } \\right) - \\phi \\left( {{x_{t - 1}} - \\mu } \\right)} \\right]\\left( {\\phi - 1} \\right)} \\hfill \\\\ &amp;= \\frac{{1 - \\phi }}{{{\\sigma ^2}}}\\sum\\limits_{t = 2}^T {\\left[ {\\left( {{x_t} - \\mu } \\right) - \\phi \\left( {{x_{t - 1}} - \\mu } \\right)} \\right]} \\hfill \\\\ &amp;= \\frac{{1 - \\phi }}{{{\\sigma ^2}}}\\sum\\limits_{t = 2}^T {\\left( {{x_t} - \\phi {x_{t - 1}} - \\mu \\left( {1 - \\phi } \\right)} \\right)} \\hfill \\\\ &amp;= -\\frac{{{{\\left( {1 - \\phi } \\right)}^2}}}{{{\\sigma ^2}}}\\mu \\left( {T - 1} \\right) + \\frac{{\\left( {1 - \\phi } \\right)}}{{{\\sigma ^2}}}\\sum\\limits_{t = 2}^T {\\left( {{x_t} - \\phi {x_{t - 1}}} \\right)} \\hfill \\\\ \\end{aligned} \\] Solving for \\(\\mu^{*}\\) gives: \\[\\begin{aligned} 0 &amp;= \\frac{\\partial }{{\\partial \\mu }}{l^*}\\left( {\\vec \\theta |{x_1}, \\ldots ,{x_t}} \\right) \\hfill \\\\ 0 &amp;= - \\frac{{{{\\left( {1 - \\phi } \\right)}^2}}}{{{\\sigma ^2}}}{\\mu ^*}\\left( {T - 1} \\right) + \\frac{{\\left( {1 - {\\phi ^*}} \\right)}}{{\\sigma _*^2}}\\sum\\limits_{t = 2}^T {\\left( {{x_t} - {\\phi ^*}{x_{t - 1}}} \\right)} \\hfill \\\\ \\frac{{{{\\left( {1 - {\\phi ^*}} \\right)}^2}}}{{\\sigma _*^2}}{\\mu ^*}\\left( {T - 1} \\right) &amp;= \\frac{{\\left( {1 - {\\phi ^*}} \\right)}}{{\\sigma _*^2}}\\sum\\limits_{t = 2}^T {\\left( {{x_t} - {\\phi ^*}{x_{t - 1}}} \\right)} \\hfill \\\\ {\\mu ^*}\\left( {1 - {\\phi ^*}} \\right)\\left( {T - 1} \\right) &amp;= \\sum\\limits_{t = 2}^T {\\left( {{x_t} - {\\phi ^*}{x_{t - 1}}} \\right)} \\hfill \\\\ {\\mu ^*} &amp;= \\frac{1}{{\\left( {1 - {\\phi ^*}} \\right)\\left( {T - 1} \\right)}}\\sum\\limits_{t = 2}^T {\\left( {{x_t} - {\\phi ^*}{x_{t - 1}}} \\right)} \\hfill \\\\ {\\mu ^*} &amp;= \\frac{1}{{1 - {\\phi ^*}}}\\left[ {\\underbrace {\\frac{1}{{T - 1}}\\sum\\limits_{t = 2}^T {{x_t}} }_{ = {{\\bar x}_{\\left( 2 \\right)}}} - \\underbrace {\\frac{{{\\phi ^*}}}{{T - 1}}\\sum\\limits_{t = 2}^T {{x_{t - 1}}} }_{ = {{\\bar x}_{\\left( 1 \\right)}}}} \\right] \\hfill \\\\ {{\\hat \\mu }^*} &amp;= \\frac{1}{{1 - {\\phi ^*}}}\\left( {{{\\bar x}_{\\left( 2 \\right)}} - \\phi {{\\bar x}_{\\left( 1 \\right)}}} \\right) \\hfill \\\\ \\end{aligned} \\] When \\(T\\) is large, we have the following: \\[\\begin{aligned} {{\\bar x}_{\\left( 1 \\right)}} \\approx \\bar x &amp;,{{\\bar x}_{\\left( 2 \\right)}} \\approx \\bar x \\hfill \\\\ \\hfill \\\\ {{\\hat \\mu }^*} &amp;= \\frac{1}{{1 - {\\phi ^*}}}\\left( {\\bar x - {\\phi ^*}\\bar x} \\right) \\hfill \\\\ &amp;= \\frac{{\\bar x}}{{1 - {\\phi ^*}}}\\left( {1 - {\\phi ^*}} \\right) \\hfill \\\\ &amp;= \\bar x \\hfill \\\\ \\end{aligned} \\] Taking the derivative with respect to \\(\\sigma^2\\) and solving for \\(\\sigma^2\\) gives: \\[\\begin{aligned} \\frac{\\partial }{{\\partial {\\sigma ^2}}}{l^*}\\left( {\\vec \\theta |{x_1}, \\ldots ,{x_T}} \\right) &amp;= - \\frac{{\\left( {T - 1} \\right)}}{{2\\sigma _*^2}} + \\frac{1}{{2\\sigma _*^4}}{S_c}\\left( {\\mu ,\\phi } \\right) \\hfill \\\\ 0 &amp;= - \\frac{{\\left( {T - 1} \\right)}}{{2\\sigma _*^2}} + \\frac{1}{{2\\sigma _*^4}}{S_c}\\left( {\\mu ,\\phi } \\right) \\hfill \\\\ \\frac{{\\left( {T - 1} \\right)}}{{2\\sigma _*^2}} &amp;= \\frac{1}{{2\\sigma _*^4}}{S_c}\\left( {\\mu ,\\phi } \\right) \\hfill \\\\ \\hat \\sigma _*^2 &amp;= \\frac{1}{{T - 1}}{S_c}\\left( {{{\\hat \\mu }^*},{{\\hat \\phi }^*}} \\right) \\hfill \\\\ \\end{aligned} \\] Taking the derivative with respect to \\(\\phi\\) gives: \\[\\begin{aligned} \\frac{\\partial }{{\\partial \\phi }}{l^*}\\left( {\\vec \\theta |{x_1}, \\ldots ,{x_T}} \\right) &amp;= - \\frac{1}{{2{\\sigma ^2}}}\\sum\\limits_{t = 2}^T { - 2\\left[ {\\left( {{x_t} - \\mu } \\right) - \\phi \\left( {{x_{t - 1}} - \\mu } \\right)} \\right]\\left( {{x_{t - 1}} - \\mu } \\right)} \\hfill \\\\ &amp;= \\frac{1}{{{\\sigma ^2}}}\\sum\\limits_{t = 2}^T {\\left[ {{x_t} - \\phi {x_{t - 1}} - \\mu \\left( {1 - \\phi } \\right)} \\right]\\left( {{x_{t - 1}} - \\mu } \\right)} \\hfill \\\\ &amp;= \\frac{1}{{{\\sigma ^2}}}\\sum\\limits_{t = 2}^T {\\left[ {{x_t}{x_{t - 1}} - \\phi x_{t - 1}^2 - \\mu \\left( {1 - \\phi } \\right){x_{t - 1}} - \\mu {x_t} + \\mu \\phi {x_{t - 1}} + {\\mu ^2}\\left( {1 - \\phi } \\right)} \\right]} \\hfill \\\\ &amp;= \\frac{1}{{{\\sigma ^2}}}\\left[ \\begin{gathered} \\sum\\limits_{t = 2}^T {{x_t}{x_{t - 1}}} - \\phi \\sum\\limits_{t = 2}^T {x_{t - 1}^2} - \\mu \\left( {1 - \\phi } \\right)\\left( {T - 1} \\right){{\\bar x}_{\\left( 1 \\right)}} \\hfill \\\\ - \\mu \\left( {T - 1} \\right){{\\bar x}_{\\left( 2 \\right)}} + \\phi \\mu \\left( {T - 1} \\right){{\\bar x}_{\\left( 1 \\right)}} + {\\mu ^2}\\left( {1 - \\phi } \\right)\\left( {T - 1} \\right) \\hfill \\\\ \\end{gathered} \\right] \\end{aligned}\\] Solving for \\(\\phi\\) gives: \\[\\begin{aligned} 0 &amp;= \\frac{\\partial }{{\\partial \\phi }}{l^*}\\left( {\\vec \\theta |{x_1}, \\ldots ,{x_T}} \\right) \\hfill \\\\ 0 &amp;= \\sum\\limits_{t = 2}^T {{x_t}{x_{t - 1}}} - {{\\hat \\phi }^*}\\sum\\limits_{t = 2}^T {x_{t - 1}^2} - \\left( {{{\\bar x}_{\\left( 2 \\right)}} - {{\\hat \\phi }^*}{{\\bar x}_{\\left( 1 \\right)}}} \\right)\\left( {T - 1} \\right){{\\bar x}_{\\left( 1 \\right)}} - \\frac{{{{\\bar x}_{\\left( 2 \\right)}} - {{\\hat \\phi }^*}{{\\bar x}_{\\left( 1 \\right)}}}}{{1 - {{\\hat \\phi }^*}}}\\left( {T - 1} \\right){{\\bar x}_{\\left( 2 \\right)}} \\\\ &amp;+ {{\\hat \\phi }^*}\\frac{{{{\\bar x}_{\\left( 2 \\right)}} - {{\\hat \\phi }^*}{{\\bar x}_{\\left( 1 \\right)}}}}{{1 - {{\\hat \\phi }^*}}}\\left( {T - 1} \\right){{\\bar x}_{\\left( 1 \\right)}} + {\\left( {\\frac{{{{\\bar x}_{\\left( 2 \\right)}} - {{\\hat \\phi }^*}{{\\bar x}_{\\left( 1 \\right)}}}}{{1 - {{\\hat \\phi }^*}}}} \\right)^2}\\left( {1 - {{\\hat \\phi }^*}} \\right)\\left( {T - 1} \\right) \\hfill \\\\ &amp;\\vdots \\hfill \\\\ &amp;{\\text{Magic}} \\hfill \\\\ &amp;\\vdots \\hfill \\\\ {{\\hat \\phi }^*} &amp;= \\frac{{\\sum\\limits_{t = 2}^T {\\left( {{x_t} - {{\\bar x}_{\\left( 2 \\right)}}} \\right)\\left( {{x_{t-1}} - {{\\bar x}_{\\left( 1 \\right)}}} \\right)} }}{{\\sum\\limits_{t = 2}^T {{{\\left( {{x_{t - 1}} - {{\\bar x}_{\\left( 1 \\right)}}} \\right)}^2}} }} \\hfill \\\\ \\end{aligned} \\] When \\(T\\) is large, we have: \\[\\begin{aligned} \\sum\\limits_{t = 2}^T {\\left( {{x_t} - {{\\bar x}_{\\left( 2 \\right)}}} \\right)\\left( {{x_t} - {{\\bar x}_{\\left( 1 \\right)}}} \\right)} &amp;\\approx \\sum\\limits_{t = 2}^T {\\left( {{x_t} - \\bar x} \\right)\\left( {{x_{t - 1}} - \\bar x} \\right)} \\hfill \\\\ \\sum\\limits_{t = 2}^T {{{\\left( {{x_{t - 1}} - {{\\bar x}_{\\left( 1 \\right)}}} \\right)}^2}} &amp;\\approx \\sum\\limits_{t = 1}^T {{{\\left( {{x_t} - \\bar x} \\right)}^2}} \\hfill \\\\ \\hfill \\\\ {{\\hat \\phi }^*} &amp;= \\frac{{\\sum\\limits_{t = 2}^T {\\left( {{x_t} - {{\\bar x}_{\\left( 2 \\right)}}} \\right)\\left( {{x_t} - {{\\bar x}_{\\left( 1 \\right)}}} \\right)} }}{{\\sum\\limits_{t = 2}^T {{{\\left( {{x_{t - 1}} - {{\\bar x}_{\\left( 1 \\right)}}} \\right)}^2}} }} \\approx \\frac{{\\sum\\limits_{t = 2}^T {\\left( {{x_t} - \\bar x} \\right)\\left( {{x_{t - 1}} - \\bar x} \\right)} }}{{\\sum\\limits_{t = 1}^T {{{\\left( {{x_t} - \\bar x} \\right)}^2}} }} = \\hat \\rho \\left( 1 \\right) \\hfill \\\\ \\end{aligned} \\] 4.6 Method of Moments The goal behind the estimation with Method of Moments is to match the theoretical moment (e.g. \\(E\\left[ {x_t^k} \\right]\\)) with the sample moment (e.g \\(\\frac{1}{n}\\sum\\limits_{i = 1}^n {x_i^k}\\)), where \\(k\\) denotes the moment. This method often leads to suboptimal estimates for general ARMA models. However, it is quite optimal for \\(AR(p)\\). 4.6.1 Method of Moments - AR(p) Consider an \\(AR(p)\\) process represented by: \\[{x_t} = {\\phi _1}{x_{t - 1}} + \\cdots + {\\phi _p}{x_{t - p}} + {w_t}\\] where \\(w_t \\sim N(0,\\sigma^2)\\) To begin, we find the Covariance of the process when \\(h &gt; 0\\): \\[\\begin{aligned} Cov\\left( {{x_{t + h}},{x_t}} \\right) &amp;\\mathop = \\limits^{\\left( {h &gt; 0} \\right)} Cov\\left( {{\\phi _1}{x_{t + h - 1}} + \\cdots + {\\phi _p}{x_{t + h - p}} + {w_{t + h}},{x_t}} \\right) \\hfill \\\\ &amp;= {\\phi _1}Cov\\left( {{x_{t + h - 1}},{x_t}} \\right) + \\cdots + {\\phi _p}Cov\\left( {{x_{t + h - p}},{x_t}} \\right) + Cov\\left( {{w_{t + h}},{x_t}} \\right) \\hfill \\\\ &amp;= {\\phi _1}\\gamma \\left( {h - 1} \\right) + \\cdots + {\\phi _p}\\gamma \\left( {h - p} \\right) \\hfill \\\\ \\end{aligned} \\] Now, we turn our attention to the variance of the process: \\[\\begin{aligned} Var\\left( {{w_t}} \\right) &amp;= Cov\\left( {{w_t},{w_t}} \\right) \\hfill \\\\ &amp;= Cov\\left( {{w_t},{w_t}} \\right) + \\underbrace {Cov\\left( {{\\phi _1}{x_{t - 1}},{w_t}} \\right)}_{ = 0} + \\cdots + \\underbrace {Cov\\left( {{\\phi _p}{x_{t - p}},{w_t}} \\right)}_{ = 0} \\hfill \\\\ &amp;= Cov\\left( {\\underbrace {{\\phi _1}{x_{t - 1}} + \\cdots + {\\phi _p}{x_p} + {w_t}}_{ = {x_t}},{w_t}} \\right) \\hfill \\\\ &amp;= Cov\\left( {{x_t},{w_t}} \\right) \\hfill \\\\ &amp;= Cov\\left( {{x_t},{x_t} - {\\phi _1}{x_{t - 1}} - \\cdots - {\\phi _p}{x_p}} \\right) \\hfill \\\\ &amp;= Cov\\left( {{x_t},{x_t}} \\right) - {\\phi _1}Cov\\left( {{x_t},{x_{t - 1}}} \\right) - \\cdots - {\\phi _p}Cov\\left( {{x_t},{x_{t - p}}} \\right) \\hfill \\\\ &amp;= \\gamma \\left( 0 \\right) - {\\phi _1}\\gamma \\left( 1 \\right) - \\cdots - {\\phi _p}\\gamma \\left( p \\right) \\hfill \\\\ \\end{aligned} \\] Together, these equations are known as the Yule-Walker equations. 4.6.2 Yule-Walker Definition Equation form: \\[\\begin{aligned} \\gamma \\left( h \\right) &amp;= {\\phi _1}\\gamma \\left( {h - 1} \\right) - \\cdots - {\\phi _p}\\gamma \\left( {h - p} \\right) \\hfill \\\\ {\\sigma ^2} &amp;= \\gamma \\left( 0 \\right) - {\\phi _1}\\gamma \\left( 1 \\right) - \\cdots - {\\phi _p}\\gamma \\left( p \\right) \\hfill \\\\ h &amp; = 1, \\ldots, p \\end{aligned} \\] Matrix form: \\[\\begin{aligned} \\Gamma \\vec \\phi &amp;= \\vec \\gamma \\hfill \\\\ {\\sigma ^2} &amp;= \\gamma \\left( 0 \\right) - {{\\vec \\phi }^T}\\vec \\gamma \\hfill \\\\ \\hfill \\\\ \\vec \\phi &amp;= \\left[ {\\begin{array}{*{20}{c}} {{\\phi _1}} \\\\ \\vdots \\\\ {{\\phi _p}} \\end{array}} \\right]_{p \\times 1},\\vec \\gamma = \\left[ {\\begin{array}{*{20}{c}} {\\gamma \\left( 1 \\right)} \\\\ \\vdots \\\\ {\\gamma \\left( p \\right)} \\end{array}} \\right]_{p \\times 1},\\Gamma = \\left\\{ {\\gamma \\left( {k - j} \\right)} \\right\\}_{j,k = 1}^p \\\\ \\end{aligned} \\] More aptly, the structure of \\(\\Gamma\\) looks like the following: \\[\\Gamma = {\\left[ {\\begin{array}{*{20}{c}} {\\gamma \\left( 0 \\right)}&amp;{\\gamma \\left( { - 1} \\right)}&amp;{\\gamma \\left( { - 2} \\right)}&amp; \\cdots &amp;{\\gamma \\left( {1 - p} \\right)} \\\\ {\\gamma \\left( 1 \\right)}&amp;{\\gamma \\left( 0 \\right)}&amp;{\\gamma \\left( { - 1} \\right)}&amp; \\cdots &amp;{\\gamma \\left( {2 - p} \\right)} \\\\ {\\gamma \\left( 2 \\right)}&amp;{\\gamma \\left( 1 \\right)}&amp;{\\gamma \\left( 0 \\right)}&amp; \\cdots &amp;{\\gamma \\left( {3 - p} \\right)} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ {\\gamma \\left( {p - 1} \\right)}&amp;{\\gamma \\left( {p - 2} \\right)}&amp;{\\gamma \\left( {p - 3} \\right)}&amp; \\cdots &amp;{\\gamma \\left( 0 \\right)} \\end{array}} \\right]_{p \\times p}}\\] Note, that we are able to use the above equations to effectively estimate \\(\\vec \\phi\\) and \\(\\sigma ^2\\). \\[\\left[ \\begin{aligned} \\hat{\\vec{\\phi}} &amp;= {{\\hat \\Gamma }^{ - 1}}\\hat{\\vec{\\gamma}} \\hfill \\\\ {{\\hat \\sigma }^2} &amp;= \\hat \\gamma \\left( 0 \\right) - {{\\hat{\\vec{\\gamma}}}^T}{{\\hat \\Gamma }^{ - 1}}\\hat{\\vec{\\gamma}} \\hfill \\\\ \\end{aligned} \\right. \\to {\\text{Yule - Walker Estimates}}\\] For the second equation, we are effectively substituting in the first equation for \\(\\hat{\\vec{\\phi}}\\), hence the quadratic form \\({{\\hat{\\vec{\\gamma}}}^T}{{\\hat \\Gamma }^{ - 1}}\\hat{\\vec{\\gamma}}\\). With this being said, there are a few nice asymptotic properties that we obtain for an \\(AR(p)\\). \\(\\sqrt T \\left( {\\hat{\\vec{\\phi}} - \\vec \\phi } \\right)\\mathop \\to \\limits_{t \\to \\infty }^L N\\left( {\\vec 0,{\\sigma ^2}{\\Gamma ^{ - 1}}} \\right)\\) \\({\\hat \\sigma ^2}\\mathop \\to \\limits^p {\\sigma ^2}\\) Yule-Walker estimates are optimal in the sense that they have the smallest asymptotic variance i.e. \\[Var\\left( {\\sqrt{T} \\hat{\\vec{\\phi}} } \\right) = {\\sigma ^2}{\\Gamma ^{ - 1}}\\] However, they are not necessarily optimal with small sample sizes. Conceptually, the reason for this optimality result is a consequence from the linear dependence between moments and variables. This is not true for MA or ARMA, which are both nonlinear and suboptimal. 4.6.3 Estimates Consider \\(x_t\\) as an \\(MA(1)\\) process: \\({x_t} = \\theta {w_{t - 1}} + {w_t},{w_t}\\mathop \\sim \\limits^{i.i.d} N\\left( {0,{\\sigma ^2}} \\right)\\) Finding the covariance when \\(h = 1\\) gives: \\[\\begin{aligned} Cov\\left( {{x_t},{x_{t - 1}}} \\right) &amp;= Cov\\left( {\\theta {w_{t - 1}} + {w_t},\\theta {w_{t - 2}} + {w_{t - 1}}} \\right) \\hfill \\\\ &amp;= Cov\\left( {\\theta {w_{t - 1}},{w_{t - 1}}} \\right) \\hfill \\\\ &amp;= \\theta {\\sigma ^2} \\hfill \\\\ \\end{aligned} \\] Finding the variance (e.g. \\(h=0\\)) gives: \\[\\begin{aligned} Cov\\left( {{x_t},{x_t}} \\right) &amp;= Cov\\left( {\\theta {w_{t - 1}} + {w_t},\\theta {w_{t - 1}} + {w_t}} \\right) \\hfill \\\\ &amp;= {\\theta ^2}Cov\\left( {{w_{t - 1}},{w_{t - 1}}} \\right) + \\underbrace {2\\theta Cov\\left( {{w_{t - 1}},{w_t}} \\right)}_{ = 0} + Cov\\left( {{w_t},{w_t}} \\right) \\hfill \\\\ &amp;= {\\theta ^2}{\\sigma ^2} + {\\sigma ^2} \\hfill \\\\ &amp;= {\\sigma ^2}\\left( {1 + {\\theta ^2}} \\right) \\hfill \\\\ \\end{aligned} \\] This gives us the MA(1) ACF of: \\[\\rho \\left( h \\right) = \\left\\{ {\\begin{array}{*{20}{c}} 1&amp;{h = 0} \\\\ {\\frac{\\theta }{{{\\theta ^2} + 1}}}&amp;{h = \\pm 1} \\end{array}} \\right.\\] With this in mind, let’s solve for possible \\(\\theta\\) values: \\[\\begin{aligned} \\rho \\left( 1 \\right) &amp;= \\frac{\\theta }{{{\\theta ^2} + 1}} \\hfill \\\\ \\Rightarrow \\theta &amp;= \\left( {{\\theta ^2} + 1} \\right)\\rho \\left( 1 \\right) \\hfill \\\\ \\theta &amp;= \\rho \\left( 1 \\right){\\theta ^2} + \\rho \\left( 1 \\right) \\hfill \\\\ 0 &amp;= \\rho \\left( 1 \\right){\\theta ^2} - \\theta + \\rho \\left( 1 \\right) \\hfill \\\\ \\end{aligned} \\] Yuck, that looks nasty. Let’s dig out an ol’ friend from middle school known as the quadratic formula: \\[\\theta = \\frac{{ - b \\pm \\sqrt {{b^2} - 4ac} }}{{2a}}\\] Applying the quadratic formula leads to: \\[\\begin{aligned} a &amp;= \\rho \\left( h \\right), b = -1, c = \\rho \\left( h \\right) \\\\ \\theta &amp;= \\frac{{1 \\pm \\sqrt {{1^2} - 4\\rho \\left( h \\right)\\rho \\left( h \\right)} }}{{2\\rho \\left( h \\right)}} \\hfill \\\\ \\theta &amp;= \\frac{{1 \\pm \\sqrt {1 - 4{{\\left[ {\\rho \\left( h \\right)} \\right]}^2}} }}{{2\\rho \\left( h \\right)}} \\hfill \\\\ \\end{aligned} \\] Thus, we have two possibilities: \\[\\begin{aligned} {\\theta _1} &amp;= \\frac{{1 + \\sqrt {1 - 4{{\\left[ {\\rho \\left( h \\right)} \\right]}^2}} }}{{2\\rho \\left( h \\right)}} \\hfill \\\\ {\\theta _2} &amp;= \\frac{{1 - \\sqrt {1 - 4{{\\left[ {\\rho \\left( h \\right)} \\right]}^2}} }}{{2\\rho \\left( h \\right)}} \\hfill \\\\ \\end{aligned}\\] To ensure invertibility, we mandate that \\(\\left| {\\rho \\left( 1 \\right)} \\right| &lt; \\frac{1}{2}\\). Thus, we opt for \\({\\theta _2}\\). So, our estimator is: \\[\\hat \\theta = \\frac{{1 - \\sqrt {1 - 4{{\\left[ {\\hat \\rho \\left( 1 \\right)} \\right]}^2}} }}{{2\\hat \\rho \\left( 1 \\right)}}\\] Furthermore, it can be shown that: \\[\\sqrt T \\left( {\\hat \\theta - \\theta } \\right)\\mathop \\to \\limits_{T \\to \\infty }^L N\\left( {0 ,\\frac{{1 + {\\theta ^2} + 4{\\theta ^4} + {\\theta ^6} + {\\theta ^8}}}{{{{\\left( {1 - {\\theta ^2}} \\right)}^2}}}} \\right)\\] So, this is not a really optimal estimator… 4.7 Prediction (Forecast) "],
["linear-regression.html", "Chapter 5 Linear Regression 5.1 Review on Linear Regression 5.2 Linear Regression with Autocorrelated Errors", " Chapter 5 Linear Regression 5.1 Review on Linear Regression In this chapter we discuss how the classical linear regression setting can be extended to accomodate for autocorrelated error. Before considering this more general setting, we start by discussing the usual linear regression model with Gaussian errors, i.e. \\begin{equation*} \\y = \\X \\bbeta + \\bepsilon, \\;\\;\\; \\bepsilon \\sim \\mathcal{N}\\left( \\0,\\sigma_{\\epsilon}^2 \\I \\right) , \\end{equation*} where \\(\\X\\) is a known \\(n \\times p\\) design matrix of rank \\(p\\) and \\(\\bbeta\\) is a \\(p \\times 1\\) vector of unknown parameters. Under this setting, the MLE and LSE are equivalent (due to normality of \\(\\bepsilon\\)) and corresponds to the ordinary LS parameter estimates of \\(\\bbeta\\), i.e. \\begin{equation} \\hat{\\bbeta} = \\left(\\X^T \\X \\right)^{-1} \\X^T \\y , \\label{eq:betaLSE} \\end{equation} leading to the (linear) prediction \\begin{equation*} \\hat{\\y} = \\X \\hat{\\bbeta} = \\S \\y \\end{equation*} where \\(\\S = \\X\\left(\\X^T \\X \\right)^{-1} \\X^T\\) denotes the “hat” matrix. The unbiased and maximum likelihood estimates of \\(\\sigma^2_{\\epsilon}\\) are, respectively, given by \\begin{equation} \\tilde{\\sigma}^2_{\\epsilon} = \\frac{||\\y - \\hat{\\y} ||_2^2}{n - p} \\;\\;\\, \\text{and} \\;\\;\\, \\hat{\\sigma}^2_{\\epsilon} = \\frac{||\\y - \\hat{\\y} ||_2^2}{n}\\,, \\label{eq:LM:sig2:hat} \\end{equation} where \\(|| \\cdot ||_2\\) denotes the \\(L_2\\) norm. Throughout this chapter we assume that \\(0 &lt; \\sigma_{\\epsilon}^2 &lt; \\infty\\). Under this setting (i.e. Gaussian iid errors) \\(\\tilde{\\sigma}^2_{\\epsilon}\\) is distributed proportiinally to \\(\\chi^2\\) random vcaraible with \\(n-p\\) degrees of freedom independent of \\(\\hat{\\bbeta}\\) (a proof of this result can for example be found in ?????). Consequently, it follow that \\begin{equation} \\frac{\\hat{\\beta}_i - \\beta_i}{\\left(\\boldsymbol{C}\\right)_{i}} \\sim t_{n-p}, \\label{eq:beta_t_dist} \\end{equation} where \\(\\left(\\boldsymbol{C}\\right)_{i}\\) denotes the \\(i\\)-th diagonal element of the following matrix \\begin{equation} \\boldsymbol{C} = \\cov \\left(\\hat{\\bbeta} \\right) = \\sigma_{\\epsilon}^2 \\left(\\X^T \\X\\right)^{-1}, \\label{eq:covbeta} \\end{equation} and where \\(\\hat{\\beta}_i\\) denotes the \\(i\\)-th element of \\(\\hat{\\bbeta}\\). Thus, this allows for a natural approach for testing coefficients and selecting models. Moreover, a common quantity used ton evaluate the “quality” of a model is the \\(R^2\\), which corresponds to the proportion of variation explained by the model, i.e. \\[R^2 = \\frac{\\sum_{i=1}^n \\left(y_i - \\hat{\\y}_i\\right)^2 - \\sum_{i=1}^n \\left(y_i - \\bar{y}\\right)^2}{\\sum_{i=1}^n \\left(y_i - \\bar{y}\\right)^2},\\] where \\(y_i\\) and \\(\\hat{y}_i\\) denote, respectively, the \\(i\\)-th element of \\(\\y\\) and \\(\\hat{\\y}\\), and \\(\\bar{y}\\) represent the mean value of the vector \\(\\y\\). This goodness-of-fit is widely used in practice but its limits are often misunderstood as illustrated in the example below. Example: Suppose that we have two nested models, say \\(\\mathcal{M}_1\\) and \\(\\mathcal{M}_2\\), i.e. \\[\\begin{aligned} \\mathcal{M}_1: \\;\\;\\;\\;\\; \\y &amp;= \\X_1 \\bbeta_1 + \\bepsilon,\\\\ \\mathcal{M}_2: \\;\\;\\;\\;\\; \\y &amp;= \\X_1 \\bbeta_1 + \\X_2 \\bbeta_2 + \\bepsilon,\\\\ \\end{aligned}\\] and assume that \\(\\bbeta_2 = \\0\\). In this case, it is interesting to compare the \\(R^2\\) of both models, say \\(R_1^2\\) and \\(R^2_2\\). Using \\(\\hat{\\y}_i\\) to denote the predictions made from model \\(\\mathcal{M}_i\\), we have that \\[||\\y - \\hat{\\y}_1 ||_2^2 \\geq ||\\y - \\hat{\\y}_2 ||_2^2.\\] By letting \\(||\\y - \\hat{\\y}_1 ||_2^2 = ||\\y - \\hat{\\y}_2 ||_2^2 + c\\) where \\(c\\) is a non-negartive constant we obtain: \\[R_1^2 = 1 - \\frac{ ||\\y - \\hat{\\y}_1 ||_2^2 }{ \\sum_{i=1}^n \\left(y_i - \\bar{y}\\right)^2} = 1 - \\frac{||\\y - \\hat{\\y}_2 ||_2^2 + c}{\\sum_{i=1}^n \\left(y_i - \\bar{y}\\right)^2} = R_2^2 + \\frac{c}{\\sum_{i=1}^n \\left(y_i - \\bar{y}\\right)^2}.\\] This implies that \\(R_1^2 \\leq R_2^2\\), regardelss of the value of \\(\\bbeta_2\\) and therefore the \\(R^2\\) is essentially useless in terms of model selection. This results is well known and is further discuss in ??????REF REGRESSION and TIME SERIES MODEL SELECTION&lt; TSAI&lt; CHAP 2. A more approriate measure of the goodness-of-fit of a particular model is for example Mallow’s \\(C_p\\) introduced in REF see STEF PHD. This metric balances the error of fit against its complexity and can be defined as \\begin{equation} C_p = || \\y - \\X\\hat{\\bbeta}||_2^2 + 2 \\hat{\\sigma}_{\\ast}^2 p, \\label{eq:MallowCp} \\end{equation} where \\(\\hat{\\sigma}_{\\ast}^2\\) is an unbiased estimates of \\({\\sigma}_{\\epsilon}^2\\), generally \\(\\tilde{\\sigma}^2_{\\epsilon}\\) computed on a “low-bias” model (i.e. a sufficiently “large” model). To understand how this result is derived, we let \\(\\y_0\\) denote an independent “copy” of \\(\\y\\) issued from the same data-generating process and let \\(E_0[\\cdot]\\) denotes the expectation under the distribution of \\(\\y_0\\) (conditionally on \\(\\X\\)). Then, it can be argued that the following quantity is approriate at measuring the adequacy of model as it compares how \\(\\y\\) can be used to predict \\(\\y_0\\), \\[E \\left[ E_0 \\left[ || \\y_0 - \\X\\hat{\\bbeta}||_2^2 \\right] \\right].\\] As we will see Mallow’s \\(C_p\\) is an unbiased estimator of this quantity. There are several ways of showing it, one of them is presented here using the following “optimism” theorem. Note that this result is based on Theorem 2.1 of REF MISSING, TWO HERE PHD STEF and on the Optimism Theorem of ** REF MISSING EFRON COVARIACNE PAPER 2004 JASA**. Theorem: Let \\(\\y_0\\) denote an independent “copy” of \\(\\y\\) issued from the same data-generating process and let \\(E_0[\\cdot]\\) denotes the expectation under the distribution of \\(\\y_0\\) (conditionally on \\(\\X\\)). Then we have that, \\[E \\left[ E_0 \\left[ || \\y_0 - \\X\\hat{\\bbeta}||_2^2 \\right] \\right] = E \\left[ || \\y - \\X\\hat{\\bbeta}||_2^2 \\right] + 2 \\tr \\left( \\cov \\left(\\y, \\X \\hat{\\bbeta} \\right)\\right).\\] Proof: We first expend \\(|| \\y - \\X{\\bbeta}||_2^2\\) as follows: \\[|| \\y - \\X{\\bbeta}||_2^2 = \\y^T \\y + \\bbeta^T \\X^T \\X \\bbeta - 2 \\y^T \\X \\bbeta = \\y^T \\y - \\bbeta^T \\X^T \\X \\bbeta - 2 \\left(\\y - \\X\\bbeta\\right)^T \\X \\bbeta. \\] Then, we define C and C\\(^\\ast\\) and used the above expension \\[\\begin{aligned} \\text{C} &amp;= E \\left[ E_0 \\left[ || \\y_0 - \\X\\hat{\\bbeta}||_2^2 \\right] \\right] = E_0 \\left[ \\y_0^T \\y_0 \\right] - E \\left[ \\hat{\\bbeta}^T \\X^T \\X \\hat{\\bbeta}\\right] - 2 E \\left[\\left(E_0 \\left[ \\y_0\\right] - \\X\\hat{\\bbeta}\\right)^T \\X \\hat{\\bbeta}\\right],\\\\ \\text{C}^\\ast &amp;= E \\left[ || \\y - \\X\\hat{\\bbeta}||_2^2 \\right] = E \\left[ \\y^T \\y \\right] - E \\left[ \\hat{\\bbeta}^T \\X^T \\X \\hat{\\bbeta}\\right] - 2 E \\left[\\left( \\y - \\X\\hat{\\bbeta}\\right)^T \\X \\hat{\\bbeta}\\right]. \\end{aligned}\\] Next, we consider the difference between C and C\\(^\\ast\\), i.e. \\[\\begin{aligned} \\text{C} - \\text{C}^\\ast &amp;= 2 E \\left[\\left( \\y - E_0 \\left[ \\y_0\\right]\\right)^T \\X \\hat{\\bbeta}\\right] = 2 \\tr \\left( \\cov \\left(\\y - E_0 [\\y_0], \\X \\hat{\\bbeta} \\right)\\right) + 2 \\tr \\left(E \\left[\\y - E_0 [\\y_0] \\right] E^T [\\X \\hat{\\bbeta}]\\right) \\\\ &amp;= 2 \\tr \\left( \\cov \\left(\\y - E_0 [\\y_0], \\X \\hat{\\bbeta} \\right)\\right) = 2 \\tr \\left( \\cov \\left(\\y, \\X \\hat{\\bbeta} \\right)\\right), \\end{aligned} \\] which concludes our proof. Note that in the above equation we used the following equality, which is based on two vector valued random variation of approriate dimensions: \\[E \\left[\\X^T \\boldsymbol{Z}\\right] = E \\left[\\tr \\left(\\X^T \\boldsymbol{Z}\\right)\\right] = E \\left[\\tr \\left( \\boldsymbol{Z} \\X^T \\right)\\right] = \\tr \\left(\\cov \\left(\\X, \\boldsymbol{Z}\\right)\\right) + \\tr \\left(E[\\X] E^T[\\boldsymbol{Z}]\\right). \\] In the linear regression case with iid Gaussian errors we have: \\[\\tr \\left( \\cov \\left(\\y, \\X \\hat{\\bbeta} \\right)\\right) = \\tr \\left( \\cov \\left(\\y, \\S \\y \\right)\\right) = \\sigma_{\\epsilon}^2 \\tr\\left(\\S\\right) = \\sigma_{\\epsilon}^2 p.\\] Therefore, \\[\\text{C} = E \\left[ E_0 \\left[ || \\y_0 - \\X\\hat{\\bbeta}||_2^2 \\right] \\right] = E \\left[ || \\y - \\X\\hat{\\bbeta}||_2^2 \\right] + 2 \\sigma_{\\epsilon}^2 p, \\] yielding to the unbiased estimate \\[\\widehat{\\text{C}} = C_p = || \\y - \\X\\hat{\\bbeta}||_2^2 + 2 \\hat{\\sigma}_{\\ast}^2 p.\\] An alternative famous goodness-of-fit criterion was proposed by Akaike (1969, 1973, 1974) REF MISSING and is given by \\begin{equation}\\text{AIC} = \\log \\left(\\hat{\\sigma}^2_{\\epsilon} \\right) + \\frac{n + 2p}{n}. \\label{eq:defAIC} \\end{equation} where \\(\\hat{\\sigma}^2_{\\epsilon}\\) denotes the MLE for \\(\\sigma_{\\epsilon}^2\\) defined in \\ref{eq:LM:sig2:hat}. The AIC is based on a divergence (i.e. a generalization of the notion of distance) that informally speaking measures “how far” is the density of the estimated model compared to the “true” density. This divergence is called the Kullback-Leibler information which in this context can be defined for two densities of the same family as \\[\\KL = \\frac{1}{n} E \\left[ E_0 \\left[\\log \\left( \\frac{f (\\y_0| \\btheta_0)} {f (\\y_0| \\hat{\\btheta})} \\right)\\right] \\right],\\] where we assume \\(\\btheta_0\\) and \\(\\hat{\\btheta}\\) to denote, respectively, the true parameter vector of interest and an estimator \\(\\btheta_0\\) based on a postulated model. Similarly to the setting used to derive Mallow’s \\(C_p\\), the expectations \\(E \\left[\\cdot\\right]\\) and \\(E_0 \\left[\\cdot\\right]\\) denote the expectation with respect to the densities of \\(\\y\\) and \\(\\y_0\\) (conditionally on \\(\\X\\)). Note that \\(\\hat{\\btheta}\\) dependences on \\(\\y\\) and not \\(\\y_0\\). Informally speaking this divergence measure how far is \\(f (\\y_0| \\btheta_0)\\) from \\(f (\\y_0| \\hat{\\btheta})\\), where in the latter \\(\\hat{\\btheta}\\) is estimated on \\(\\y\\), a sample independent from \\(\\y_0\\). To derive the AIC we start by considering a generic a linear model \\(\\mathcal{M}\\) with parameter vector \\(\\btheta = [\\bbeta^T \\;\\;\\; \\sigma_{\\epsilon}^2]\\). Indeed, we have that its density is given by \\[\\begin{aligned} f\\left( {\\y|\\btheta } \\right) &amp;= {\\left( {2\\pi } \\right)^{ - n/2}}{\\left| { \\sigma_{\\epsilon}^2 \\I} \\right|^{ - 1/2}}\\exp \\left( { - \\frac{1}{2}{{\\left( {\\y - \\X{\\bbeta}} \\right)}^T}{{\\left( {\\sigma_{\\epsilon}^2 \\I} \\right)}^{ - 1}}\\left( {\\y - \\X{\\beta _i}} \\right)} \\right) \\\\ &amp;= {\\left( {2\\pi } \\right)^{ - n/2}}{\\left( {\\sigma_{\\epsilon}^2} \\right)^{ - n/2}}\\exp \\left( { - \\frac{1}{{2 \\sigma_{\\epsilon}^2}}{{\\left( {\\y - \\X{\\bbeta}} \\right)}^T}\\left( {\\y - \\X{\\bbeta}} \\right)} \\right). \\\\ \\end{aligned} \\] Using this result and letting \\[{\\btheta}_0 = [{\\bbeta}_0^T \\;\\;\\; {\\sigma}^2_0] \\;\\;\\;\\;\\; \\text{ and } \\;\\;\\;\\;\\; \\hat{\\btheta} = [\\hat{\\bbeta}^T \\;\\;\\; \\hat{\\sigma}^2], \\] where \\(\\hat{\\btheta}\\) denotes the MLE for \\(\\hat{\\btheta}\\), we obtain \\[\\scriptsize \\begin{aligned} \\frac{1}{n} {E}\\left[ {E_0}\\left[ {\\log \\left( {\\frac{{f\\left( {\\y_0|{\\btheta_0}} \\right)}}{{f\\left( {\\y_0|{\\hat{\\btheta}}} \\right)}}} \\right)} \\right]\\right] &amp;= \\frac{1}{n} {E}\\left[ {E_0}\\left[ \\log \\left( {\\frac{{{{\\left( {\\sigma _0^2} \\right)}^{ - n/2}}}}{{{{\\left( {\\hat{\\sigma}^2} \\right)}^{ - n/2}}}}} \\right) + \\log \\left( \\frac{{\\exp \\left( { - \\frac{1}{{2\\sigma _0^2}}{{\\left( {\\y_0 - \\X{\\bbeta _0}} \\right)}^T}\\left( {\\y_0 - \\X{\\bbeta _0}} \\right)} \\right)}}{{\\exp \\left( { - \\frac{1}{{2\\hat{\\sigma}^2}}{{\\left( {\\y_0 - \\X{\\hat{\\bbeta}}} \\right)}^T}\\left( {\\y_0 - \\X{\\hat{\\bbeta}}} \\right)} \\right)}} \\right) \\right] \\right] \\\\ &amp;= -\\frac{1}{2} E \\left[\\log \\left( {\\frac{{\\sigma _0^2}}{{\\hat{\\sigma}^2}}} \\right)\\right] - \\frac{1}{{2n\\sigma _0^2}}{E_0}\\left[ {{{\\left( {\\y_0 - \\X{\\bbeta _0}} \\right)}^T}\\left( {\\y_0 - \\X{\\bbeta _0}} \\right)} \\right] \\\\ &amp;+ \\frac{1}{{2n}}{E}\\left[\\frac{1}{\\hat{\\sigma}^2}E_0\\left[ {{{\\left( {\\y_0 - \\X{\\hat{\\bbeta}}} \\right)}^T}\\left( {\\y_0 - \\X{\\hat{\\bbeta}}} \\right)} \\right]\\right]. \\end{aligned} \\] Next, we consider each term of the above equation. For the first term, we have \\[ -\\frac{1}{2} E \\left[\\log \\left( {\\frac{{\\sigma _0^2}}{{\\hat{\\sigma}^2}}} \\right)\\right] = \\frac{1}{2} \\left(E \\left[ \\log \\left( \\hat{\\sigma}^2 \\right) \\right] - \\log \\left( \\sigma_0^2 \\right)\\right). \\] For the second term, we obtain \\[ -\\frac{1}{{2n\\sigma _0^2}} {E_0}\\left[ {{{\\left( {\\y_0 - \\X{\\bbeta _0}} \\right)}^T}\\left( {\\y_0 - \\X{\\bbeta _0}} \\right)} \\right] = -\\frac{1}{2}. \\] Finally, we have for the last term \\[\\scriptsize \\begin{aligned} \\frac{1}{{2n}} {E}\\left[ \\frac{1}{\\hat{\\sigma}^2} {E_0}\\left[ {{{\\left( {\\y_0 - \\X{\\hat{\\bbeta}}} \\right)}^T}\\left( {\\y_0 - \\X\\hat{\\bbeta}} \\right)} \\right]\\right] &amp;= \\frac{1}{{2n}} {E}\\left[ \\frac{1}{\\hat{\\sigma}^2} {E_0}\\left[ {{{\\left( {\\y_0 - \\X \\bbeta_0 - \\X\\left( {\\hat{\\bbeta} - \\bbeta_0} \\right)} \\right)}^T}\\left( {\\y_0 - \\X \\bbeta_0 - \\X\\left( {\\hat{\\bbeta} - \\bbeta_0} \\right)} \\right)} \\right] \\right] \\\\ &amp;= \\frac{1}{{2n}} E\\left[ \\frac{1}{\\hat{\\sigma}^2} \\left[ {E_0}\\left[ {{{\\left( {\\y_0 - \\X \\bbeta_0} \\right)}^T}\\left( {\\y_0 - \\X \\bbeta_0} \\right)} \\right]\\right] \\right]\\\\ &amp;+ \\frac{1}{{2n}} E \\left[ \\frac{1}{\\hat{\\sigma}^2} \\left( \\bbeta_0 - \\hat{\\bbeta} \\right)^T \\X^T \\X\\left( \\bbeta_0 - \\hat{\\bbeta} \\right)\\right]\\\\ &amp;= \\frac{1}{{2}} E \\left[ \\frac{\\sigma_0^2}{\\hat{\\sigma}^2} \\right] + \\frac{1}{{2n}} E \\left[ \\frac{\\sigma_0^2}{\\hat{\\sigma}^2} \\frac{\\left( \\bbeta_0 - \\hat{\\bbeta} \\right)^T \\X^T \\X\\left( \\bbeta_0 - \\hat{\\bbeta} \\right)}{\\sigma_0^2}\\right].\\\\ \\end{aligned}\\] To simplify further this result it is usefull to remeber that \\[ U_1 = \\frac{n \\hat{\\sigma}^2}{\\sigma_0^2} \\sim \\chi^2_{n-p}, \\;\\;\\;\\;\\;\\; U_2 = \\frac{\\left( \\bbeta_0 - \\hat{\\bbeta} \\right)^T \\X^T \\X\\left( \\bbeta_0 - \\hat{\\bbeta} \\right)}{\\sigma_0^2} \\sim \\chi^2_p, \\] and that \\(U_1\\) and \\(U_2\\) are independent. Moreover, we have that if \\(U \\sim \\chi^2_k\\) then \\(E[1/U] = 1/(k-2)\\). Thus, we obtain \\[\\begin{aligned} \\frac{1}{{2n}} {E}\\left[ \\frac{1}{\\hat{\\sigma}^2} {E_0}\\left[ {{{\\left( {\\y_0 - \\X{\\hat{\\bbeta}}} \\right)}^T}\\left( {\\y_0 - \\X\\hat{\\bbeta}} \\right)} \\right]\\right] = \\frac{n+p}{{2(n-p-2)}}. \\end{aligned}\\] Combining, the above result we have \\[\\KL = \\frac{1}{2} \\left[ E \\left[ \\log \\left( \\hat{\\sigma}^2 \\right) \\right] + \\frac{n+p}{(n-p-2)} + c \\right],\\] where \\(c = - \\log \\left( \\sigma_0^2 \\right) - 1\\). Since the constant \\(c\\) is common to all models it can neglated for the purpose of model selection. Therefore, neglecting the constant we obtain that \\[\\KL \\propto E \\left[ \\log \\left( \\hat{\\sigma}^2 \\right) \\right] + \\frac{n+p}{(n-p-2)}.\\] Thus, an unbiased estimator of \\(\\KL\\) is given by \\[\\text{AICc} = \\log \\left( \\hat{\\sigma}^2 \\right) + \\frac{n+p}{(n-p-2)},\\] since an unbiased estimator of \\(E \\left[\\log \\left( \\hat{\\sigma}^2 \\right)\\right]\\) is simply \\(\\log \\left( \\hat{\\sigma}^2 \\right)\\). However, it can be observed that the result we derived is not equal to the AIC defined in (\\ref{eq:defAIC}). Indeed, this result is known as the bias-corrected AIC or AICc. To understand the relationship between the AIC and AICc it is instructif to consider their difference and letting \\(n\\) diverge to infinity, i.e. \\[\\lim_{n \\to \\infty} \\; \\text{AIC} - \\text{AICc} = \\frac{2 \\left(p^2 + 2p + n\\right)}{n \\left(p - n - 2\\right)} = 0.\\] Therefore, the AIC is an asymptotically unbiased estimator of \\(\\KL\\). In practice, the AIC and AICc provides very similar result expect when the sample size is rather small. TO DO Talk about BIC Illustration for model selection with linear model: TO DO add comments # Load libraries library(astsa) # Load data data(gtemp) # Degree of polynomial regression deg_max = 30 # Construct design matrix (no intercept) year = time(gtemp) X = cbind(year) for (i in 2:deg_max){X = cbind(X,year^i)} # Define response vector y = gtemp # Initialisation model.AIC = rep(NA,deg_max) model.BIC = rep(NA,deg_max) model.pred = matrix(NA,deg_max,length(y)) # Fit models for (i in 1:deg_max){ # Fit model model = lm(y~X[,1:i]) # Compute AIC, BIC and \\hat{y} model.AIC[i] = AIC(model) model.BIC[i] = BIC(model) model.pred[i,] = fitted(model) } # Compute best AIC and BIC aic.best = which.min(model.AIC) bic.best = which.min(model.BIC) # Plot results par(mfrow = c(1,2)) plot(NA, xlim = c(1,deg_max), ylim = range(cbind(model.AIC, model.BIC)), xlab = &quot;Polynomial order&quot;, ylab = &quot;AIC/BIC&quot;) grid() lines(model.AIC, type = &quot;b&quot;, col = &quot;dodgerblue3&quot;, lty = 2) lines(model.BIC, type = &quot;b&quot;, col = &quot;darkgoldenrod2&quot;, pch = 22) points(aic.best,model.AIC[aic.best], col = &quot;dodgerblue3&quot;, pch = 16, cex = 2) points(bic.best,model.BIC[bic.best], col = &quot;darkgoldenrod2&quot;, pch = 15, cex = 2) legend(&quot;topright&quot;, c(&quot;BIC&quot;,&quot;Min BIC&quot;,&quot;AIC&quot;,&quot;Min AIC&quot;), pch = c(22,15,21,16), pt.cex = rep(c(1,2),2), col = rep(c(&quot;darkgoldenrod2&quot;,&quot;dodgerblue3&quot;), each = 2), lty = c(1,NA,2,NA), lwd = c(1,NA,1,NA), bty = &quot;n&quot;, bg = &quot;white&quot;, box.col = &quot;white&quot;, cex = 1.2) plot(NA, xlim = range(year), ylim = range(y), xlab = &quot;Time (year)&quot;, ylab = &quot;Global Temperature Deviation&quot;) grid() lines(gtemp, col = &quot;darkgrey&quot;) lines(cbind(year,model.pred[aic.best,])[,2], col = &quot;dodgerblue3&quot;, lty = 2, lwd = 2) legend(&quot;topleft&quot;, c(&quot;Data&quot;,&quot;Fitted (best AIC)&quot;), col = c(&quot;darkgrey&quot;,&quot;dodgerblue3&quot;), lty = c(1,2), lwd = c(1,2), bty = &quot;n&quot;, bg = &quot;white&quot;, box.col = &quot;white&quot;, cex = 1.2) 5.2 Linear Regression with Autocorrelated Errors TO DO "],
["state-space-models.html", "Chapter 6 State-Space Models", " Chapter 6 State-Space Models "],
["time-series-models-of-heteroskedasticity.html", "Chapter 7 Time Series Models of Heteroskedasticity", " Chapter 7 Time Series Models of Heteroskedasticity "],
["proofs.html", "A Proofs A.1 Proof of Theorem 1", " A Proofs A.1 Proof of Theorem 1 Let \\(X_t = W_t + \\mu\\), where \\(\\mu &lt; \\infty\\) and \\((W_t)\\) is a strong white noise process with variance \\(\\sigma^2\\) and finite fourth moment. Next, we consider the sample autocovariance function computed on \\((X_t)\\), i.e. \\begin{equation*} \\hat \\gamma \\left( h \\right) = \\frac{1}{T}\\sum\\limits_{t = 1}^{T - h} {\\left( {{X_t} - \\bar X} \\right)\\left( {{X_{t + h}} - \\bar X} \\right)}. \\end{equation*} For this equation, it is clear that \\(\\hat \\gamma \\left( 0 \\right)\\) and \\(\\hat \\gamma \\left( h \\right)\\) (with \\(h &gt; 0\\)) are two statistics involving sums of different lengths. As we will see, this prevents us from using directely the multivariate central limit theorem on the vector \\([ \\hat \\gamma \\left( h \\right) \\;\\;\\; \\hat \\gamma \\left( h \\right) ]^T\\). However, the lag \\(h\\) is fixed and therefore the difference in the number of elements of both sums is asymptotically negligible. Therefore, we define a new statistic \\begin{equation*} \\tilde{\\gamma} \\left( h \\right) = \\frac{1}{T}\\sum\\limits_{t = 1}^{T} {\\left( {{X_t} - \\mu} \\right)\\left( {{X_{t + h}} - \\mu} \\right)}, \\end{equation*} which is easier to “handle” and show that \\(\\hat \\gamma \\left( h \\right)\\) and \\(\\tilde{\\gamma} \\left( h \\right)\\) are asymptotically equivalent in the sense that: \\begin{equation*} T^{\\frac{1}{2}}[\\tilde{\\gamma} \\left( h \\right) - \\hat \\gamma \\left( h \\right)] = o_p(1). \\end{equation*} ADD HERE PROOF OF THE ABOVE RESULT Therefore \\(\\tilde{\\gamma} \\left( h \\right)\\) and \\(\\hat \\gamma \\left( h \\right)\\) have the same asymptotic distribution, it is suffice to show the asymptotic distribution of \\(\\tilde{\\gamma} \\left( h \\right)\\). The expectation of \\(\\tilde{\\gamma} \\left( h \\right)\\) \\begin{equation*} \\mathbb{E}[\\tilde{\\gamma} \\left( h \\right)] = \\frac{1}{T}\\mathbb{E}\\left[\\sum\\limits_{t = 1}^{T} {\\left( {{X_t} - \\mu} \\right)\\left( {{X_{t + h}} - \\mu} \\right)}\\right]. \\end{equation*} By Cauchy–Schwarz inequality and \\(\\var(X_t) = \\sigma^2\\), we have \\begin{equation*} \\sum\\limits_{t = 1}^{T} {\\left( {{X_t} - \\mu} \\right)\\left( {{X_{t + h}} - \\mu} \\right)} \\leq \\sum\\limits_{t = 1}^{T} {\\left( {{X_t} - \\mu} \\right)^2} \\sum\\limits_{t = 1}^{T} {\\left( {{X_{t + h}} - \\mu} \\right)^2} &lt; \\infty. \\end{equation*} Then by bounded convergence theorem and \\(\\{X_t\\}_1^T\\) are independent, we have \\begin{equation*} \\mathbb{E}[\\tilde{\\gamma} \\left( h \\right)] = \\frac{1}{T}\\left[\\sum\\limits_{t = 1}^{T} { \\mathbb{E}\\left( {{X_t} - \\mu} \\right)\\mathbb{E}\\left( {{X_{t + h}} - \\mu} \\right)}\\right] = \\begin{cases} \\sigma^2, &amp; \\text{for } h = 0\\\\ 0, &amp; \\text{for } h \\neq 0 \\end{cases}. \\end{equation*} The variance of \\(\\tilde{\\gamma} \\left( h \\right)\\), when \\(h \\neq 0\\), \\begin{equation*} \\begin{aligned} \\var[\\tilde{\\gamma} \\left( h \\right)] &amp;= \\frac{1}{T^2}\\mathbb{E}\\left\\{\\left[\\sum\\limits_{t = 1}^{T} {\\left( {{X_t} - \\mu} \\right)\\left( {{X_{t + h}} - \\mu} \\right)}\\right]^2\\right\\}\\\\ &amp;= \\frac{1}{T^2}\\mathbb{E}\\left\\{\\left[\\sum\\limits_{i = 1}^{T} {\\left( {{X_i} - \\mu} \\right)\\left( {{X_{i + h}} - \\mu} \\right)}\\right] \\left[\\sum\\limits_{j = 1}^{T} {\\left( {{X_j} - \\mu} \\right)\\left( {{X_{j + h}} - \\mu} \\right)}\\right]\\right\\}\\\\ &amp;= \\frac{1}{T^2}\\mathbb{E}\\left[\\sum\\limits_{i = 1}^{T}\\sum\\limits_{j = 1}^{T} {\\left( {{X_i} - \\mu} \\right)\\left( {{X_{i + h}} - \\mu} \\right)}{\\left( {{X_j} - \\mu} \\right)\\left( {{X_{j + h}} - \\mu} \\right)}\\right]. \\end{aligned} \\end{equation*} Also by Cauchy–Schwarz inequality and the finite fourth moment assumption, we can use the bounded convergence theorem. And since \\(\\{X_t\\}_1^T\\) is white noise process, we have \\begin{equation*} \\mathbb{E}\\left[{\\left( {{X_i} - \\mu} \\right)\\left( {{X_{i + h}} - \\mu} \\right)}{\\left( {{X_j} - \\mu} \\right)\\left( {{X_{j + h}} - \\mu} \\right)}\\right] \\neq 0 \\end{equation*} only when \\(i = j\\). Therefore, \\begin{equation*} \\begin{aligned} \\var[\\tilde{\\gamma} \\left( h \\right)] &amp;= \\frac{1}{T^2}\\sum\\limits_{i = 1}^{T} \\mathbb{E}\\left[ {\\left( {{X_i} - \\mu} \\right)^2\\left( {{X_{i + h}} - \\mu} \\right)^2}\\right]\\\\ &amp;= \\frac{1}{T^2}\\sum\\limits_{i = 1}^{T} \\mathbb{E}{\\left( {{X_i} - \\mu} \\right)^2\\mathbb{E}\\left( {{X_{i + h}} - \\mu} \\right)^2}\\\\ &amp;= \\frac{1}{T}\\sigma^4, \\text{for } (h \\neq 0). \\end{aligned} \\end{equation*} Similarly, when \\(h = 0\\), we have \\begin{equation*} \\begin{aligned} \\var[\\tilde{\\gamma} \\left( 0 \\right)] &amp;= \\frac{1}{T^2}\\mathbb{E}\\left\\{\\left[\\sum\\limits_{t = 1}^{T} {\\left( {{X_t} - \\mu} \\right)^2}\\right]^2\\right\\} - \\frac{1}{T^2}\\left[\\mathbb{E}\\sum\\limits_{t = 1}^{T} {\\left( {{X_t} - \\mu} \\right)^2}\\right]^2\\\\ &amp;= \\frac{2}{T}\\sigma^4, \\text{for } (h = 0). \\end{aligned} \\end{equation*} Next, since \\(\\{\\left( {{X_t} - \\mu} \\right)\\left( {{X_{t + h}} - \\mu} \\right)\\}_1^T\\) are iid, we can apply CLT to it, and have \\begin{equation*} \\sqrt{T}\\tilde{\\gamma} \\left( h \\right) = \\frac{1}{\\sqrt{T}}\\sum\\limits_{t = 1}^{T} {\\left( {{X_t} - \\mu} \\right)\\left( {{X_{t + h}} - \\mu} \\right)} \\overset{\\mathcal{D}}{\\to} \\begin{cases} N(\\sigma^2, 2\\sigma^4), &amp; \\text{for } h = 0\\\\ N(0, \\sigma^4), &amp; \\text{for } h \\neq 0 \\end{cases}. \\end{equation*} Therefore, we also have \\begin{equation*} \\sqrt{T}\\hat \\gamma \\left( h \\right) \\overset{\\mathcal{D}}{\\to} \\begin{cases} N(\\sigma^2, 2\\sigma^4), &amp; \\text{for } h = 0\\\\ N(0, \\sigma^4), &amp; \\text{for } h \\neq 0 \\end{cases}. \\end{equation*} Since we can show that \\(\\hat \\gamma \\left( 0 \\right) \\overset{p}{\\to} \\sigma^2\\), by Slutsky Theorem, for \\(h \\neq 0\\) \\begin{equation*} \\hat \\rho \\left( h \\right) = \\frac{ \\hat \\gamma \\left( h \\right)}{\\hat \\gamma \\left( 0 \\right)} \\overset{\\mathcal{D}}{\\to} N(0, \\frac{1}{T}). \\end{equation*} "],
["remark-on-syntax.html", "B Remark on Syntax", " B Remark on Syntax Hi guys, since we are all using a different syntax here is what I propose. Let me know if we should change it. Once we agree, we will unify the syntax: Symbol Meaning Code \\(\\e{ }\\) Expected value \\e{ } \\(\\var\\) Variance \\var \\(\\cov\\) Covariance \\cov \\(\\corr\\) Correlation \\corr \\(\\ind\\) Indicator \\ind \\(\\real\\) Real \\real \\(\\natural\\) Natural \\natural \\(\\integers\\) Integers \\integers \\((X_t)\\) A time series (X_t) \\(\\{X_t\\}\\) A set \\{X_t\\} \\(\\gamma()\\) Autocov. fun. \\gamma() \\(\\rho()\\) Autocor. fun. \\rho() \\(W_t\\) White noise W_t \\(\\normal\\) Normal dist. \\normal \\(\\KL\\) KL diverg. \\KL \\(\\AIC\\) AIC \\AIC \\(\\BIC\\) BIC \\BIC \\(n\\) Length of time series n Also for the parenthesis, I propose to use this as much as possible \\(\\{[(\\{[( \\cdot )]\\})]\\}\\), for which I am a terrible example. "]
]
